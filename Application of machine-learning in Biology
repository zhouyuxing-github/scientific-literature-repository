非洲猪瘟病毒在非洲猪上不致病，但是血洗了其他国家的猪，给我国造成直接损失一万亿。所以：重视看似温和的病毒！很可能在其它国家或其它物种上表现凶猛！因此，监测高丰度contigs很重要！               
Raj Reddy：人工智能是用来增强人类能力的，而非用来替代人类的                                                        ——写在前面

疫苗设计数据库介绍

随着疫情的发展，目前对于新冠疫苗相信很多人都有一定了认识。当然在新冠之前也就存在其他疾病的疫苗了。疫苗的接种可以有效的防止我们受到其他物种感染的影响。之前常见的还有天花疫苗，HPV疫苗这类。因此对于很多对人体有影响的其他病菌，其实都可以通过疫苗来进行预防。那么对于相关的疫苗要怎么设计呢？今天就来给大家介绍一个在线疫苗设计的工具：Vaxign2(http://www.violinet.org/vaxign2)

内置算法介绍
Vaxign2数据库主要是利用机器学习的算法来进行疫苗预测的。对于机器学习而言，首先需要一个训练集来进行模型训练。在这个数据库当中，作者使用Protegen数据库(一个储存了近十年经过实验验证保护性抗原的数据库)来进行模型训练。

数据库使用
在Vaxign2当中，主要是包括两个功能。
动态分析(Dynamic Analysis)：输入自己想要分析的序列来进行保护性抗原预测。
已知结果查询(Precompute Query)：作者提前分析好了一些病菌的保护性抗原。直接进行查询结果即可。
这里我们就主要介绍一下怎么来动态分析这个功能。
1. 数据输入
在Vaxign2当中，支持多种输入方式，我们可以输入想要分析的FASTA蛋白序列。同时也可以输入想要分析蛋白的Uniprot ID或者NCBI蛋白ID。这里，我们就用数据库给的病菌序列来进行演示。

在输入完想要分析的序列。就可以选择分析的保护性抗原的内容。这里主要是包括三个选项。
基本分析：
基于PSORTb的细胞定位分析
基于TMHMM的跨螺旋结构分析
基于SPAAN的粘附概率分析
保护性抗原和不同物种(人、老鼠以及猪)的序列比对。
是否进行机器学习分析
是否进行Vaxitop分析

在选择好基本的分析之后，点击Submit即可进行数据提交了。
2. 结果解读
在进行分析之后，首先可以得到的是一个表格。在这个表格当中，我们可以看到预测的的基本信息。

除了最基本的信息，也可以对进行额外的分析。

2.1 Vaxitop分析
所谓的Vaxtop分析。是可以来在实验验证之前，利用Vaxign2可以进一步研究预测的保护性抗原作为候选疫苗的免疫原性潜力。在Vaxign2中，主要支持对输入蛋白进行 MHC-I 和 MHC-II T 细胞表位预测。作者首先使用了IEDB免疫表位数据库来获得MHC-I和MHC-II T细胞表位。然后基于已知的表位来分析之前预测的保护性抗原是否存在。
我们点击Show Vaxitop Result之后。就到了进行免疫抗原分析的界面。在这个只需要选择想要分析的内容即可。

选择好之后，点击分析。就可以获得分析的结果了。结果主要是通过表格和热图的形式展示的。

2.1 MHC地域多态性分析
由于MHC存在有丰富的多态性，因此一个保护性抗原想要发挥作用也要基于不同的地域进行额外的分析。点击Show Population Coveragee Result即可获得一个全球性的图。

总的来说
以上就是这个数据库的主要功能了。相关的病菌研究，在疫苗设计方面也算是一种科研到临床的转化的过程的。所以如果发现了一个和疾病很相关的病菌倒是可以考虑来进行疫苗设计研究的。
另外，由于在Vaxign2当中还包括了一些之前分析好的数据。所以如果想要研究一个病菌的疫苗的话，倒是可以提前在在分析好的数据当中看看有没有结果的。
至于都分析了哪些病菌，可以在这里查看：http://www.violinet.org/vaxign2/stats

（virsorter和virfinder挖掘病毒序列）西湖大学鞠峰组：环境宏病毒组学分析思路与常用工具

摘要：
本文主要基于已开展的代表性地球病毒组研究，探讨环境宏病毒组的分析思路，然后介绍目前两种主流的病毒重叠群的识别工具VirSorter和VirFinder的使用方法。

环境宏病毒组学分析思路：
 通过以下宏病毒组分析的常规流程（图1），可重构环境样本中病毒基因组草图，进而获取基因组注释、分类学信息、基因组质量、组成与丰度分布、宿主预测等病毒基因组研究所需的关键信息1。
 1）病毒样品采集：首先，研究者需根据研究目的和环境样品类型进行针对性的病毒颗粒采集。针对海洋2, 3、淡水4, 5、温泉6、土壤7, 8、工程系统9, 10（例如生物反应器）等不同类型栖息地病毒样品采集操作流程可参考相关文献。
2）病毒核酸提取：研究者可以根据具体的研究对象来决定提取的核酸类型，即：提取DNA（针对DNA病毒）或提取RNA（正在活跃侵染宿主的DNA、RNA病毒）。目前针对病毒组的核酸提取有两种策略：
i）首先提取微生物群落总DNA或总RNA，然后在后续测序数据处理过程中再去除大量属于细胞生物的序列及其他污染序列4, 11, 12。这种策略没有繁琐的病毒颗粒富集及核酸提取过程，但在后续数据处理过程中可能会丢失部分低丰度的病毒信号。值得注意的是，该策略产生的海量宏基因组和宏转录组测序数据，其重分析是挖掘环境样品中宏病毒组信息的合适选择；
ii) 富集病毒颗粒后再进行DNA或（和）RNA的提取，在后续测序数据处理过程中需要去除的属于细胞生物的序列及其他污染序列将会大大减少2, 3, 13, 14。这种策略需首先经过较为复杂且可能存在一定偏差的病毒富集过程后再进行核酸提取15, 16，但得到的测序数据集病毒信号已经被放大，这对于识别病毒序列更加有利。
3）扩增建库与高通量测序：通过病毒核酸样品的建库与高通量测序得到原始序列（raw reads）。通常能从环境样品中成功提取的病毒核酸总量低（~ng级别），建库测序前还需要对核酸样本进行全基因组扩增。常见的扩增建库策略可参考Rinke et al. 2016 17。
4）质量控制：通过原始序列的质量控制（quality control）、测序接头去除和低质量序列过滤后得到干净序列（clean reads）。
5）组装：通过干净序列的组装得到重叠群（contigs）。在众多的组装软件中，metaSPAdes、MEGAHIT和IDBA-UD被证明具有最好的病毒组组装效果18。
6）识别病毒重叠群：无论使用哪种采样富集或核酸提取策略，得到的序列数据集中均包含大量不属于病毒的信号，因此研究者需要识别来自于病毒的序列信号。目前最常用的识别病毒重叠群（viral contigs）的方法为使用VirSorter软件，同时可结合其他软件的使用（如VirFinder）或直接比对到现有的数据库做辅助验证。这两款常用病毒组研究常用软件的使用方法会在后文详细介绍。


图1 环境宏病毒组分析常规流程示意图

7）恢复病毒类群：鉴于从不同样本中识别的病毒重叠群可能存在序列冗余，且片段化的病毒重叠群可能实际来源于同一病毒基因组，因此在识别病毒重叠群之后还需要进行病毒类群（Viral populations）的恢复。目前主要的病毒类群恢复方法列举如下：
i）根据序列长度和平均核苷酸相似度（average nucleotide identity, ANI）作为参数进行聚类得到病毒类群，例如在80%序列长度ANI达到95%或以上的病毒重叠群将被聚类为同一病毒类群3, 19（尽管这仍然会高估病毒的多样性）；
ii）基于病毒重叠群在样本中的覆盖度（coverage）和四核苷酸频率（tetranucleotide frequencies）进行分箱获得病毒类群2；
iii）对病毒重叠群进行延长、拼接、环化并手动修正组装偏差得到某些在样本中丰度较高的病毒完整基因组4。
与细菌不同，病毒没有一定数量普遍存在的单拷贝标记基因（universal single-copy marker genes），使得病毒类群恢复后，无法根据基因组中标记基因的编码情况判断其完整度和污染度18，目前可参考CheckV软件20基于基因组比对进行病毒基因组完整度和宿主污染情况的评估。
8）病毒基因预测和注释：为了得到恢复的病毒序列的物种分类和功能信息，先通过常规方法（如：使用Prodigal）对病毒重叠群中的基因进行预测，然后将预测得到的基因比对到数据库进行功能注释。常用的数据库均储存了来自病毒的基因信息，如：NCBI RefSeq、GenBank、PFAM、KEGG、UniProt、EggNOG等。由于目前数据库中病毒基因序列的注释信息还远远不足，这使得研究者即使使用非常宽松的比对参数，仍然会得到大量未知功能的病毒基因。目前，病毒类群的分类学信息常常遵循多数原则，即恢复基因组中大部分基因（如基因组中>50%基因）注释到某一病毒科级别（family），该病毒类群则被认为属于此病毒科3, 14。
9）计算病毒类群/病毒基因的相对丰度/表达量：通过将干净序列映射（mapping）回病毒重叠群或病毒基因，计算其在样本中的相对丰度（如为转录组数据则为表达量），得到病毒的群落结构、病毒基因的表达水平等重要信息。
10）宿主预测：病毒组研究的另一重要方面即为病毒的宿主预测。然而目前没有一种全面、准确的方法能够完成宿主预测，一般会选择用几种方法结合使用来进行宿主预测。目前较为常用的病毒宿主预测方法、原理和举例文献列于表1。

表1 目前常用病毒-宿主预测方法、原理与文献举例
宿主预测方法	原理	举例
在潜在宿主基因组中检索CRISPR spacer序列，检查恢复的病毒类群中是否含有一致的序列预测宿主。	宿主的CRISPR系统反映其被侵染的历史。	2, 4, 12
i) 基于病毒基因组中编码tRNA序列的宿主判定宿主。	病毒（噬菌体）具有基因转移的能力，可以从宿主处获得某些基因。通过对这些基因的溯源或系统发育分析，可以追踪到部分病毒的宿主信息。	4, 12, 21
ii)基于病毒基因组中编码辅助代谢基因 (AMG) 序列的系统发育关系分析判定宿主。	2, 4
iii) 基于病毒基因组片段与潜在宿主基因组片段的相似性判定宿主。	2, 21
根据病毒类群和潜在宿主在样本中的丰度、表达水平的关联预测宿主。

	病毒与宿主在样本中的丰度存在共现关联性，此外，由于来自病毒的转录本一定起源于宿主细胞内，因此病毒和宿主的基因表达存在正相关性。	4, 21, 22
根据病毒与潜在宿主之间相近的四核苷酸频率（寡核苷酸频率）预测宿主。	经验主义认为宿主和病毒的基因组有着相近的四核苷酸频率。	2

MIUViG：未培养病毒基因组的最少信息标准
病毒粒子在绝大多数栖息地的数量大大超过活细胞，但仅有极小一部分病毒可在实验室培养。通过上述不依赖培养的高通量测序和宏病毒组数据挖掘，有助于发现前所未有的病毒多样性。目前，领域已在基因组标准联盟框架内制定了未培养病毒基因组（Minimum Information about any (x) Sequence，MIUViG）标准的最少信息1，包括病毒起源、基因组质量、基因组注释、分类信息、生物地理分布和宿主预测。未培养的病毒基因组（UViGs）的大规模重构与解读有助于提高领域对病毒进化历史和病毒-宿主之间相互作用的理解。

VirSorter2使用教程
原网站：https://github.com/jiarong/VirSorter2

简介：
VirSorter是一款能够在基因组数据集中识别病毒信号的软件，是目前病毒组研究中使用最广泛的一款病毒长序列(viral contigs)识别软件，已被引用四百余次。
该软件在2020年新推出了VirSorter2新版本，本教程将基于VirSorter2新版本进行介绍。

安装：
Option1:
conda create -n vs2 -c bioconda virsorter=2 
#conda创建环境vs2并将VirSorter2安装进该环境
conda activate vs2 
#激活vs2环境
Option2:
安装最新更新开发版本，官方推荐。
conda create -n vs2 -c bioconda -c conda-forge \
"python>=3.6" scikit-learn=0.22.1 imbalanced-learn \
pandas seaborn hmmer prodigal screed ruamel.yaml \
"snakemake>=5.16,<=5.26" click 
#conda创建环境vs2并安装指定版本的相关软件至该环境
conda activate vs2
#激活vs2环境
git clone https://github.com/jiarong/VirSorter2.git 
#从github下载源文件
cd VirSorter2 
#进入VirSorter2目录
pip install -e . 
#安装

下载数据库：
在使用VirSorter2之前，用户需下载其数据库和相关文件，目前VirSorter2数据库包含dsDNAphage,NCLDV,RNA,ssDNA,lavidaviridae五大类病毒数据。
rm -rf db 
#注意，如果用户此前取消下载过，则需先运行该命令移除db目录，如之前未失败下载不需要进行此步骤
virsorter setup -d db -j 4 
#正式下载数据库，大约需要10+分钟下载VirSorter2所有的数据库和相关文件，该命令下载内容储存于db目录中。

使用：
Usage：
virsorter run [options] [all|classify]
Options：
all or classify               VirSorter2运行有三个步骤：1)序列预处理，2)提取序列特
征，3)分类。如选择all(默认值)则全部三步骤均运行，如选
择classify则只运行第三步，适合改变参数重新运行分析的
情况。
-w|--working-dir PATH         输出结果路径
-d|--db-dir PATH              数据库路径，安装时路径即为默认路径
-i|--seqfile PATH             输入的序列文件，需为fa或fq格式
-l|--label TEXT               为输出结果文件添加前缀，在使用不同参数重新分析时较有用
--include-groups TEXT         用户需要的病毒类别，可选值有：
dsDNAphage,NCLDV,RNA,ssDNA,lavidaviridae，
多选需以逗号分隔，无空格。默认值为：
dsDNAphage,NCLDV,RNA,ssDNA,lavidaviridae
-j|--jobs INTEGER             最大并行任务数，默认值为256
--min-score FLOAT             被识别为病毒的最小打分值，默认值为0.5
--hallmark-required           在所有序列中均要求有标记基因，默认值为False
--hallmark-required-on-short  在短序列中要求有标记基因，默认值为False。短序列标准在
template-config.yaml中确定，用户可自行更改，默认值为
3kb。该选项可在丢失可接受的敏感度的同时降低识别假阳性
--viral-gene-required         需要有属于病毒的基因被注释，无基因被注释的可能病毒序列
被移除。该选项可在丢失可接受的敏感度的同时降低识别假阳
性，默认值为False
--provirus-off                识别完所有序列后不提取溶原性病毒序列，和--max-orf-
per-seq结合使用可大大提速运行，默认值为False
--max-orf-per-seq INTEGER     该选项仅在--provirus-off模式下可用，后接计算序列分类
特征所需要最大ORF数量，如某序列拥有超过该数量的ORFs，
则对其进行subsample至该数量。默认值值为-1
--min-length INTEGER          进行识别的序列的最短长度要求，默认值为0
--prep-for-dramv              为DRAMv生成病毒序列文件和viralaffi-contigs.tab文
件，DRAMv是一款能够注释VirSorter2输出的病毒序列的软
件，默认值为False
--tmpdir TEXT                 为临时文件命名
--rm-tmpdir                   移除临时文件，默认值为False
--verbose                     显示详细输出，默认值为False
-h|--help                     显示帮助信息

实例：
使用VirSorter2官网给出的实例文件进行举例。
wget -O test.fa \ https://raw.githubusercontent.com/jiarong/VirSorter2/master/test/8seq.fa
#下载示例序列文件
virsorter run -w test.out -i test.fa -j 4 all
#例1:以test.fa作为输入，使用默认下载数据库，使用4个线程，运行VirSorter2全部三步骤进行病毒序列的识别，结果输出至test.out
virsorter run all -w test.out -i test.fa -j 20 \
-l DNA_mins0.7_minl1.5 \
--include-groups dsDNAphage,ssDNA \
--min-score 0.7 \
--hallmark-required-on-short 、
--min-length 1500 \
--prep-for-dramv
#例2:以test.fa作为输入，使用默认下载数据库，使用20个线程，运行VirSorter2全部三步骤进行病毒序列的识别，仅对dsDNAphage和ssDNA病毒进行识别，长度>1.5kb且打分>0.7的结果保留，短于3kb的识别序列必须有hallmark基因，为DRAMv输出相关文件，结果输出至test.out并加上DNA_mins0.7_minl1.5前缀区分

核心输出结果：
VirSorter2运行后会生成三个核心输出文件，分别为：
final-viral-combined.fa #储存识别得到的病毒序列
final-viral-score.tsv #每条序列的各分类类别得分表格
final-viral-boundary.tsv #每条序列的信息表格
核心输出结果的详细描述：
1. final-viral-combined.fa
该fa文件储存VirSorter2识别得到的病毒序列，每条序列的名称为原始序列名称加后缀，后缀有以下三种可能：
i) ||full 代表整条序列都被识别为病毒序列
ii) ||_partial 代表该序列仅有部分区域被识别为病毒序列，其中i可以为0至该序列中找到的病毒片段数量的最大值
iii) ||lt2gene 拥有病毒标记基因的短序列（少于2个基因的序列）
2. final-viral-score.tsv
该表格主要用于进一步筛选识别的病毒序列，其中主要内容包括：
seqname                                     序列名称
dsDNAphage,NCLDV,RNA,ssDNA,lavidaviridae  各类别病毒下的打分
max_score                                   最高分
max_score_group                           最高分所述类别
length                                    序列长度
hallmark                                   标记基因个数
viral                                      病毒基因比例
cellular                                   非病毒基因比例
#需要注意的是，VirSorter在此给出的分类学分类并不可靠，VirSorter2的目的仅限于病毒识别。
3.final-viral-boundary.tsv
该表格主要内容包括：
seqname                                     序列名称
trim_orf_index_start,trim_orf_index_end   该序列被识别为病毒的部分的ORF的起止位置
trim_bp_start,trim_bp_end                 该序列被识别为病毒的部分的碱基的起止位置
trim_pr                                    该序列被识别为病毒的部分的最终得分
partial                                    该序列是否全部被识别为病毒，如果该序列的整
          体得分>cutoff，则该序列整体都被视为病毒序
            列(0)，反之则仅有部分被视为病毒序列(1)
pr_full                                   该序列的整体得分
hallmark_cnt                               标记基因个数
group                                      最高分所述病毒类别

VirFinder使用教程
原网站：https://github.com/jessieren/VirFinder

简介：
VirFinder是另一款常用的病毒长序列(viral contigs)识别软件，该软件不基于现有数据库比对，而是基于病毒和宿主具有不同的k-mer频率特征，使用机器学习的方法来识别病毒序列，常常与VirSorter结合使用。

安装：
Linux系统推荐使用conda安装
conda create -n VF -c bioconda r-virfinder
#conda创建环境VF并将VirFinder安装进该环境
conda activate VF
#激活VF环境
Windows系统需先进入https://github.com/jessieren/VirFinder/blob/master/windows/VirFinder_1.1.zip，下载VirFinder_1.1.zip至本地，然后进入R安装。
install.packages("glmnet", dependencies=TRUE)
install.packages("Rcpp", dependencies=TRUE)
source("https://bioconductor.org/biocLite.R")
biocLite("qvalue")
#安装相关依赖包
install.packages("/VirFinder_1.1.zip", repos = NULL, type="source")
#安装VirFinder，为下载VirFinder_1.1.zip时保存的路径
library(VirFinder)

使用实例：
VirFinder实际为一R包，在R环境中运行分析，在此使用官方提供的序列文件进行实例讲解。
R #进入R，如为Windows系统则直接进入R
library(VirFinder) #加载VirFinder包
inFaFile <- system.file("data", "contigs.fa", package="VirFinder")
#指定需要识别病毒信号的序列文件，此处以官网给出的contigs.fa为例
#用户使用自己的文件只需将此句替换为 inFaFile <- ""
predResult <- VF.pred(inFaFile)
#VirFinder执行病毒序列识别预测
predResult[order(predResult$pvalue),]
#升序排列p-value
predResult$qvalue <- VF.qvalue(predResult$pvalue)
#根据p-values估算q-value
predResult[order(predResult$qvalue),]
#升序排列q-value
write.table(predResult,file = "",sep = "\t")
#将predResult结果输出，为用户指定的输出路径

核心输出：
使用实例中最后输出的结果即为VirFinder核心输出。

结果储存了各条序列的名称、长度、打分、p-value和q-value。
用户可根据打分和p-value进一步筛选得到的病毒长序列。

参考文献：
1.       Roux, S.;  Adriaenssens, E. M.;  Dutilh, B. E.;  Koonin, E. V.;  Kropinski, A. M.; Krupovic, M.;  Kuhn, J. H.;  Lavigne, R.;  Brister, J. R.;  Varsani, A.;  Amid, C.;  Aziz, R. K.; Bordenstein, S. R.;  Bork, P.;  Breitbart, M.;  Cochrane, G. R.;  Daly, R. A.;  Desnues, C.; Duhaime, M. B.;  Emerson, J. B.;  Enault, F.;  Fuhrman, J. A.;  Hingamp, P.;  Hugenholtz, P.;  Hurwitz, B. L.;  Ivanova, N. N.;  Labonte, J. M.;  Lee, K. B.;  Malmstrom, R. R.; Martinez-Garcia, M.;  Mizrachi, I. K.;  Ogata, H.;  Paez-Espino, D.;  Petit, M. A.;  Putonti, C.;  Rattei, T.;  Reyes, A.;  Rodriguez-Valera, F.;  Rosario, K.;  Schriml, L.;  Schulz, F.; Steward, G. F.;  Sullivan, M. B.;  Sunagawa, S.;  Suttle, C. A.;  Temperton, B.;  Tringe, S. G.;  Thurber, R. V.;  Webster, N. S.;  Whiteson, K. L.;  Wilhelm, S. W.;  Wommack, K. E.; Woyke, T.;  Wrighton, K. C.;  Yilmaz, P.;  Yoshida, T.;  Young, M. J.;  Yutin, N.;  Allen, L. Z.;  Kyrpides, N. C.; Eloe-Fadrosh, E. A., Minimum Information about an Uncultivated Virus Genome (MIUViG). Nat Biotechnol 2019, 37 (1), 29-37.
2.       Roux, S.;  Brum, J. R.;  Dutilh, B. E.;  Sunagawa, S.;  Duhaime, M. B.;  Loy, A.; Poulos, B. T.;  Solonenko, N.;  Lara, E.;  Poulain, J.;  Pesant, S.;  Kandels-Lewis, S.; Dimier, C.;  Picheral, M.;  Searson, S.;  Cruaud, C.;  Alberti, A.;  Duarte, C. M.;  Gasol, J. M.;  Vaque, D.;  Tara Oceans, C.;  Bork, P.;  Acinas, S. G.;  Wincker, P.; Sullivan, M. B., Ecogenomics and potential biogeochemical impacts of globally abundant ocean viruses. Nature 2016, 537 (7622), 689-693.
3.       Gregory, A. C.;  Zayed, A. A.;  Conceicao-Neto, N.;  Temperton, B.;  Bolduc, B.; Alberti, A.;  Ardyna, M.;  Arkhipova, K.;  Carmichael, M.;  Cruaud, C.;  Dimier, C.; Dominguez-Huerta, G.;  Ferland, J.;  Kandels, S.;  Liu, Y.;  Marec, C.;  Pesant, S.;  Picheral, M.;  Pisarev, S.;  Poulain, J.;  Tremblay, J. E.;  Vik, D.;  Tara Oceans, C.;  Babin, M.; Bowler, C.;  Culley, A. I.;  de Vargas, C.;  Dutilh, B. E.;  Iudicone, D.;  Karp-Boss, L.;  Roux, S.;  Sunagawa, S.;  Wincker, P.; Sullivan, M. B., Marine DNA Viral Macro- and Microdiversity from Pole to Pole. Cell 2019, 177 (5), 1109-1123 e14.
4.       Chen, L. X.;  Meheust, R.;  Crits-Christoph, A.;  McMahon, K. D.;  Nelson, T. C.; Slater, G. F.;  Warren, L. A.; Banfield, J. F., Large freshwater phages with the potential to augment aerobic methane oxidation. Nat Microbiol 2020, 5 (12), 1504-1515.
5.       Gu, X.;  Tay, Q. X. M.;  Te, S. H.;  Saeidi, N.;  Goh, S. G.;  Kushmaro, A.;  Thompson, J. R.; Gin, K. Y., Geospatial distribution of viromes in tropical freshwater ecosystems.Water Res 2018, 137, 220-232.
6.       Sharma, A.;  Schmidt, M.;  Kiesel, B.;  Mahato, N. K.;  Cralle, L.;  Singh, Y.; Richnow, H. H.;  Gilbert, J. A.;  Arnold, W.; Lal, R., Bacterial and Archaeal Viruses of Himalayan Hot Springs at Manikaran Modulate Host Genomes. Front Microbiol 2018, 9, 3095.
7.       Starr, E. P.;  Nuccio, E. E.;  Pett-Ridge, J.;  Banfield, J. F.; Firestone, M. K., Metatranscriptomic reconstruction reveals RNA viruses with the potential to shape carbon cycling in soil. Proc Natl Acad Sci U S A 2019, 116 (51), 25900-25908.
8.       Jin, M.;  Guo, X.;  Zhang, R.;  Qu, W.;  Gao, B.; Zeng, R., Diversities and potential biogeochemical impacts of mangrove soil viruses. Microbiome 2019, 7 (1), 58.
9.       Wang, Y.;  Jiang, X.;  Liu, L.;  Li, B.; Zhang, T., High-Resolution Temporal and Spatial Patterns of Virome in Wastewater Treatment Systems. Environ Sci Technol 2018,52 (18), 10337-10346.
10.       Subirats, J.;  Sanchez-Melsio, A.;  Borrego, C. M.;  Balcazar, J. L.; Simonet, P., Metagenomic analysis reveals that bacteriophages are reservoirs of antibiotic resistance genes. Int J Antimicrob Agents 2016, 48 (2), 163-7.
11.       Sieradzki, E. T.;  Ignacio-Espinoza, J. C.;  Needham, D. M.;  Fichot, E. B.; Fuhrman, J. A., Dynamic marine viral infections and major contribution to photosynthetic processes shown by spatiotemporal picoplankton metatranscriptomes.Nat Commun 2019, 10 (1), 1169.
12.       Paez-Espino, D.;  Eloe-Fadrosh, E. A.;  Pavlopoulos, G. A.;  Thomas, A. D.; Huntemann, M.;  Mikhailova, N.;  Rubin, E.;  Ivanova, N. N.; Kyrpides, N. C., Uncovering Earth's virome. Nature 2016, 536 (7617), 425-30.
13.       Brum, J. R.; Sullivan, M. B., Patterns and ecological drivers of ocean viral communities. Science 2016, 348 (6237).
14.       Dzunkova, M.;  Low, S. J.;  Daly, J. N.;  Deng, L.;  Rinke, C.; Hugenholtz, P., Defining the human gut host-phage network through single-cell viral tagging. Nat Microbiol 2019, 4 (12), 2192-2203.
15.       John, S. G.;  Mendez, C. B.;  Deng, L.;  Poulos, B.;  Kauffman, A. K.;  Kern, S.; Brum, J.;  Polz, M. F.;  Boyle, E. A.; Sullivan, M. B., A simple and efficient method for concentration of ocean viruses by chemical flocculation. Environ Microbiol Rep 2011, 3(2), 195-202.
16.       Miller, D. N.;  Bryant, J. E.;  Madsen, E. L.; Ghiorse, W. C., Evaluation and optimization of DNA extraction and purification procedures for soil and sediment samples. Appl Environ Microbiol 1999, 65 (11), 4715-4724.
17.       Rinke, C.;  Low, S.;  Woodcroft, B. J.;  Raina, J. B.;  Skarshewski, A.;  Le, X. H.; Butler, M. K.;  Stocker, R.;  Seymour, J.;  Tyson, G. W.; Hugenholtz, P., Validation of picogram- and femtogram-input DNA libraries for microscale metagenomics. PeerJ2016, 4, e2486.
18.       Roux, S.;  Emerson, J. B.;  Eloe-Fadrosh, E. A.; Sullivan, M. B., Benchmarking viromics: an in silico evaluation of metagenome-enabled estimates of viral community composition and diversity. PeerJ 2017, 5, e3817.
19.       <47.patterns and="" ecological="" drivers="" of="" ocean="" viral="" communities.pdf="">.
20.       Nayfach, S.;  Camargo, A. P.;  Schulz, F.;  Eloe-Fadrosh, E.;  Roux, S.; Kyrpides, N. C., CheckV assesses the quality and completeness of metagenome-assembled viral genomes. Nat Biotechnol 2020.
21.       Coutinho, F. H.;  Silveira, C. B.;  Gregoracci, G. B.;  Thompson, C. C.;  Edwards, R. A.;  Brussaard, C. P. D.;  Dutilh, B. E.; Thompson, F. L., Marine viruses discovered via metagenomics shed light on viral strategies throughout the oceans. Nat Commun2017, 8, 15955.
22.       Moniruzzaman, M.;  Wurch, L. L.;  Alexander, H.;  Dyhrman, S. T.;  Gobler, C. J.; Wilhelm, S. W., Virus-host relationships of marine single-celled eukaryotes resolved from metatranscriptomics. Nat Commun 2017, 8, 16054.

Cell重磅：新冠病毒跨物种传播的关键突变被发现

截至2021年7月初，SARS-CoV-2已在全球范围内造成超过1.88亿人感染，至少400万人死亡。研究发现，一些蝙蝠和穿山甲衍生的病毒与SARS-CoV-2的关系更为密切。蝙蝠冠状病毒RaTG13最初于2013年在中国从嗜鼻蝙蝠中分离，在整个基因组中与SARS-CoV-2具有96%的核苷酸同源性。同样，在马来穿山甲中发现的几种病毒与SARS-CoV-2密切相关，S蛋白受体结合域（RBD）氨基酸同源性高达97.4%，这表明SARS-CoV-2为人畜共患病病毒。然而，SARS-CoV-2跨物种传播的确切起源和机制尚不清楚。

2021年7月6日，一篇题为“A selective sweep in the Spike gene has driven SARS-CoV-2 human adaptation”的文章在线发表在Cell上。该研究通过对182,000个SARS-CoV-2基因组进行分析发现：

1、一个重要的碱基突变标签（A1114G），其突变后表达的氨基酸T372A位于S抗原RBD内。
2、T372A突变可以减少病毒表面糖基，从而增强病毒与宿主受体hACE2的结合能力。
3、T372A单一突变是SARS-CoV-2适应人类宿主的关键突变。T372A 突变增强了病毒在人类细胞中的传播能力，并且这种影响比D614G的变化要大得多。

碱基突变标签A1114G的发现
在病毒感染过程中，棘突蛋白在受体识别和细胞膜融合过程中起重要作用。研究者通过比较分析SARS-CoV-2和其他四种Sarbecovirus成员（一种穿山甲冠状病毒和三种蝙蝠冠状病毒）基因组序列，总共确定了六个保守基因位点。值得注意的是，只有一个位点（A1114G；基因组位置22676）位于扫描区域的中心，这在棘突蛋白的密码子位置372内。在人类SARS-CoV-2中，372位置的氨基酸苏氨酸被丙氨酸（Thr372Ala）取代。

对 WT（A372）、T372 和 G614 SARS-CoV-2病毒S 蛋白进行分子建模发现，在 T372 变体中观察到 N370 处 N-连接糖基化的可能性增加，因为苏氨酸突变提供了标准的 N-连接糖基化位点基序（NXT/S）和溶剂可及的表面积N370。G614 的结构分析未表明 S 蛋白结构的任何主要局部修饰，也不接近 RBD。此外，残基 G 614 与糖基化位点 (N616) 非常接近，但与 D614 相比，G614 突变没有改变糖基化的可能性或一般表面特性。

回复突变验证与hACE2 的结合能力
研究者试图使用功能性 ELISA实验验证WT (A372)、A372T 和 N501Y 的 RBD 与hACE2的结合能力。结果显示，N510Y 突变体的 50% 有效剂量（EC50）值（5.83±0.94 ng/mL）低于 WT（12.48±1.26 ng/mL），表明N510Y突变使刺突蛋白对 hACE2 的结合亲和力更强。相比之下，A372T 突变体的 EC50值明显更高（26.29±0.08 ng/mL），表明与 WT 相比，A372T 突变使Spike与 hACE2 的相互作用变弱。总之，这些结果表明 SARS-CoV-2 祖先病毒中发生的S蛋白 T372A 突变增强了与 hACE2 的亲和力。

T372A 突变增强病毒在人类细胞中的传播能力
研究者进一步评估了每种病毒（WT、S A372T 和 S D614G）在 Vero E6 和 Calu-3 细胞系、猴肾和人肺上皮细胞系中的复制动力学。在 Vero E6 细胞中感染后，所有三种病毒的病毒滴度都迅速上升，并且在病毒之间仅观察到峰值滴度的微小差异。在 Calu-3 细胞中，D614G 突变体在感染后 1 天（dpi）产生显着高于WT的滴度，但其余时间点的水平相似。

WT和A372T突变体在感染后24小时未观察到差异，但稍后的时间点显示 A372T 突变体的复制显着减少。与WT相比，D614G在1、2、3和4dpi上的病毒滴度分别为2.9、2.9、1.3和0.8倍；相反，与野生型相比，A372T滴度在1、2、3和4dpi上分别降低1.8、5.5、31.1和64.1倍。这些数据表明，T372A 突变增强了病毒在人类细胞中的传播能力，并且这种影响比D614G的变化要大得多。


小结
这项研究通过分析182000个SARS-CoV-2基因组，发现了一个重要的碱基突变标签（A1114G），其突变后表达的氨基酸T372A位于S抗原RBD内。T372A突变可以减少病毒表面糖基，从而增强病毒与宿主受体hACE2的结合能力。体外回复突变实验显示实验表明：

T372A RBD以更高的亲和力结合hACE2，A372（WT-SARS-CoV-2）相对于其原型T372增强了SARS-CoV-2在人肺上皮细胞的传播能力，T372A对病毒的复制增强效果比D614G突变要强的多。

Science新闻：肠道菌群又添一大功能，揭示你的真实年龄

（随着年龄的增加，与溃疡性结肠炎相关的普通拟杆菌会减少）
众所周知，目前最火的研究领域，莫过于肠道菌群和深度学习，而此项研究的作者，完美的将这两项研究结合，为我们带来一项非常有趣的研究。该研究通过深度学习的算法分析肠道菌群组成来预测人体年龄，误差在4岁以内，说明菌群或许可作为标志物用于衰老相关的研究。
以下为对文章的研究思路与研究结果解读：
先来一张整体思路图，作者的主要思路就是利用深度神经网络的方法，从健康人的肠道菌群里寻找可以预测年龄的细菌物种。

Figure 0
热心肠日报
https://www.mr-gut.cn/papers/read/1099651870
Science新闻：肠道菌群，你的微生物“生命之钟”
原标题：肠道里的细菌可能会揭示你的真实年龄
研究者使用机器学习分析了1165名健康人的3600多个肠道菌群样本，初步锁定了90%样本中均含有的95种细菌；
其中有39种细菌与年龄预测明显相关，算法预测的年龄可精准到4岁以内；
随年龄增加，霍氏真杆菌等变得丰富，而与溃疡性结肠炎相关的普通拟杆菌会减少；
“菌群老化时钟”不仅能预测人体年龄和衰老速度，也可用于研究饮食、抗生素等物质对人体衰老和健康的影响；
这种菌群时钟特征的普遍性，以及其与衰老的因果关系，仍需深入研究。
主编评语：Science上周发表的新闻对bioRxiv上一项预印版的研究进行了报道( https://www.biorxiv.org/content/early/2018/12/28/507780 )，该研究用机器学习算法可通过分析肠道菌群组成来预测人体年龄，误差在4岁以内，说明菌群或许可作为标志物用于衰老相关研究。虽然因果性仍待更多研究阐明，但肠道菌群与衰老之间存在密切关联已无法否认。
摘要
人体肠道微生物组是一个复杂的生态系统，它既影响宿主的生存状态，又受宿主生存状态的影响。前人对肠道微生物组的分析揭示了特定微生物与宿主健康和疾病状态、基因型和饮食之间的关系。在本研究中，基于一个包含1165个健康个体(3663个微生物组样本)的数据集，作者开发了一种基于肠道微生物组来预测宿主生理年龄的方法。此预测模型，可以称之为“人体微生物组时钟”，基于深度神经网络方法，交叉验证的平均绝对误差达到3.94年。“人体微生物组时钟”的预测性能也在其他几个群体数据中得到了验证。此外，作者还开发一个基于重排特征重要性和累积局部效应，对年龄模型中单个微生物特征进行生物学解释的平台。通过此方法，作者得到了两份包含95种与人类年龄相关的肠道生物标志物的清单。进一步研究表明，这个清单可以精简到39个与宿主年龄最想关的细菌物种。整体结果表明：
(a)微生物特征可以用来预测人类年龄;
(b)模型选择的微生物特征与年龄有关。
背景
人类的肠道被数以亿计的微生物所覆盖， 大约包含10^14个细胞， 大大高于宿主细胞的数量。肠道微生物组是一个复杂的生态系统， 具有多种重要功能的有机体。除了作为消化系统的核心部分，微生物还可以调节免疫，控制外源性物质，产生重要的代谢产物，甚至影响神经功能。然而，微生物组影响宿主的特性仅仅是这其中的一面，它们还可以通过信号反馈循环对宿主的信号作出响应。例如，多项研究表明，肠易激综合症（IBD）表现为肠道感染后产生强烈的免疫反应。微生物组对促炎环境的反应是有益菌的数量减少，在这种不利条件下，这些有益菌缺乏生存的能力。反过来，宿主免疫系统会抑制导致产生慢性炎症的致病菌的繁衍。这些变化在每个人的生命中不断发生，可能对我们有害，也可能对我们有益，可以反映出个人选择或更广泛的跨人群影响因素。
宏基因组研究为我们了解肠道微生物如何随着年龄的发展加深了认识。研究发现，肠道定殖主要是发生在出生时与产道细菌的接触。“先锋微生物”主要由兼性需氧菌(如Escherichia，Enterococcus等)构成，并在母乳喂养阶段逐渐被专性厌氧菌(如Bifidobacterium infantis)取代。断奶后，肠道微生物的群落逐渐变得与成人微生物相似。微生物的这些早期定殖阶段是极其重要的，因为正常的婴儿肠道菌群可以促进肠道粘液形成，防止病原体繁殖，调节免疫T细胞。具有非正常的肠道菌群(例如Clostridium和Escherichia属的细菌大量增加)的婴儿更容易患湿疹和食物过敏，这进一步说明肠道菌群早期定殖的重要性。许多因素，如出生方式(顺产或剖腹产)，婴儿饮食(母乳或配方奶粉)和产妇微生物组组成都大大影响婴儿肠道微生物组的发展。
尽管关于婴儿肠道菌群演替研究很多，并且可用于评估各种健康问题的风险。但是成人肠道微生物的演替研究却很少。此外，由于地理位置，病史，饮食，和其他因素的影响，分析成人肠道微生物的演替成比婴儿要困难许多。与年龄相关的人体微生物组研究未能得出肠道菌群衰老的直接理论。一些研究表明，在老年人肠道中生物多样性减少。但是另一些研究则表明老年人肠道中具有和年轻人同样的微生物多样性。有些研究发现在衰老过程中某些特定微生物群系的变化。例如，在老年人中Bacteroides， Bifidobacterium， Blautia， Lactobacilli，Ruminococcus减少，而Clostridium， Escherichia， Streptococci， Enterobacteria增加。然而，由于方法学不同、数据不平衡等问题导致这些发现在不同的研究中差异很大。
在老年人肠道中，通常产短链脂肪酸(short-chain fatty acid, SCFA)的细菌Roseburia和Faecalibacterium会减少，而耐氧菌和致病菌会增加。这些变化会导致失调，进而引起多种老年性疾病的发生。肠道微生物组可能影响衰老的的想法并不新鲜，早在20世纪初，俄罗斯科学家、诺贝尔奖获得者Ilya Metchnikoff就提出有害微生物处理未消化的食物(特别是导致食物腐败的细菌，如Escherichia和Clostridium)可以导致自身中毒。用前处理或者后处理的生物制剂（如乳酸菌制剂)治疗自身中毒，可以有效缓解与年龄有关的身体机能的下降。最近的研究表明，这一具有百年历史的假设很可能是正确的。
目前，想要将肠道微生物组明确分为三个时间状态（幼年，成年和老年）的标准方法还缺少明确的规则。其中，成人微生物组仍然是最大的谜团。它没有确定的继承阶段，如新生儿，也不像老年人那样有明显的梯度变化过程。这就提出了一个问题:正常的成年微生物组究竟是在发育，还是处于停滞状态?考虑到老化过程是渐进的，包括损伤的积累和其他有害变化，假设肠道微生物群落的演替是渐进的似乎很合乎逻辑。然而，想使用微生物组驱动的特性来预测实际年龄有很大的不确定性。采用支持向量机模型将人类宏基因组样本归为年轻或老年，准确率只比随机归类高10-15%。另一项研究试图使用聚类方法，将0-100岁的宿主归类，但是某些特定的类群在年轻和中年人群中变化极大。然而，由于缺乏饮食和生活方式方面的数据，作者无法对肠道菌群的发展形成一个有效的结论。与平均绝对误差(MAE)小于5岁的DNAm年龄预测时钟（DNAm aging clocks）相比，这些方法预测年龄的能力还有很大的改进空间。
深度学习(Deep learning, DL)方法的复兴从2015年开始，主要应用为图像，声音，文字识别，以及一系列的生物医学应用程序，如药物再利用和目标识别。其中一个最重要的例子就是深度学习在生物医学中关于从头分子设计中的应用。在衰老研究的背景下，这些新方法可以结合起来进行老年保护器（geroprotector）的开发。事实上，自2013年以来，基于人类和其他生物模型已研制出许多衰老时钟。已经发表的利用深度学习方法的衰老时钟主要依靠临床血液检测，面部图像，身体活动数据和转录组数据。这些时钟通过排列特性重要性(PFI)，深度特征选择(DFS)和其他一些技术来寻找提升预测准确度的最重要特征。这些时钟也被用来评估各种数据类型的群体特异性。
本研究的目的是基于全基因组测序(WGS)和多项其他数据，使用机器学习的方法，检验微生物群落的演替模式，构建一个年龄预测模型。在本文中，我们基于宿主微生物组构建了一个估计宿主年龄的方法，可以评估在宿主衰老过程中特定微生物类群的重要性，并提供合适的微生物干预措施建议。
研究方法
数据获取
在本研究中，作者只选择了已经公开发表，并可以在ENA和SRA两大数据库中获得的的人类宏基因组测序(WGS)研究项目，在这些项目中，只有有年龄信息的健康个体的样本被纳入研究。这些健康人分别来自于澳大利亚、中国、丹麦、法国、德国、哈萨克斯坦、西班牙、瑞典和美国，年龄介于20到90岁之间。最后，总共有来自于1165个健康人的3663份样品被整合到分析之中。数据分布如Figure 1所示:
神经网络训练
Neural networks training
回归
Regression
所有的深度神经网络(DNNs)分析都是使用Python 3.6 Keras库中的Tensorflow实现的。利用包含1673个微生物类群的物种特征的完整数据来训练特征选择模型。在后续所有分析中，训练集和验证集分别包含所有数据集的90%和10%。分别构建了两个回归器:一个使用来自单个样本的分类特征数据(基于样本的模型)，另一个使用来自同一宿主所有样本的平均值的分类特征数据(基于宿主的模型)。采用五倍交叉验证的方式训练回归器模型。在对各模型配完成网格搜索后，根据R^2的最大值选择性能最好的模型。采用基于样本的模型确定最好的模型结构，共包括3个隐藏层，每层包含512个节点，PReLU激活函数、Adam优化器，每层dropout率为0.5，学习率为0.001（Figure 2）。相同的结构参数用于基于宿主的模型中。使用Python库中的XGBoost梯度提升方法来验证来自基于样本的DNN模型的特性的重要性。最优的XGBoost模型使用如下参数p: linear_nthread = 35， max_depth = 6， max_delta_step = 2， lambda= 0， gamma=0.1， eta=0.1， alpha = 0.5。XGBoost模型的表现采用MAE评估.

Figure 2
分类
Classification
年龄分类器模型分别使用95个特征或39个特征的子集进行训练。训练和验证分别包含所有数据集的80%和20%。使用Python Keras库的Tensorflow后端实现年龄分类器的训练。加权的F1评分作为评价模型性能的指标。Figure 3展示了性能最优的结构。对于包含95个特征的分类器，3个隐含层分别包含128、32、8个节点，dropout率0.5，隐含层为PReLU激活函数，输出层为softmax激活函数54。对于包含39个特征的分类器，2个隐含层分别包含64和8个节点，dropout率0.5，隐含层为PReLU激活函数，输出层为Softmax激活函数。

Figure 3
过采样
Oversampling
在建立年龄预测模型时，为解决分类不平衡的问题，我们采用了过采样的方法。使用Python库Somoclu模块为每个年龄组构建了基于零一特征(如果在样本中检测到一个分类单元，则为1，如果没有检测到，则为0)的自组织映射（Self Organizing Maps; SOM)。每个SOM由放置在环形栅格上的100个格组成。为了使代表不足的分类产生合成特征，代码本向量（codebook vectors）是随机选择的，并根据映射到它们的最佳匹配单元(BMUs)的数量进行替换。代码本值用作将某个物种包含到假样本中的概率。然后Fake presence/absence特征乘以相应BMUs的平均丰度向量并进行归一化。
特征重要性
Feature importance
为了评估单个特征的重要性，我们采用了重排特征重要性(Permutation feature importance， PFI)技术。PFI通过对单个特征向量的排列来测量预测质量的变化(R2评分降低)。质量信号下降越多，该特征越重要。这些被认为是最重要的特征进一步采用累积局部效应(ALE)的方法评估，以确定微生物物种丰度发生微小变化时年龄预测的变化。ALE方法采用如下算法实现：对所选的95个物种，每一个物种组成一个分位数值表(5%步展)。每个分位数的局部效应(LE)是通过测量预测的平均变化来计算，这些预测方面的变化是通过左右边界值代替观测特征的丰度实现的。每个分位数的ALEs是通过将前面所有的LEs相加并以结果为中心使每个分类单元的平均效应为零来计算的。
研究结果
机器学习预测年龄
为了研究人类肠道分类特征与实际年龄之间的关系，我们从10个公开数据集中收集了1165个健康个体(总共3663个样本)的完整元基因组序列。在我们的数据集中，所有人的年龄都在20到90岁之间，平均年龄为46岁。将3663份样本随机分为训练集(90%)和验证集(10%)，利用1673种微生物的相对丰度向量，训练深度神经网络回归器预测宿主年龄。最佳模型的MAE为3.94年，R2为0.81(Figure 5A)。然后我们将样本分为20-39岁、40-59岁和60-90岁三个年龄组，发现模型生成的预测年龄分布与实际年龄分布非常吻合(Figure 6)。

Figure 5

Figure 6
为了验证DNN的结果，我们还采用了随机森林、支持向量机和弹性网络回归方法进行验证。与DNN方法相比，所有这些方法的效果都很差（平均绝对误差超过11年）。除此之外，我们还训练了一个与DNN模型(MAE = 4.69 years， R2 = 0.81)精度相当的梯度增强(gradient boost， XGB)回归器(regression)(Figure 5B)。这两种方法都倾向于预测中位年龄-46岁(Figure 6)。由于地理位置或饮食类型的不同，尽管在分类特征中存在某些差异，但所描述的预测因子同样适用于来自不同人群的成年人(见补充材料)。
微生物对年龄预测的影响
利用重排特征重要性(PFI)，我们评估了哪些类群的丰度在微生物年龄预测中发挥了最大的作用。我们确定了95个物种，这些物种使XGB和DFS模型的R2评分降低了>0.001(Figure 7)。从PFI评分来看，DNN回归器对高度丰富的物种更为敏感，而XGB回归器最重要的特征中包含了一些小类群。与其他方法相比，这表明DNN具有更强的稳定性。95个分类单元的得分值、丰度和富集度的完整列表见补充表1。

Figure 7
为了描述这95个特征是如何影响年龄预测的，我们使用了累积局部效应(ALE)方法(Figure 4)。ALE方法测量回归因子对特定类群丰度变化的响应。每个特性的ALE的计算仅使用丰度> 1e-5的特征值计算。一些微生物物种随着年龄的增加其数量稳步增加(如[Eubacterium] hallii); 其他微生物则与预测年龄(如Bacteroides vulgatus)呈负相关(Figure 8)。有趣的是，之前被PFI鉴定为重要的某些微生物对预测年龄(如[Eubacterium] rectale)影响很小(Figure 8)。

Figure 4

Figure 8
使用ALEs，所有特征均可分为seno-positive(单调递增ALE图)、seno-negative(单调递减ALE图)和更复杂的类群(非单调情况)(Figure 9)。在95个物种中，只有39个物种在5%-95%分位数范围内显示预测年龄大于1岁的平均变化，其中21个为seno-positive，15个为seno-negative，3个为非单调。

Figure 9
DNN预测年龄
尽管在包含所有分类群的数据集上，DNN和XGB回归器均表现出可接受的精度，但是在训练过程中，当特征的数量下降到100以下时，会产生表现不佳的模型(MAE> 11年)。为了估计95个和39个标记物种(Figure 9)，我们应用一个更容易的任务年龄段预测。所有样品按年龄被分为三组:年轻(20-39岁，占32%)，中年(40-59岁，占41%)和老年(60-90年，占27%)。代表不足的分类采用过采样方法(见方法)。
在这种设置下，表现最佳DNN模型表现出比随机分配年龄组更优的预测精度。随机模型的平均加权F值不超过38±1%，而95个标志物种的模型的F值达到67±4%。使用ALEs方法将标志物种降低到39个时，F值仅降低了5%(62±3%)。我们还比较了使用ALEs筛选的39个标记物种集所构造的分类器与随机选择39个物种所构建的分类器的优劣。结果显示随机产生的100个数据集没有产生一个与ALEs选取物种(38±3%)一样好的分类器。(Figure 10)

Figure 10
基于宿主的年龄预测模型
尽管DNN已经很准确，但是由于数据不足，在训练过程中每个样本均被视为独立的样本。通过将来自于同一个宿主的数据平均化可以消除存在的数据污染问题，这使得样品总数减少到1165个。基于宿主的年龄预测模型采用基于样品的年龄预测模型所得到的最优参数进行配置(Figure 2)。结果显示：基于宿主预测年龄的模型准确性要低于基于样品的年龄预测模型，MAE为8.56年(Figure 11)。但是该模型仍然好于基线年龄分配(MAE=12.47年)。有趣的是，男性和女性标本得到相同的准确性，并且预测的肠道年龄与BMI呈正相关(r = 0.23)，这一点符合现有的数据中BMI和生物年龄之间的关系，但是，这种相关性低于样本BMI与实际年龄:r = 0.3。

Figure 11
结论
本研究证明采用机器学习方法使用人体肠道菌群特征预测年龄是可行的，最优的DNN模型的MAE仅为3.94年。此模型与前人的年龄预测模型具有可比性，例如图像预测年龄模型（PhotoAgeClock）的MAE为1.9，最优的甲基化年龄预测模型（methylation aging clock）的MAE为2.7，转录组年龄预测模型（transcriptomic aging clock）的MAE为7.8，血液年龄预测模型（hematological aging clock）的MAE为5.5。此外，本研究还开发了一个微生物特性选择和注释的方法，此方法基于DNN训练模型，采用PFI和ALE两种方法对重要性进行评估。这种方法可以同时选取最相关的特征作为生物标志物并且量化其对目标变量的影响，例如本文中的年龄。基于这种方法，我们发现95个和39个原核生物类群的生物标志物可以很好的预测肠道年龄。尽管当将整个物种群包含进模型中时，预测能力降低，但是它可以准确（86%）将数据分配到三个年龄组(年轻、中年和老年人)，比随机分类模型更准确(F值分别为0.71和0.34)。
本文所确定生物标志物（物种）的丰度，有些与年龄正相关，有些与年龄负相关，。这些物种在将来研究中应该得到更多的关注，将为我们进一步理解人类衰老与肠道微生物的关系提供帮助。
Science新闻介绍
我们知道数以亿计的细菌以我们的肠道为家，它们可以调节我们消化食物的能力，也可以调节我们免疫系统的功能。尽管肠道菌群与疾病的相关研究已经很多，但科学家们对这个被称为微生物组的系统是如何随时间变化的却知之甚少，甚至不知道一个“正常”的系统应该是什么样子。现在，研究人员对全球数千健康人的肠道细菌进行了研究，并得出结论:微生物组是一个非常精确的生物钟，能够预测大多数人的年龄并且预测误差仅为4年。
为了发现微生物群是如何随着时间变化的，研究人员亚Alex Zhavoronkov和他的同事们对全球1165名健康人的3663多个肠道细菌样本进行了研究。在这些样本中，大约三分之一的人年龄在20岁至39岁之间，三分之一的人年龄在40岁至59岁之间，三分之一的人年龄在60岁至90岁之间。然后科学家们用机器学习的方法来分析数据。首先，他们采用90%的样本对深度学习的算法进行训练，结果发现有95种不同的细菌可以很好的预测人的年龄。然后，他们算采用剩余10%的样本对深度学习算法的准确度进行验证，结果发现此算法可以很好的预测人的年龄，误差在4岁以内。进一步的研究发现，在95种细菌中，39种细菌是预测年龄最重要的。
Zhavoronkov和他的同事们发现，随着人们年龄的增长，一些细菌增加了，例如Eubacterium hallii，此菌被认为对肠道的新陈代谢很重要，而有些细菌则减少了，例如Bacteroides vulgatus，通常这种细菌与溃疡性结肠炎(消化道的一种炎症)有关。哈佛大学研究衰老问题的生物学家（也是此论文共同作者之一）Vadim Gladyshev说，饮食、睡眠习惯和体育活动的变化可能是细菌种类变化的原因之一。Zhavoronkov说，这种“微生物组年龄时钟（microbiome aging clock）”可以作为基线，用来测试一个人肠道衰老的快慢，以及酒精、抗生素、益生菌或饮食是否对寿命产生影响，也可以用来比较健康人与某些患有疾病（例如阿尔茨海默氏症）的人的肠道菌群，看看他们的微生物群是否偏离正常。如果这个想法被证实，它将与其他生物标记（包括端粒的长度以及DNA表达的变化）一起被科学家用来预测生物年龄。将这种新的年龄时钟与其他生物标志物结合起来，将能更准确地展示出一个人的真实生理年龄和健康状况。它还可以帮助研究人员更好地测试某些干预（包括药物和其他治疗）是否对衰老过程有影响。
计算机科学家、微生物组研究专家、加州大学圣地亚哥分校微生物组创新中心主任Robin Knight表示，通过肠道微生物组来预测一个人的年龄的想法“非常合理”，而且对研究衰老的科学家“非常有吸引力”。目前他的团队也正在分析来自美国肠道项目(American Gut Project)的1.5万个样本，该项目是他创立的一项全球微生物学研究，目的是开发类似的年龄预测因子。但Knight也补充说，开发这样一个时钟的挑战之一是，世界各地的人的肠道中的细菌存在着巨大的差异，在明显不同的人群中重复这类研究更为重要，可以发现不同人群中是否存在不同的年龄预测信号。目前还不清楚是微生物组的变化导致人们衰老得更快，还是这些变化仅仅是衰老的副作用。目前，In Silico Medicine正在基于机器学习构建几个可以与微生物组相结合的年龄时钟。Zhavoronkov说：“我们每一秒都在改变，年龄是所有疾病的一个重要参数。”
Reference
Fedor Galkin，  Alexander Aliper，  Evgeny Putin， Igor Kuznetsov，  Vadim N Gladyshev，  Alex Zhavoronkov. 2018. Human microbiome aging clocks based on deep learning and tandem of permutation feature importance and accumulated local effects. bioRvix doi: https://doi.org/10.1101/507780
Emily Mullin. 2019. The bacteria in your gut may reveal your true age. Science 01-11 http://www.sciencemag.org/news/2019/01/bacteria-your-gut-may-reveal-your-true-age
热心肠日报——Science新闻：肠道菌群，你的微生物“生命之钟” https://www.mr-gut.cn/papers/read/1099651870

值得思考，机器学习模型做出的决策是你想要的吗？

区分预测模型和分类模型是很重要的一个事情。在很多决策应用中，分类模型代表着一个“不成熟”的决定，它组合了预测模型和决策制定，但剥夺了决策者对错误决定带来的损失的控制权 (如随机森林中的服从大多数原则，51棵树预测为患病49棵树预测为正常与91棵树预测为患病9棵树预测为正常返回的结果都是患病)。如果采样标准或损失/收益规则 (在预测疾病时，更看重敏感性而非假阳性)发生改变，分类模型也需要相应的改变。而预测模型是与决策分开的，可用于任何决策制定。
分类模型适用于频繁发生的非随机性(或者说确定性)的结果，而不适用于两个个体有同样的输入而输出却不同的情况。对于后者，模型的趋势（比如概率）则是关键因素。
分类模型的适用条件：
分类结果很不同
分类变量有很强的分类能力，可以在接近概率为1的情况下预测出其中一个分类结果
机器学习这一领域在某种程度上独立于统计学领域。因此，机器学习专家往往不强调概率思维。概率思维和理解不确定性和波动性 (variation)是统计学的重要特征。顺便说一下，关于概率思维最好的书之一是Nate Silver的The Signal and The Noise: Why So Many Predictions Fail But Some Don’t。在医学领域，David Spiegelhalter的《患者管理和临床试验中的概率预测》(Probability Prediction In Patient Management and Clinical Trials)是一篇经典论文。
摒除概率思维后，机器学习提倡频繁使用分类器，而不是使用风险预测模型。情况已经变得有些极端:许多机器学习专家实际上把逻辑回归 (logistic regression)列为一种分类方法(其实不是)。我们现在需要认真思考：分类真正意味着什么。分类实际上是一种决策。最佳决策需要充分利用现有数据来进行预测，并通过最小化损失函数/最大化效用函数来做出决策。不同的终端用户有不同的损失函数/效用函数 (在预测疾病时，如更看重敏感性，还是假阳性)，进而有不同的决策风险阈值。分类模型则假设每个用户都有相同的效用函数，就是分类系统所用的效用函数。
分类通常是一种被迫的选择。比如在市场营销中，广告预算是固定的，分析师通常还没有笨到直接使用模型把潜在客户归类为需要忽略的人或需要花费资源进行投放的人。相反，他们对概率进行建模，根据潜在客户购买产品的估计概率对其进行排序绘制一个Lift曲线。为了获得“最大的效果”，营销人员会选择n个可能性最高的客户作为目标进行广告投放。这是合理的，而且不需要分类。
模型使用者(如医生)经常提出的一个观点是，最终他们需要做出二元决策 (binary decision)，因此需要进行二元分类。而事实并非如此。首先，通常情况下，当预测出患病的概率是中等时，最好的决定是不做决定;去收集更多数据。在许多其他情况下，决定是可撤销的，例如，医生开始给病人低剂量的药物，然后决定是否改变剂量或更换药物。在外科治疗中，动手术的决定是不可改变的，但何时动手术取决于外科医生和病人，并取决于疾病的严重程度和症状。无论如何，如果需要进行二元分类，必须在所有情况都考虑到时，而非在数据建模时。
什么时候强制做出选择是合适的?我认为需要考虑这个问题是机械的 (确定性的)还是随机/概率的。机器学习的提倡者经常想把为前者 (机械性问题)所做的方法应用到存在生物变异、抽样变化和测量误差的问题上。而实际上最好是将分类模型仅仅应用于高信噪比的情况下，比如有一个已知的黄金标准，可以重复实验，每次得到几乎相同的结果。模式识别就是一个例子:
视觉、声音、化学成分等。如果创建一个光学字符识别算法 (OCR),该算法可以被任意数量的样品进行训练并尝试把图像分类为字母A, B,……等。这样一个分类器的用户可能没有时间来考虑每个分类是否足够可信。但这种分类器信噪比是极高的。此外，每个字母都有一个“正确”答案。这种情况主要是机械性或非随机性的结果。而预测死亡或疾病时，两个症状相同的患者却很容易有不同的疾病发展方向。
当预测概率居中时，或者当结果有固有的随机性时，就需要进行概率估计。概率的一个优点是，它们是自己的错误的度量。如果预测疾病发生的概率是0.1，而当前的决定是不进行治疗；这个决定犯错的概率也是0.1。而如果发病概率是0.4，这会促使医生进行另一次实验检测或或采用活检等其它检测方式。当信噪比较小时，分类模型通常不是一个好的应用方式; 而是需要对趋势也就是概率进行建模。
美国气象局一直用概率来预测降雨。我不想得到一个分类结论“今天要下雨”。而是想着是否带伞应该由我来根据下雨的概率权衡后作出决定。
无论是从事信用风险评分、天气预报、气候预测、市场营销、病人疾病的诊断，还是评估病人的预后，我都不想使用分类的方法。而是希望获得带有可信区间或置信区间的风险估计得分。我的观点是，机器学习分类器最好用于机械的/确定性的高信噪比的数据或应用场景中，而概率模型应该用于大多数其他情况。
这与许多分析师忽略的一个微妙问题有关。复杂的机器学习算法可以通过进行高阶交互等处理问题的复杂性，但在信噪比较低时需要大量的数据。基于可加性假设的回归模型(当它们是正确的时，它们在绝大多数情况都是正确的)可以在没有大量数据集的情况下产生准确的概率模型。当被预测的结果变量有两个以上的水平时，一个回归模型可以获得各种感兴趣的量，如预测均值、分位数、超标概率 (exceedance probabilities)、瞬时危险率 (instantaneous hazard rates)等。
(机器学习中的问题，不平衡的训练集会获得奇怪的分类器)
分类模型的一个特殊问题也反映了这样一个重要概念。使用机器学习分类模型的用户都知道，一个高度不平衡的样本训练集会获得一个奇怪的二元分类器。例如，如果训练集中有1000名患者和100万名非患者，那么最佳分类器可能将每个人都划分为非患者;这样获得的正确率是0.999。出于这个原因，对数据进行子集抽样的奇怪做法被用来平衡训练集中样本的频率，从而产生看起来合理的分类器 (回归模型的用户永远不会为了得到答案而排除好的数据)。然后，他们必须以某种不明确的方式构造分类器，以弥补训练集中样本组成的偏差。很简单，一个基于发病率为1/2的情况训练的模型将不能应用于发病率为1/1000的新数据的预测。分类器必须在新的样本上重新训练，检测到的模式可能会发生很大的变化。另一方面，Logistic回归巧妙地处理了这种情况，要么(1)将导致患病率如此之低的变量作为预测变量，要么(2)只需要重新校准另一个发病率高的数据集的截距。分类器对发病率的极端依赖可能足以使一些研究人员总是使用概率估计，如logistic回归进行代替。人们甚至可以说，当结果变量的变化很小时，根本不应该使用分类器，而应该只对概率建模。
选择一种方法的关键因素之一是它应该具有正确统计属性的敏感的准确性评分规则。机器分类的专家很少有了解这一极其重要问题的背景，选择一个不正确的准确性得分，如正确分类的比例，将导致一个虚假的模型。这里对此进行了详细讨论。
References
https://www.fharrell.com/post/classification/

Nature：宏基因组研究发现了神秘古老新成员——Asgard archaea

真核生物的起源和细胞复杂性是生物学中的一个主要谜团。目前的数据支持一种情况，即一个古菌细胞和一个变形细菌（线粒体）内共生合并在一起，形成了第一个真核细胞。宿主细胞与Lokiarchaeota有关，Lokiarchaeota是一个具有许多真核生物特征的古菌门。在这里，我们描述了“阿斯加德”超级门，一组未探究的古菌，以及Lokiarchaeota，包括Thor-、Odin-和Heimdallarchaeota。阿斯加德古菌与真核生物在系统发育分析中关联，它们的基因组被富集到以前被认为是真核生物特有的蛋白质中。值得注意的是，基因组编码真核膜转运元件的几个同系基因，包括Sec23/24和TRAPP结构域。此外，我们还鉴定了具有与真核生物相似特征的蛋白参与囊泡生物发生的外套蛋白。我们的结果扩大了已知的“真核生物特异性”蛋白库，表明古生物宿主细胞已经包含了许多关键基因控制真核细胞复杂性的成分。

真核细胞的起源被认为是我们星球生命史上的主要进化创新之一。然而，真核生物的复杂和分割性质的出现是现代生物学中的一个主要难题。最近的见解支持了真核生物进化的共生情景，第一个真核细胞的出现是估计宿主细胞和变形细菌（线粒体）内共生体之间的合并而成。虽然线粒体的变形细菌来源是压倒性的，但直到最近，古菌宿主细胞的身份和性质仍然难以捉摸。通过鉴定在深海沉积物中发现的一种古细菌Lokiarchaeota基因组，为深入了解真核生物的考古学起源提供了几个关键的见解。首先，对精心选择的基因组数据集进行系统发育分析，将Lokiarchaeota作为与真核生物最密切相关的群体，为真核生物从古菌域内分支提供了进一步的有力证据。此外，对合成的Lokiarchaeum基因组的基因进行了仔细的分析，发现它编码了大量关于真核生物产生的基因。

在这里，我们描述了亚斯加德超级门，包括几个新的未探索的古菌，其生存在各种各样的环境中。我们发现这些谱系包含了新的门级群体，代表了真核发生的初级阶段。为了深入了解古细菌到真核生物的转变，我们的目的是识别和描述与最近描述的Lokiarchaeota有关的新的古菌谱系，这是一种古老的类群，以前证明与真核生物比任何其他原核谱系更密切相关。我们从七个地理位置上分离的地点水体沉积物进行了取样。从所有样本中提取总DNA并进行测序，共产生644.88gigabase对(GBP)的序列。序列组装总共产生了3.28GBP的连续序列(Contigs)≥5kb。为了评估潜在的Lokiarchaeota相关谱系的存在，包含至少6个保守的15核糖体蛋白(RP15)基因的连体。我们决定把这个古老的克劳德·阿斯加德命名为北欧神话中的神域。除了Lokiarchaeota和Thorarchaeota相关古菌外，我们还可以定义另外两个候选在阿斯加德家族中的门，我们称为Odinarchaeota和Heimdallarchaeota。为了分析这些新的阿斯加德谱系的基因组含量和进化历史，在四核苷酸序列频率的基础上，将连体组合成宏组组装基因组和DNA序列覆盖模式。我们重建了近完整的基因组箱代表每个主要阿斯加德古菌谱系。

接下来，为了进一步了解真核基因含量，我们在重建的阿斯加德基因组中鉴定了潜在的真核蛋白(ESP)。值得注意的是，所有的阿斯加德古菌都被发现这样的ESP丰富，表明这些是广泛分布于阿斯加德超级门。值得注意的是，在Heimdallarchaeote AB_125和Odinarchaeote LCB_4的基因组中，编码ESCRT和泛素修饰系统成分的基因在同一个基因簇中共同组织。这些系统可能参与ESCRT介导的蛋白质降解，这是一种以前仅在真核生物中知道的途径。不过，事实上泛素修饰系统不存在于Thorarchaeal基因组中，这表明这些ESCRT蛋白可能与替代功能有关，或者这些后古菌具有不同的机制。我们在Asgard系中发现了几个新的真核生物特征，这些特征以前没有在Lokiarchaeum基因组中发现。
在本研究中，我们采用了一种宏基因组学方法来揭示未培养的古菌谱系的存在，这些谱系与最近描述的Lokiarchaeota有很远的关系。我们已经证明了这些古菌形成了一个候选的超级门，被指定为阿斯加德古菌，在系统发育分析中与真核生物紧密关联。阿斯加德古菌编码许多导致真核细胞复杂性出现的关键成分，包括真核蛋白同系物参与细胞骨架功能和囊泡形成和转运。

Reference:
Zarembaniedzwiedzka K, Caceres E F, Saw J H, et al. Asgard archaea illuminate the origin of eukaryotic cellular complexity.[J]. Nature, 2017, 541(7637):353

科研| ISME：肺癌患者肠道菌群的独特组成和代谢功能与恶病质有关

该文章对31名肺癌患者的血浆代谢组、肠道细菌分类和功能进行了深入分析，方法是将非靶向代谢组学（untargeted metabolomics）应用于患者血浆样本和鸟枪法宏基因组（shotgun metagenomics）来收集粪便样本，研究表明：特定代谢物、肠道微生物种类及其代谢途径与恶病质状态相关。随后本文整合了分类、功能特征以及代谢组学数据以便全面了解肠道微生物在恶病质中的作用。此外，本研究还开发了一种针对恶病质和非恶病质患者的机器学习分类器，考虑了微生物群特征的组合效应，并进一步支持了肠道微生物群的假定作用。本文的研究目的是识别与恶病质相关的微生物群，为这一影响癌症治疗结果的关键疾病的新的治疗选择开辟道路。

摘要：恶病质（Cachexia）与癌症患者的存活率下降有关，患病率高达80%，其病因主要是了解程度有限和治疗方案具有局限性。本研究通过整合31例肺癌患者的鸟枪法宏基因组学（shotgun metagenomics）和血浆代谢组学（plasma metabolomics）揭示了人类肠道微生物在恶病质中的作用。与非恶病质患者相比，恶病质组在肠道微生物组成、宏基因组的功能通路和相关血浆代谢物方面存在显著差异。恶病质患者血浆中支链氨基酸（Branched-chain amino acids，BCAAs）、甲基组胺（ methylhistamine）和维生素明显减少，这一规律也体现在相关肠道微生物群功能通路的消耗上。非恶病质患者体内BCAAs和3-oxolic acid的富集量与肠道微生物Prevotella copri和加氏乳杆菌Lactobacillus gasseri分别呈正相关。此外，在恶病质患者中，肠道微生物群脂多糖生物合成（lipopolysaccharides biosynthesis）能力显著提高。在一个单独使用肠道微生物特征的高性能机器学习模型（high-performance machine learning model）中，我们进一步观察到肠道微生物与恶病质的相互作用。我们的研究证明了在临床环境中，恶病质宿主代谢与特定肠道微生物种类和功能之间的联系，表明肠道微生物群可能对恶病质产生影响，并具有可能的治疗应用。

原名：Distinct composition and metabolic functions of human gut microbiota are associated with cachexia in lung cancer patients
译名：肺癌患者肠道菌群的独特组成和代谢功能与恶病质有关
期刊：The ISME Journal
IF：9.180
发表时间：2021.5.17
通讯作者：Gianni Panagiotou
通讯作者单位：德国莱布尼茨天然产物研究和感染生物学-Hans Knöll 研究所、中国香港大学生物科学院
DOI号：10.1038/s41396-021-00998-8

神经网络简介

Artificial Neural Network, 缩写ANN,  简称为神经网络，在机器学习，尤其是深度学习领域广泛应用。
神经网络采用了一种仿生学的思想，通过模拟生物神经网络的结构和功能来实现建模。神经元细胞结构如下

在两侧分布着树突和轴突两种结构，树突用于接受其他神经元传递的信号，而轴突用于向其他神经元传递信号，信号在多个神经元之间传导，构成了神经网络。许许多多的神经元细胞构成了神经中枢，用于对刺激作出响应。
借鉴神经元这一生物结构，1943年MoCulloch和Pitts提出了人工神经元模型，即M-P神经元模型，结构如下

input层的不同信号，首先通过一个线性加和模型进行汇总，每个信号有一个不同的权重，然后通过一个激活函数来判断是否需要进行输出。激活函数可以有多种形式，部分激活函数展示如下

激活函数与线性组合的关系表示如下

其中的θ表示阈值，ω表示权重，在MP神经元模型中，权重和阈值是固定值，是一个不需要学习的模型。
为了让机器具备学习的能力，在MP神经元模型的基础上，提出了最早的神经网络模型， 单层感知器perceptron，结构如下

是一个两层的神经网络，第一层为输入层，第二层为输出层。因为只有在输出层需要进行计算，就是说只有一层计算层，所以称之为单层感知器。从形式上看，仅仅是将MP模型中的输入信号当作了独立的一层神经元，但是本质上却有很大差别。
感知器模型中权重和阈值不再是固定的了，而是计算机"学习"出来的结果。引入了损失函数的概念，通过迭代不断调整权重和阈值，使得损失函数最小，以此来寻找最佳的权重和阈值。
单层感知器只可以解决线性可分的问题，在单层感知器的基础上，再引入一层神经元，构成一个3层的神经网络，结构如下

这样的一个神经网络模型，适用范围更广，涵盖了线性和非线性可分的场景。其中的每一层称之为layer, 除了输出层和输出层之外，还有中间的隐藏层。这样的神经网络模型，通过反向传播算法来求解。
增加一层的好处在于更好的数据表示和函数拟合的能力，在3层的基础上，再引入更多的隐藏层，就变成了深度神经网络，图示如下


可以看到，每增加一层，模型的参数数量急剧增加，所以深度学习对计算资源的要求特别高，在实际使用中，模型训练时间非常久。
虽然耗费计算资源，但是深度学习的优点也很突出，相比机器学习，模型自动完成特征提取，不需要人工的特征工程，这一点对于高维数据的处理特别重要.
由输入层，隐藏层，输出层这3种典型结构组成的神经网络统称为前馈神经网络，通过反向传播算法来迭代更新参数。
除此之外，还有卷积神经网络，循环神经网络，生成对抗网络等多种变种，在计算机视觉，自然语言处理，图像生成等领域，各自发挥着重大作用。

深度学习基本概念｜激活函数 Activation Function

人工神经网络由多层神经元构成，对于单个神经元而言，首先对接收到的输入信号进行线性组合，示意如下

但是在输出信号时，会使用一个称之为激活函数的函数对线性组合的信号进一步处理。激活函数是一种非线性函数，由多种具体的函数表达式。
为何一定需要激活函数呢？如果没有激活函数的话，神经元的信号处理本质上就是一个线性组合，即使叠加再多层的神经元，整个神经网络也还是线性组合，这样就不能解决非线性的问题，所以激活函数的作用，是为神经网络引入非线性组合的能力，使其可以适用于复杂的应用场景。
激活函数由多种，下面介绍几种常用的激活函数
1. Sigmod
就是逻辑回归中的sigmod函数，函数图像如下

取值范围为0到1，sigmod是最常用的激活函数之一，作为最早使用的激活函数，在神经网络发展的早期最为常用，但是该函数存在着以下两个缺点。
神经网络通过基于梯度下降的反向传播算法来训练参数，在反向的过程中，需要借助链式法则来计算梯度

梯度消失的基本原理
对于sigmod函数而言，其导函数的取值都很小，导函数的图像如下

导数的最大值为0.25，这样在链式法则的连乘过程中，会使得梯度很快趋近于0，这种现象称之为梯度消失kill gradients。研究表明，对于5层以上的神经网络，特别容易出现梯度消失，所以sigmod函数对于层数很多的深度神经网络效果不太好。
其次，sigmod函数的输出值恒大于0，即not zero-centered, 这样在梯度下降的过程中只能按照下图所示的路径来收敛

收敛速度较慢，神经网络参数非常多，收敛速度慢会大大增加模型的训练时长。
2. Tanh
Tanh函数的图像如下

相比sigmod函数，tanh函数输出范围为-1到1，是zero-centered, 所以收敛速度有所提高。
但是本质上该函数其实是sigmod的变体

所以与sigmod函数一样，该函数也会出现梯度消失的问题。
3. ReLU
ReLU是Rectified  Linear Unit的缩写，简称修正线性单元，函数表达式如下

函数图像如下

ReLU函数的求导很简单

而且在x>0的区间，不会梯度消失的现象，使得其性能比sigmod和tanh好很多，收敛速度更快，但是ReLU函数也有一个缺点。
ReLU函数对于负数，其导数为0，此时会出现神经元的参数永远无法更新的情况，推导过成如下



称之为神经元死亡，造成这种现象的原因由两个，第一种是在初始化参数时出现负值，第二种是学习率设置较大，导致参数更新幅度太大，出现负值。
所以在使用ReLU时，对学习率的设置要注意，需要一个合适的较小的学习率。
4. PReLU
全称是Parametric Relu, 是ReLU的改进版，函数表达式如下

针对ReLU负数为0的情况，在负数部分，设置了斜率α，当α取值为0.01时，激活函数称之为Leaky Relu, 函数图像如下

PReLU克服了ReLU会出现的神经元死亡问题，但是超参数α需要根据先验知识人工设置。
5. ELU
ELU是Exponential Linear Unit的缩写, 简称指数线性单元，函数表达式和图像如下

和PReLU类似，也定义了超参数α，对负数部分进行修正，不同的是这里不在是PReLU中的线性关系，而是一个指数关系。
该函数也避免了神经元死亡的问题，同样的超参数α需要根据先验知识人工设置，而且其求导运算比PReLU速度慢。
6. softplus
该函数是ReLU的一种平滑版本，表达式如下

函数图像如下

相比ReLU,该函数出现神经元死亡的概率更小。
7.  softsign
函数表达式如下

和tanh函数图像非常相似，图像如下

可以看作是tanh的替代品，梯度消失的概率会降低。
8. swish
同样属于ReLU的改进版，函数表达式如下


图像如下

上述几种激活函数用于隐含层神经元的输出处理，除此之外，还有两种特殊的激活函数，针对输出层的神经元，简介如下
1. softmax
专门用于处理多分类问题，在神经网络的输出层之后，在添加一个softmax层，示意如下

通过softmax函数，将神经元的输出值转换为概率，示意如下

2. MaxOut
maxout是由人为设定的K个神经元构成的一层神经元，示意如下

对于maxout层的输出，取k个神经元输出值的最大值作为最终的输出值，这就是maxout的含义。maxout可以看作是分段的线性函数，可以拟合任意的凸函数，提供模型的拟合能力。
引入maxout层，意味着额外增加了一层权重和参数，使得神经网络整体的参数变多，计算量更大。

前馈神经网络与反向传播算法
在单层感知器的基础上，引入隐藏层即可得到多层感知器和深度神经网络，结构如下

在上述网络中，信号从输入层开始，经过线性组合和激活函数的处理，输入到下一层的神经元，信号依次经过各个隐藏层的处理，最终输出到输出层，这样的信号传递是一个正向传递的过程，这种结构的神经网络称之为前馈神经网络。对比循环神经网络RNN, 更能体会前馈神经网络的这一特性

在RNN中，存在一层循环神经元，信号在自身进行递归，而前馈神经网络中信号是层层传递的，从输入层依次传输到输出层。
对于前馈神经网络而言，其参数训练的过程通过反向传播算法来实现。反向传播，对应的英文为Back proprgation, 与前馈神经网络中信号的正向传递相对应，图示如下

反向传播算法将均方误差作为模型训练的代价函数，本质上是梯度下降法。和信号从输入层到隐藏层到输出层的正向传播相反，误差值从输出层传递到隐藏层再到输入层，这也是其名称中反向传播的由来。
下面通过一个实际的例子来感受下反向传播算法，神经网络结构如下

在输入层和隐藏层中，每一层都添加了一个值为1的神经元，这样的神经元称之为bias, 类比线性拟合中随机误差对应的常数项。首先随机初始化各条边的权重值，结果如下

神经网络训练的目标是调整各条边的权重，使得模型输出值与真实值o1,o2的误差最小。类比机器学习，这个目标可以用损失函数来定量描述，这里采用均方根误差。
根据初始权重值，通过正向传播，可以计算隐藏层h1和h2的值，这里激活函数采用sigmod函数，计算过程如下

接着计算输出层的输出

根据输出层的计算结构和真实值的差异，可以计算损失函数的值

接下来进行反向传播，对于神经网络而言，由于其参数很多，损失函数的求解通过梯度下降法来实现。以w5这个权重为例，根据链式法则，其偏导数如下

依次计算链式展开的各个子项，结果如下



学习率设定为0.5，则更新后的w5参数值计算如下

其他隐藏层的权重值计算过程是一样的，隐藏层计算完之后，再传播到输入层，按照同样的方式来调整输入层的权重。在每次迭代中，信号正向传播，利用更新后的权重值来计算输出层的总体误差，然后误差反向传播，依次更新更层神经元对应的权重值

(机器学习词典，由谷歌总结) 机器学习核心概念完全解析

刚接触机器学习框架 TensorFlow 的新手们，这篇由 Google 官方出品的常用术语词汇表，一定是你必不可少的入门资料！本术语表列出了基本的机器学习术语和 TensorFlow 专用术语的定义，希望能帮助您快速熟悉 TensorFlow 入门内容，轻松打开机器学习世界的大门。
本文来源：
https://developers.google.cn/machine-learning/glossary?hl=zh-CN

A
A/B 测试 (A/B testing)
一种统计方法，用于将两种或多种技术进行比较，通常是将当前采用的技术与新技术进行比较。A/B 测试不仅旨在确定哪种技术的效果更好，而且还有助于了解相应差异是否具有显著的统计意义。A/B 测试通常是采用一种衡量方式对两种技术进行比较，但也适用于任意有限数量的技术和衡量方式。
准确率 (accuracy)
分类模型的正确预测所占的比例。在多类别分类中，准确率的定义如下：

在二元分类中，准确率的定义如下：

请参阅正例和负例。
激活函数 (activation function)
一种函数（例如ReLU或S 型函数），用于对上一层的所有输入求加权和，然后生成一个输出值（通常为非线性值），并将其传递给下一层。
AdaGrad
一种先进的梯度下降法，用于重新调整每个参数的梯度，以便有效地为每个参数指定独立的学习速率。如需查看完整的解释，请参阅这篇论文。
ROC 曲线下面积 (AUC, Area under the ROC Curve)
一种会考虑所有可能分类阈值的评估指标。
ROC 曲线下面积是，对于随机选择的正类别样本确实为正类别，以及随机选择的负类别样本为正类别，分类器更确信前者的概率。

B
反向传播算法 (backpropagation)
在神经网络上执行梯度下降法的主要算法。该算法会先按前向传播方式计算（并缓存）每个节点的输出值，然后再按反向传播遍历图的方式计算损失函数值相对于每个参数的偏导数。
基准 (baseline)
一种简单的模型或启发法，用作比较模型效果时的参考点。基准有助于模型开发者针对特定问题量化最低预期效果。
批次 (batch)
模型训练的一次迭代（即一次梯度更新）中使用的样本集。
另请参阅批次大小。
批次大小 (batch size)
一个批次中的样本数。例如，SGD的批次大小为 1，而小批次的大小通常介于 10 到 1000 之间。批次大小在训练和推断期间通常是固定的；不过，TensorFlow 允许使用动态批次大小。
偏差 (bias)
距离原点的截距或偏移。偏差（也称为偏差项）在机器学习模型中用  或  表示。例如，在下面的公式中，偏差为 ：

请勿与预测偏差混淆。
二元分类 (binary classification)
一种分类任务，可输出两种互斥类别之一。例如，对电子邮件进行评估并输出“垃圾邮件”或“非垃圾邮件”的机器学习模型就是一个二元分类器。
分箱 (binning)
请参阅分桶。
分桶 (bucketing)
将一个特征（通常是连续特征）转换成多个二元特征（称为桶或箱），通常根据值区间进行转换。例如，您可以将温度区间分割为离散分箱，而不是将温度表示成单个连续的浮点特征。假设温度数据可精确到小数点后一位，则可以将介于 0.0 到 15.0 度之间的所有温度都归入一个分箱，将介于 15.1 到 30.0 度之间的所有温度归入第二个分箱，并将介于 30.1 到 50.0 度之间的所有温度归入第三个分箱。

C
校准层 (calibration layer)
一种预测后调整，通常是为了降低预测偏差的影响。调整后的预测和概率应与观察到的标签集的分布一致。
候选采样 (candidate sampling)
一种训练时进行的优化，会使用某种函数（例如 softmax）针对所有正类别标签计算概率，但对于负类别标签，则仅针对其随机样本计算概率。例如，如果某个样本的标签为“小猎犬”和“狗”，则候选采样将针对“小猎犬”和“狗”类别输出以及其他类别（猫、棒棒糖、栅栏）的随机子集计算预测概率和相应的损失项。这种采样基于的想法是，只要正类别始终得到适当的正增强，负类别就可以从频率较低的负增强中进行学习，这确实是在实际中观察到的情况。候选采样的目的是，通过不针对所有负类别计算预测结果来提高计算效率。
分类数据 (categorical data)
一种特征，拥有一组离散的可能值。以某个名为 house style 的分类特征为例，该特征拥有一组离散的可能值（共三个），即 Tudor, ranch, colonial。通过将 house style 表示成分类数据，相应模型可以学习 Tudor、ranch 和 colonial 分别对房价的影响。
有时，离散集中的值是互斥的，只能将其中一个值应用于指定样本。例如，car maker 分类特征可能只允许一个样本有一个值 (Toyota)。在其他情况下，则可以应用多个值。一辆车可能会被喷涂多种不同的颜色，因此，car color 分类特征可能会允许单个样本具有多个值（例如 red 和 white）。
分类特征有时称为离散特征。
与数值数据相对。
形心 (centroid)
聚类的中心，由k-means或k-median算法决定。例如，如果 k 为 3，则 k-means 或 k-median 算法会找出 3 个形心。
检查点 (checkpoint)
一种数据，用于捕获模型变量在特定时间的状态。借助检查点，可以导出模型权重，跨多个会话执行训练，以及使训练在发生错误之后得以继续（例如作业抢占）。请注意，图本身不包含在检查点中。
类别 (class)
为标签枚举的一组目标值中的一个。例如，在检测垃圾邮件的二元分类模型中，两种类别分别是“垃圾邮件”和“非垃圾邮件”。在识别狗品种的多类别分类模型中，类别可以是“贵宾犬”、“小猎犬”、“哈巴犬”等等。
分类不平衡的数据集 (class-imbalanced data set)
一种二元分类问题，在此类问题中，两种类别的标签在出现频率方面具有很大的差距。例如，在某个疾病数据集中，0.0001 的样本具有正类别标签，0.9999 的样本具有负类别标签，这就属于分类不平衡问题；但在某个足球比赛预测器中，0.51 的样本的标签为其中一个球队赢，0.49 的样本的标签为另一个球队赢，这就不属于分类不平衡问题。
分类模型 (classification model)
一种机器学习模型，用于区分两种或多种离散类别。例如，某个自然语言处理分类模型可以确定输入的句子是法语、西班牙语还是意大利语。请与回归模型进行比较。
分类阈值 (classification threshold)
一种标量值条件，应用于模型预测的得分，旨在将正类别与负类别区分开。将逻辑回归结果映射到二元分类时使用。以某个逻辑回归模型为例，该模型用于确定指定电子邮件是垃圾邮件的概率。如果分类阈值为 0.9，那么逻辑回归值高于 0.9 的电子邮件将被归类为“垃圾邮件”，低于 0.9 的则被归类为“非垃圾邮件”。
聚类 (clustering)
将关联的样本分成一组，一般用于非监督式学习。在所有样本均分组完毕后，相关人员便可选择性地为每个聚类赋予含义。
聚类算法有很多。例如，k-means 算法会基于样本与形心的接近程度聚类样本，如下图所示：

之后，研究人员便可查看这些聚类并进行其他操作，例如，将聚类 1 标记为“矮型树”，将聚类 2 标记为“全尺寸树”。
再举一个例子，例如基于样本与中心点距离的聚类算法，如下所示：

协同过滤 (collaborative filtering)
根据很多其他用户的兴趣来预测某位用户的兴趣。协同过滤通常用在推荐系统中。
混淆矩阵 (confusion matrix)
一种 NxN 表格，用于总结分类模型的预测效果；即标签和模型预测的分类之间的关联。在混淆矩阵中，一个轴表示模型预测的标签，另一个轴表示实际标签。N 表示类别个数。在二元分类问题中，N=2。例如，下面显示了一个二元分类问题的混淆矩阵示例：

上面的混淆矩阵显示，在 19 个实际有肿瘤的样本中，该模型正确地将 18 个归类为有肿瘤（18 个正例），错误地将 1 个归类为没有肿瘤（1 个假负例）。同样，在 458 个实际没有肿瘤的样本中，模型归类正确的有 452 个（452 个负例），归类错误的有 6 个（6 个假正例）。
多类别分类问题的混淆矩阵有助于确定出错模式。例如，某个混淆矩阵可以揭示，某个经过训练以识别手写数字的模型往往会将 4 错误地预测为 9，将 7 错误地预测为 1。
混淆矩阵包含计算各种效果指标（包括精确率和召回率）所需的充足信息。
连续特征 (continuous feature)
一种浮点特征，可能值的区间不受限制。与离散特征相对。
收敛 (convergence)
通俗来说，收敛通常是指在训练期间达到的一种状态，即经过一定次数的迭代之后，训练损失和验证损失在每次迭代中的变化都非常小或根本没有变化。也就是说，如果采用当前数据进行额外的训练将无法改进模型，模型即达到收敛状态。在深度学习中，损失值有时会在最终下降之前的多次迭代中保持不变或几乎保持不变，暂时形成收敛的假象。
另请参阅早停法。
另请参阅 Boyd 和 Vandenberghe 合著的 Convex Optimization（《凸优化》）。
凸函数 (convex function)
一种函数，函数图像以上的区域为凸集。典型凸函数的形状类似于字母U。例如，以下都是凸函数：

相反，以下函数则不是凸函数。请注意图像上方的区域如何不是凸集：

严格凸函数只有一个局部最低点，该点也是全局最低点。经典的 U 形函数都是严格凸函数。不过，有些凸函数（例如直线）则不是这样。
很多常见的损失函数（包括下列函数）都是凸函数：
L2 损失函数
对数损失函数
L1 正则化
L2 正则化
梯度下降法的很多变体都一定能找到一个接近严格凸函数最小值的点。同样，随机梯度下降法的很多变体都有很高的可能性能够找到接近严格凸函数最小值的点（但并非一定能找到）。
两个凸函数的和（例如 L2 损失函数 + L1 正则化）也是凸函数。
深度模型绝不会是凸函数。值得注意的是，专门针对凸优化设计的算法往往总能在深度网络上找到非常好的解决方案，虽然这些解决方案并不一定对应于全局最小值。
凸优化 (convex optimization)
使用数学方法（例如梯度下降法）寻找凸函数最小值的过程。机器学习方面的大量研究都是专注于如何通过公式将各种问题表示成凸优化问题，以及如何更高效地解决这些问题。
如需完整的详细信息，请参阅 Boyd 和 Vandenberghe 合著的 Convex Optimization（《凸优化》）。
凸集 (convex set)
欧几里得空间的一个子集，其中任意两点之间的连线仍完全落在该子集内。例如，下面的两个图形都是凸集：

相反，下面的两个图形都不是凸集：

卷积 (convolution)
简单来说，卷积在数学中指两个函数的组合。在机器学习中，卷积结合使用卷积过滤器和输入矩阵来训练权重。
机器学习中的“卷积”一词通常是卷积运算或卷积层的简称。
如果没有卷积，机器学习算法就需要学习大张量中每个单元格各自的权重。例如，用 2K x 2K 图像训练的机器学习算法将被迫找出 400 万个单独的权重。而使用卷积，机器学习算法只需在卷积过滤器中找出每个单元格的权重，大大减少了训练模型所需的内存。在应用卷积过滤器后，它只需跨单元格进行复制，每个单元格都会与过滤器相乘。
卷积过滤器 (convolutional filter)
卷积运算中的两个参与方之一。（另一个参与方是输入矩阵切片。）卷积过滤器是一种矩阵，其等级与输入矩阵相同，但形状小一些。以 28×28 的输入矩阵为例，过滤器可以是小于 28×28 的任何二维矩阵。
在图形操作中，卷积过滤器中的所有单元格通常按照固定模式设置为 1 和 0。在机器学习中，卷积过滤器通常先选择随机数字，然后由网络训练出理想值。
卷积层 (convolutional layer)
深度神经网络的一个层，卷积过滤器会在其中传递输入矩阵。以下面的 3x3卷积过滤器为例：

下面的动画显示了一个由 9 个卷积运算（涉及 5x5 输入矩阵）组成的卷积层。请注意，每个卷积运算都涉及一个不同的 3x3 输入矩阵切片。由此产生的 3×3 矩阵（右侧）就包含 9 个卷积运算的结果：

卷积神经网络 (convolutional neural network)
一种神经网络，其中至少有一层为卷积层。典型的卷积神经网络包含以下几层的组合：
卷积层
池化层
密集层
卷积神经网络在解决某些类型的问题（如图像识别）上取得了巨大成功。
卷积运算 (convolutional operation)
如下所示的两步数学运算：
对卷积过滤器和输入矩阵切片执行元素级乘法。（输入矩阵切片与卷积过滤器具有相同的等级和大小。）
对生成的积矩阵中的所有值求和。
以下面的 5x5 输入矩阵为例：

现在，以下面这个 2x2 卷积过滤器为例：

每个卷积运算都涉及一个 2x2 输入矩阵切片。例如，假设我们使用输入矩阵左上角的 2x2 切片。这样一来，对此切片进行卷积运算将如下所示：

卷积层由一系列卷积运算组成，每个卷积运算都针对不同的输入矩阵切片。
成本 (cost)
与损失的含义相同。
交叉熵 (cross-entropy)
对数损失函数向多类别分类问题的一种泛化。交叉熵可以量化两种概率分布之间的差异。另请参阅困惑度。
自定义 Estimator (custom Estimator)
您按照这些说明自行编写的 Estimator。
与预创建的 Estimator 相对。

D
数据分析 (data analysis)
根据样本、测量结果和可视化内容来理解数据。数据分析在首次收到数据集、构建第一个模型之前特别有用。此外，数据分析在理解实验和调试系统问题方面也至关重要。
DataFrame
一种热门的数据类型，用于表示 Pandas 中的数据集。DataFrame 类似于表格。DataFrame 的每一列都有一个名称（标题），每一行都由一个数字标识。
数据集 (data set)
一组样本的集合。
Dataset API (tf.data)
一种高级别的 TensorFlow API，用于读取数据并将其转换为机器学习算法所需的格式。tf.data.Dataset 对象表示一系列元素，其中每个元素都包含一个或多个张量。tf.data.Iterator对象可获取 Dataset 中的元素。
如需详细了解 Dataset API，请参阅《TensorFlow 编程人员指南》中的导入数据。
决策边界 (decision boundary)
在二元分类或多类别分类问题中，模型学到的类别之间的分界线。例如，在以下表示某个二元分类问题的图片中，决策边界是橙色类别和蓝色类别之间的分界线：

密集层 (dense layer)
与全连接层的含义相同。
深度模型 (deep model)
一种神经网络，其中包含多个隐藏层。深度模型依赖于可训练的非线性关系。
与宽度模型相对。
密集特征 (dense feature)
一种大部分值是非零值的特征，通常是浮点值张量。与稀疏特征相对。
设备 (device)
一类可运行 TensorFlow 会话的硬件，包括 CPU、GPU 和 TPU。
离散特征 (discrete feature)
一种特征，包含有限个可能值。例如，某个值只能是“动物”、“蔬菜”或“矿物”的特征便是一个离散特征（或分类特征）。与连续特征相对。
丢弃正则化 (dropout regularization)
正则化的一种形式，在训练神经网络方面非常有用。丢弃正则化的运作机制是，在一个梯度步长中移除从神经网络层中随机选择的固定数量的单元。丢弃的单元越多，正则化效果就越强。这类似于训练神经网络以模拟较小网络的指数级规模集成学习。如需完整的详细信息，请参阅Dropout: A Simple Way to Prevent Neural Networks from Overfitting（《丢弃：一种防止神经网络过拟合的简单方法》）。
动态模型 (dynamic model)
一种模型，以持续更新的方式在线接受训练。也就是说，数据会源源不断地进入这种模型。

E
早停法 (early stopping)
一种正则化方法，是指在训练损失仍可以继续降低之前结束模型训练。使用早停法时，您会在验证数据集的损失开始增大（也就是泛化效果变差）时结束模型训练。
嵌套 (embeddings)
一种分类特征，以连续值特征表示。通常，嵌套是指将高维度向量映射到低维度的空间。例如，您可以采用以下两种方式之一来表示英文句子中的单词：
表示成包含百万个元素（高维度）的稀疏向量，其中所有元素都是整数。向量中的每个单元格都表示一个单独的英文单词，单元格中的值表示相应单词在句子中出现的次数。由于单个英文句子包含的单词不太可能超过 50 个，因此向量中几乎每个单元格都包含 0。少数非 0 的单元格中将包含一个非常小的整数（通常为 1），该整数表示相应单词在句子中出现的次数。
表示成包含数百个元素（低维度）的密集向量，其中每个元素都存储一个介于 0 到 1 之间的浮点值。这就是一种嵌套。
在 TensorFlow 中，会按反向传播损失训练嵌套，和训练神经网络中的任何其他参数一样。
经验风险最小化 (ERM, empirical risk minimization)
用于选择可以将基于训练集的损失降至最低的函数。与结构风险最小化相对。
集成学习 (ensemble)
多个模型的预测结果的并集。您可以通过以下一项或多项来创建集成学习：
不同的初始化
不同的超参数
不同的整体结构
深度模型和宽度模型属于一种集成学习。
周期 (epoch)
在训练时，整个数据集的一次完整遍历，以便不漏掉任何一个样本。因此，一个周期表示（N/批次大小）次训练迭代，其中N是样本总数。
Estimator
tf.Estimator类的一个实例，用于封装负责构建 TensorFlow 图并运行 TensorFlow 会话的逻辑。您可以创建自定义 Estimator（如需相关介绍，请点击此处），也可以实例化其他人预创建的 Estimator。
样本 (example)
数据集的一行。一个样本包含一个或多个特征，此外还可能包含一个标签。另请参阅有标签样本和无标签样本。

F
假负例 (FN, false negative)
被模型错误地预测为负类别的样本。例如，模型推断出某封电子邮件不是垃圾邮件（负类别），但该电子邮件其实是垃圾邮件。
假正例 (FP, false positive)
被模型错误地预测为正类别的样本。例如，模型推断出某封电子邮件是垃圾邮件（正类别），但该电子邮件其实不是垃圾邮件。
假正例率（false positive rate, 简称 FP 率）
ROC 曲线中的 x 轴。FP 率的定义如下：

特征 (feature)
在进行预测时使用的输入变量。
特征列 (tf.feature_column)
指定模型应该如何解读特定特征的一种函数。此类函数的输出结果是所有 Estimators 构造函数的必需参数。
借助 tf.feature_column 函数，模型可对输入特征的不同表示法轻松进行实验。有关详情，请参阅《TensorFlow 编程人员指南》中的特征列一章。
“特征列”是 Google 专用的术语。特征列在 Yahoo/Microsoft 使用的 VW 系统中称为“命名空间”，也称为场。
特征组合 (feature cross)
通过将单独的特征进行组合（求笛卡尔积）而形成的合成特征。特征组合有助于表达非线性关系。
特征工程 (feature engineering)
指以下过程：确定哪些特征可能在训练模型方面非常有用，然后将日志文件及其他来源的原始数据转换为所需的特征。在 TensorFlow 中，特征工程通常是指将原始日志文件条目转换为 tf.Example 协议缓冲区。另请参阅 tf.Transform。
特征工程有时称为特征提取。
特征集 (feature set)
训练机器学习模型时采用的一组特征。例如，对于某个用于预测房价的模型，邮政编码、房屋面积以及房屋状况可以组成一个简单的特征集。
特征规范 (feature spec)
用于描述如何从 tf.Example 协议缓冲区提取特征数据。由于 tf.Example 协议缓冲区只是一个数据容器，因此您必须指定以下内容：
要提取的数据（即特征的键）
数据类型（例如 float 或 int）
长度（固定或可变）
Estimator API 提供了一些可用来根据给定 FeatureColumns 列表生成特征规范的工具。
少量样本学习 (few-shot learning)
一种机器学习方法（通常用于对象分类），旨在仅通过少量训练样本学习有效的分类器。
另请参阅单样本学习。
完整 softmax (full softmax)
请参阅 softmax。与候选采样相对。
全连接层 (fully connected layer)
一种隐藏层，其中的每个节点均与下一个隐藏层中的每个节点相连。
全连接层又称为密集层。

G
泛化 (generalization)
指的是模型依据训练时采用的数据，针对以前未见过的新数据做出正确预测的能力。
广义线性模型 (generalized linear model)
最小二乘回归模型（基于高斯噪声）向其他类型的模型（基于其他类型的噪声，例如泊松噪声或分类噪声）进行的一种泛化。广义线性模型的示例包括：
逻辑回归
多类别回归
最小二乘回归
可以通过凸优化找到广义线性模型的参数。
广义线性模型具有以下特性：
最优的最小二乘回归模型的平均预测结果等于训练数据的平均标签。
最优的逻辑回归模型预测的平均概率等于训练数据的平均标签。
广义线性模型的功能受其特征的限制。与深度模型不同，广义线性模型无法“学习新特征”。
梯度 (gradient)
偏导数相对于所有自变量的向量。在机器学习中，梯度是模型函数偏导数的向量。梯度指向最高速上升的方向。
梯度裁剪 (gradient clipping)
在应用梯度值之前先设置其上限。梯度裁剪有助于确保数值稳定性以及防止梯度爆炸。
梯度下降法 (gradient descent)
一种通过计算并且减小梯度将损失降至最低的技术，它以训练数据为条件，来计算损失相对于模型参数的梯度。通俗来说，梯度下降法以迭代方式调整参数，逐渐找到权重和偏差的最佳组合，从而将损失降至最低。
图 (graph)
TensorFlow 中的一种计算规范。图中的节点表示操作。边缘具有方向，表示将某项操作的结果（一个张量）作为一个操作数传递给另一项操作。可以使用 TensorBoard 直观呈现图。

H
启发法 (heuristic)
一种非最优但实用的问题解决方案，足以用于进行改进或从中学习。
隐藏层 (hidden layer)
神经网络中的合成层，介于输入层（即特征）和输出层（即预测）之间。神经网络包含一个或多个隐藏层。
合页损失函数 (hinge loss)
一系列用于分类的损失函数，旨在找到距离每个训练样本都尽可能远的决策边界，从而使样本和边界之间的裕度最大化。KSVM 使用合页损失函数（或相关函数，例如平方合页损失函数）。对于二元分类，合页损失函数的定义如下：

其中“y\'”表示分类器模型的原始输出：

“y”表示真标签，值为 -1 或 +1。
因此，合页损失与 (y * y\') 的关系图如下所示：

维持数据 (holdout data)
训练期间故意不使用（“维持”）的样本。验证数据集和测试数据集都属于维持数据。维持数据有助于评估模型向训练时所用数据之外的数据进行泛化的能力。与基于训练数据集的损失相比，基于维持数据集的损失有助于更好地估算基于未见过的数据集的损失。
超参数 (hyperparameter)
在模型训练的连续过程中，您调节的“旋钮”。例如，学习速率就是一种超参数。
与参数相对。
超平面 (hyperplane)
将一个空间划分为两个子空间的边界。例如，在二维空间中，直线就是一个超平面，在三维空间中，平面则是一个超平面。在机器学习中更典型的是：超平面是分隔高维度空间的边界。核支持向量机利用超平面将正类别和负类别区分开来（通常是在极高维度空间中）。

I
独立同等分布 (i.i.d, independently and identically distributed)
从不会改变的分布中提取的数据，其中提取的每个值都不依赖于之前提取的值。i.i.d. 是机器学习的理想气体 - 一种实用的数学结构，但在现实世界中几乎从未发现过。例如，某个网页的访问者在短时间内的分布可能为 i.i.d.，即分布在该短时间内没有变化，且一位用户的访问行为通常与另一位用户的访问行为无关。不过，如果将时间窗口扩大，网页访问者的分布可能呈现出季节性变化。
推断 (inference)
在机器学习中，推断通常指以下过程：通过将训练过的模型应用于无标签样本来做出预测。在统计学中，推断是指在某些观测数据条件下拟合分布参数的过程。（请参阅维基百科中有关统计学推断的文章。）
输入函数 (input function)
在 TensorFlow 中，用于将输入数据返回到 Estimator 的训练、评估或预测方法的函数。例如，训练输入函数会返回训练集中的一批特征和标签。
输入层 (input layer)
神经网络中的第一层（接收输入数据的层）。
实例 (instance)
与样本的含义相同。
可解释性 (interpretability)
模型的预测可解释的难易程度。深度模型通常不可解释，也就是说，很难对深度模型的不同层进行解释。相比之下，线性回归模型和宽度模型的可解释性通常要好得多。
评分者间一致性信度 (inter-rater agreement)
一种衡量指标，用于衡量在执行某项任务时评分者达成一致的频率。如果评分者未达成一致，则可能需要改进任务说明。有时也称为注释者间一致性信度或评分者间可靠性信度。另请参阅 Cohen\'s kappa（最热门的评分者间一致性信度衡量指标之一）。
迭代 (iteration)
模型的权重在训练期间的一次更新。迭代包含计算参数在单批次数据上的梯度损失。

K
k-means
一种热门的聚类算法，用于对非监督式学习中的样本进行分组。k-means 算法基本上会执行以下操作：
以迭代方式确定最佳的 k 中心点（称为形心）。
将每个样本分配到最近的形心。与同一个形心距离最近的样本属于同一个组。
k-means 算法会挑选形心位置，以最大限度地减小每个样本与其最接近形心之间的距离的累积平方。
以下面的小狗高度与小狗宽度的关系图为例：

如果 k=3，则 k-means 算法会确定三个形心。每个样本都被分配到与其最接近的形心，最终产生三个组：

假设制造商想要确定小、中和大号狗毛衣的理想尺寸。在该聚类中，三个形心用于标识每只狗的平均高度和平均宽度。因此，制造商可能应该根据这三个形心确定毛衣尺寸。请注意，聚类的形心通常不是聚类中的样本。
上图显示了 k-means 应用于仅具有两个特征（高度和宽度）的样本。请注意，k-means 可以跨多个特征为样本分组。
k-median
与 k-means 紧密相关的聚类算法。两者的实际区别如下：
对于 k-means，确定形心的方法是，最大限度地减小候选形心与它的每个样本之间的距离平方和。
对于 k-median，确定形心的方法是，最大限度地减小候选形心与它的每个样本之间的距离总和。
请注意，距离的定义也有所不同：
k-means 采用从形心到样本的欧几里得距离。（在二维空间中，欧几里得距离即使用勾股定理来计算斜边。）例如，(2,2) 与 (5,-2) 之间的 k-means 距离为：

k-median 采用从形心到样本的曼哈顿距离。这个距离是每个维度中绝对差异值的总和。例如，(2,2) 与 (5,-2) 之间的 k-median 距离为：

Keras
一种热门的 Python 机器学习 API。Keras 能够在多种深度学习框架上运行，其中包括 TensorFlow（在该框架上，Keras 作为 tf.keras 提供）。
核支持向量机 (KSVM, Kernel Support Vector Machines)
一种分类算法，旨在通过将输入数据向量映射到更高维度的空间，来最大化正类别和负类别之间的裕度。以某个输入数据集包含一百个特征的分类问题为例。为了最大化正类别和负类别之间的裕度，KSVM 可以在内部将这些特征映射到百万维度的空间。KSVM 使用合页损失函数。

L
L1 损失函数 (L₁ loss)
一种损失函数，基于模型预测的值与标签的实际值之差的绝对值。与 L2 损失函数相比，L1 损失函数对离群值的敏感性弱一些。
L1 正则化 (L₁ regularization)
一种正则化，根据权重的绝对值的总和来惩罚权重。在依赖稀疏特征的模型中，L1 正则化有助于使不相关或几乎不相关的特征的权重正好为 0，从而将这些特征从模型中移除。与 L2 正则化相对。
L2 损失函数 (L₂ loss)
请参阅平方损失函数。
L2 正则化 (L₂ regularization)
一种正则化，根据权重的平方和来惩罚权重。L2 正则化有助于使离群值（具有较大正值或较小负值）权重接近于 0，但又不正好为 0。（与 L1 正则化相对。）在线性模型中，L2 正则化始终可以改进泛化。
标签 (label)
在监督式学习中，标签指样本的“答案”或“结果”部分。有标签数据集中的每个样本都包含一个或多个特征以及一个标签。例如，在房屋数据集中，特征可能包括卧室数、卫生间数以及房龄，而标签则可能是房价。在垃圾邮件检测数据集中，特征可能包括主题行、发件人以及电子邮件本身，而标签则可能是“垃圾邮件”或“非垃圾邮件”。
有标签样本 (labeled example)
包含特征和标签的样本。在监督式训练中，模型从有标签样本中学习规律。
lambda
与正则化率的含义相同。
（多含义术语，我们在此关注的是该术语在正则化中的定义。）
层 (layer)
神经网络中的一组神经元，负责处理一组输入特征，或一组神经元的输出。
此外还指 TensorFlow 中的抽象层。层是 Python 函数，以张量和配置选项作为输入，然后生成其他张量作为输出。当必要的张量组合起来后，用户便可以通过模型函数将结果转换为 Estimator。
Layers API (tf.layers)
一种 TensorFlow API，用于以层组合的方式构建深度神经网络。通过 Layers API，您可以构建不同类型的层，例如：
通过 tf.layers.Dense 构建全连接层。
通过 tf.layers.Conv2D 构建卷积层。
在编写自定义 Estimator 时，您可以编写“层”对象来定义所有隐藏层的特征。
Layers API 遵循 Keras layers API 规范。也就是说，除了前缀不同以外，Layers API 中的所有函数均与 Keras layers API 中的对应函数具有相同的名称和签名。
学习速率 (learning rate)
在训练模型时用于梯度下降的一个标量。在每次迭代期间，梯度下降法都会将学习速率与梯度相乘。得出的乘积称为梯度步长。
学习速率是一个重要的超参数。
最小二乘回归 (least squares regression)
一种通过最小化 L2 损失训练出的线性回归模型。
线性回归 (linear regression)
一种回归模型，通过将输入特征进行线性组合输出连续值。
逻辑回归 (logistic regression)
一种模型，通过将 S 型函数应用于线性预测，生成分类问题中每个可能的离散标签值的概率。虽然逻辑回归经常用于二元分类问题，但也可用于多类别分类问题（其叫法变为多类别逻辑回归或多项回归）。
对数 (logits)
分类模型生成的原始（非标准化）预测向量，通常会传递给标准化函数。如果模型要解决多类别分类问题，则对数通常变成 softmax 函数的输入。之后，softmax 函数会生成一个（标准化）概率向量，对应于每个可能的类别。
此外，对数有时也称为 S 型函数的元素级反函数。如需了解详细信息，请参阅 tf.nn.sigmoid_cross_entropy_with_logits。
对数损失函数 (Log Loss)
二元逻辑回归中使用的损失函数。
对数几率 (log-odds)
某个事件几率的对数。
如果事件涉及二元概率，则几率指的是成功概率 (p) 与失败概率 (1-p) 之比。例如，假设某个给定事件的成功概率为 90％，失败概率为 10％。在这种情况下，几率的计算公式如下：

简单来说，对数几率即几率的对数。按照惯例，“对数”指自然对数，但对数的基数其实可以是任何大于 1 的数。若遵循惯例，上述示例的对数几率应为：

对数几率是S 型函数的反函数。
损失 (Loss)
一种衡量指标，用于衡量模型的预测偏离其标签的程度。或者更悲观地说是衡量模型有多差。要确定此值，模型必须定义损失函数。例如，线性回归模型通常将均方误差用作损失函数，而逻辑回归模型则使用对数损失函数。

M
机器学习 (machine learning)
一种程序或系统，用于根据输入数据构建（训练）预测模型。这种系统会利用学到的模型根据从分布（训练该模型时使用的同一分布）中提取的新数据（以前从未见过的数据）进行实用的预测。机器学习还指与这些程序或系统相关的研究领域。
均方误差 (MSE, Mean Squared Error)
每个样本的平均平方损失。MSE 的计算方法是平方损失除以样本数。TensorFlow Playground 显示的“训练损失”值和“测试损失”值都是 MSE。
指标 (metric)
您关心的一个数值。可能可以也可能不可以直接在机器学习系统中得到优化。您的系统尝试优化的指标称为目标。
Metrics API (tf.metrics)
一种用于评估模型的 TensorFlow API。例如，tf.metrics.accuracy 用于确定模型的预测与标签匹配的频率。在编写自定义 Estimator 时，您可以调用 Metrics API 函数来指定应如何评估您的模型。
小批次 (mini-batch)
从整批样本内随机选择并在训练或推断过程的一次迭代中一起运行的一小部分样本。小批次的批次大小通常介于 10 到 1000 之间。与基于完整的训练数据计算损失相比，基于小批次数据计算损失要高效得多。
小批次随机梯度下降法 (SGD, mini-batch stochastic gradient descent)
一种采用小批次样本的梯度下降法。也就是说，小批次 SGD 会根据一小部分训练数据来估算梯度。Vanilla SGD 使用的小批次的大小为 1。
ML
机器学习的缩写。
模型 (model)
机器学习系统从训练数据学到的内容的表示形式。多含义术语，可以理解为下列两种相关含义之一：
一种 TensorFlow 图，用于表示预测的计算结构。
该 TensorFlow 图的特定权重和偏差，通过训练决定。
模型函数 (model function)
Estimator 中的函数，用于实现机器学习训练、评估和推断。例如，模型函数的训练部分可以处理以下任务：定义深度神经网络的拓扑并确定其优化器函数。如果使用预创建的 Estimator，则有人已为您编写了模型函数。如果使用自定义 Estimator，则必须自行编写模型函数。
有关编写模型函数的详细信息，请参阅创建自定义 Estimator。
模型训练 (model training)
确定最佳模型的过程。
动量 (Momentum)
一种先进的梯度下降法，其中学习步长不仅取决于当前步长的导数，还取决于之前一步或多步的步长的导数。动量涉及计算梯度随时间而变化的指数级加权移动平均值，与物理学中的动量类似。动量有时可以防止学习过程被卡在局部最小的情况。
多类别分类 (multi-class classification)
区分两种以上类别的分类问题。例如，枫树大约有 128 种，因此，确定枫树种类的模型就属于多类别模型。反之，仅将电子邮件分为两类（“垃圾邮件”和“非垃圾邮件”）的模型属于二元分类模型。
多项分类 (multinomial classification)
与多类别分类的含义相同。
N
NaN 陷阱 (NaN trap)
模型中的一个数字在训练期间变成 NaN，这会导致模型中的很多或所有其他数字最终也会变成 NaN。
NaN 是“非数字”的缩写。
负类别 (negative class)
在二元分类中，一种类别称为正类别，另一种类别称为负类别。正类别是我们要寻找的类别，负类别则是另一种可能性。例如，在医学检查中，负类别可以是“非肿瘤”。在电子邮件分类器中，负类别可以是“非垃圾邮件”。另请参阅正类别。
神经网络 (neural network)
一种模型，灵感来源于脑部结构，由多个层构成（至少有一个是隐藏层），每个层都包含简单相连的单元或神经元（具有非线性关系）。
神经元 (neuron)
神经网络中的节点，通常会接收多个输入值并生成一个输出值。神经元通过将激活函数（非线性转换）应用于输入值的加权和来计算输出值。
节点 (node)
多含义术语，可以理解为下列两种含义之一：
隐藏层中的神经元。
TensorFlow 图中的操作。
标准化 (normalization)
将实际的值区间转换为标准的值区间（通常为 -1 到 +1 或 0 到 1）的过程。例如，假设某个特征的自然区间是 800 到 6000。通过减法和除法运算，您可以将这些值标准化为位于 -1 到 +1 区间内。
另请参阅缩放。
数值数据 (numerical data)
用整数或实数表示的特征。例如，在房地产模型中，您可能会用数值数据表示房子大小（以平方英尺或平方米为单位）。如果用数值数据表示特征，则可以表明特征的值相互之间具有数学关系，并且与标签可能也有数学关系。例如，如果用数值数据表示房子大小，则可以表明面积为 200 平方米的房子是面积为 100 平方米的房子的两倍。此外，房子面积的平方米数可能与房价存在一定的数学关系。
并非所有整数数据都应表示成数值数据。例如，世界上某些地区的邮政编码是整数，但在模型中，不应将整数邮政编码表示成数值数据。这是因为邮政编码 20000 在效力上并不是邮政编码 10000 的两倍（或一半）。此外，虽然不同的邮政编码确实与不同的房地产价值有关，但我们也不能假设邮政编码为 20000 的房地产在价值上是邮政编码为 10000 的房地产的两倍。邮政编码应表示成分类数据。
数值特征有时称为连续特征。
Numpy
一个开放源代码数学库，在 Python 中提供高效的数组操作。Pandas 建立在 Numpy 之上。

O
目标 (objective)
算法尝试优化的指标。
离线推断 (offline inference)
生成一组预测，存储这些预测，然后根据需求检索这些预测。与在线推断相对。
独热编码 (one-hot encoding)
一种稀疏向量，其中：
一个元素设为 1。
所有其他元素均设为 0。
独热编码常用于表示拥有有限个可能值的字符串或标识符。例如，假设某个指定的植物学数据集记录了 15000 个不同的物种，其中每个物种都用独一无二的字符串标识符来表示。在特征工程过程中，您可能需要将这些字符串标识符编码为独热向量，向量的大小为 15000。
单样本学习（one-shot learning，通常用于对象分类）
一种机器学习方法，通常用于对象分类，旨在通过单个训练样本学习有效的分类器。
另请参阅少量样本学习。
一对多 (one-vs.-all)
假设某个分类问题有 N 种可能的解决方案，一对多解决方案将包含 N 个单独的二元分类器 - 一个二元分类器对应一种可能的结果。例如，假设某个模型用于区分样本属于动物、蔬菜还是矿物，一对多解决方案将提供下列三个单独的二元分类器：
动物和非动物
蔬菜和非蔬菜
矿物和非矿物
在线推断 (online inference)
根据需求生成预测。与离线推断相对。
操作 (op, Operation)
TensorFlow 图中的节点。在 TensorFlow 中，任何创建、操纵或销毁张量的过程都属于操作。例如，矩阵相乘就是一种操作，该操作以两个张量作为输入，并生成一个张量作为输出。
优化器 (optimizer)
梯度下降法的一种具体实现。TensorFlow 的优化器基类是 tf.train.Optimizer。不同的优化器可能会利用以下一个或多个概念来增强梯度下降法在指定训练集中的效果：
动量 (Momentum)
更新频率（AdaGrad = ADAptive GRADient descent；Adam = ADAptive with Momentum；RMSProp）
稀疏性/正则化 (Ftrl)
更复杂的数学方法（Proximal，等等）
甚至还包括 NN 驱动的优化器。
离群值 (outlier)
与大多数其他值差别很大的值。在机器学习中，下列所有值都是离群值。
绝对值很高的权重。
与实际值相差很大的预测值。
值比平均值高大约 3 个标准偏差的输入数据。
离群值常常会导致模型训练出现问题。
输出层 (output layer)
神经网络的“最后”一层，也是包含答案的层。
过拟合 (overfitting)
创建的模型与训练数据过于匹配，以致于模型无法根据新数据做出正确的预测。

P
Pandas
面向列的数据分析 API。很多机器学习框架（包括 TensorFlow）都支持将 Pandas 数据结构作为输入。请参阅 Pandas 文档。
参数 (parameter)
机器学习系统自行训练的模型的变量。例如，权重就是一种参数，它们的值是机器学习系统通过连续的训练迭代逐渐学习到的。与超参数相对。
参数服务器 (PS, Parameter Server)
一种作业，负责在分布式设置中跟踪模型参数。
参数更新 (parameter update)
在训练期间（通常是在梯度下降法的单次迭代中）调整模型参数的操作。
偏导数 (partial derivative)
一种导数，除一个变量之外的所有变量都被视为常量。例如，f(x, y) 对 x 的偏导数就是 f(x) 的导数（即，使 y 保持恒定）。f 对 x 的偏导数仅关注 x 如何变化，而忽略公式中的所有其他变量。
划分策略 (partitioning strategy)
在参数服务器间分割变量的算法。
性能 (performance)
多含义术语，具有以下含义：
在软件工程中的传统含义。即：相应软件的运行速度有多快（或有多高效）？
在机器学习中的含义。在机器学习领域，性能旨在回答以下问题：相应模型的准确度有多高？即模型在预测方面的表现有多好？
困惑度 (perplexity)
一种衡量指标，用于衡量模型能够多好地完成任务。例如，假设任务是读取用户使用智能手机键盘输入字词时输入的前几个字母，然后列出一组可能的完整字词。此任务的困惑度 (P) 是：为了使列出的字词中包含用户尝试输入的实际字词，您需要提供的猜测项的个数。
困惑度与交叉熵的关系如下：

流水线 (pipeline)
机器学习算法的基础架构。流水线包括收集数据、将数据放入训练数据文件、训练一个或多个模型，以及将模型导出到生产环境。
池化 (pooling)
将一个或多个由前趋的**卷积层**创建的矩阵压缩为较小的矩阵。池化通常是取整个池化区域的最大值或平均值。以下面的 3x3 矩阵为例：

池化运算与卷积运算类似：将矩阵分割为多个切片，然后按步长逐个运行卷积运算。例如，假设池化运算按 1x1 步长将卷积矩阵分割为 2x2 个切片。如下图所示，进行了四个池化运算。假设每个池化运算都选择该切片中四个值的最大值：

池化有助于在输入矩阵中实现平移不变性。
对于视觉应用来说，池化的更正式名称为空间池化。时间序列应用通常将池化称为时序池化。按照不太正式的说法，池化通常称为下采样或降采样。
正类别 (positive class)
在二元分类中，两种可能的类别分别被标记为正类别和负类别。正类别结果是我们要测试的对象。（不可否认的是，我们会同时测试这两种结果，但只关注正类别结果。）例如，在医学检查中，正类别可以是“肿瘤”。在电子邮件分类器中，正类别可以是“垃圾邮件”。
与负类别相对。
精确率 (precision)
一种分类模型指标。精确率指模型正确预测正类别的频率，即：
精确率正例数正例数假正例数
预测 (prediction)
模型在收到输入样本后的输出。
预测偏差 (prediction bias)
一种值，用于表明预测平均值与数据集中标签的平均值相差有多大。
预创建的 Estimator (pre-made Estimator)
其他人已建好的 Estimator。TensorFlow 提供了一些预创建的 Estimator，包括 DNNClassifier、DNNRegressor 和 LinearClassifier。您可以按照这些说明构建自己预创建的 Estimator。
预训练模型 (pre-trained model)
已经过训练的模型或模型组件（例如嵌套）。有时，您需要将预训练的嵌套馈送到神经网络。在其他时候，您的模型将自行训练嵌套，而不依赖于预训练的嵌套。
先验信念 (prior belief)
在开始采用相应数据进行训练之前，您对这些数据抱有的信念。例如，L2 正则化依赖的先验信念是权重应该很小且应以 0 为中心呈正态分布。

Q
队列 (queue)
一种 TensorFlow 操作，用于实现队列数据结构。通常用于 I/O 中。

R
等级 (rank)
机器学习中的一个多含义术语，可以理解为下列含义之一：
张量中的维数。例如，标量等级为 0，向量等级为 1，矩阵等级为 2。
在将类别从最高到最低进行排序的机器学习问题中，类别的顺序位置。例如，行为排序系统可以将狗狗的奖励从最高（牛排）到最低（枯萎的羽衣甘蓝）进行排序。
评分者 (rater)
为样本提供标签的人。有时称为“注释者”。
召回率 (recall)
一种分类模型指标，用于回答以下问题：在所有可能的正类别标签中，模型正确地识别出了多少个？即：


修正线性单元 (ReLU, Rectified Linear Unit)
一种激活函数，其规则如下：
如果输入为负数或 0，则输出 0。
如果输入为正数，则输出等于输入。
回归模型 (regression model)
一种模型，能够输出连续的值（通常为浮点值）。请与分类模型进行比较，分类模型会输出离散值，例如“黄花菜”或“虎皮百合”。
正则化 (regularization)
对模型复杂度的惩罚。正则化有助于防止出现过拟合，包含以下类型：
L1 正则化
L2 正则化
丢弃正则化
早停法（这不是正式的正则化方法，但可以有效限制过拟合）
正则化率 (regularization rate)
一种标量值，以 lambda 表示，用于指定正则化函数的相对重要性。从下面简化的损失公式中可以看出正则化率的影响：
最小化损失方程正则化方程
提高正则化率可以减少过拟合，但可能会使模型的准确率降低。
表示法 (representation)
将数据映射到实用特征的过程。
受试者工作特征曲线（receiver operating characteristic，简称 ROC 曲线）
不同分类阈值下的正例率和假正例率构成的曲线。另请参阅曲线下面积。
根目录 (root directory)
您指定的目录，用于托管多个模型的 TensorFlow 检查点和事件文件的子目录。
均方根误差 (RMSE, Root Mean Squared Error)
均方误差的平方根。
旋转不变性 (rotational invariance)
在图像分类问题中，即使图像的方向发生变化，算法也能成功地对图像进行分类。例如，无论网球拍朝上、侧向还是朝下放置，该算法仍然可以识别它。请注意，并非总是希望旋转不变；例如，倒置的“9”不应分类为“9”。
另请参阅平移不变性和大小不变性。

S
SavedModel
保存和恢复 TensorFlow 模型时建议使用的格式。SavedModel 是一种独立于语言且可恢复的序列化格式，使较高级别的系统和工具可以创建、使用和转换 TensorFlow 模型。
如需完整的详细信息，请参阅《TensorFlow 编程人员指南》中的保存和恢复。
Saver
一种 TensorFlow 对象，负责保存模型检查点。
缩放 (scaling)
特征工程中的一种常用做法，是指对某个特征的值区间进行调整，使之与数据集中其他特征的值区间一致。例如，假设您希望数据集中所有浮点特征的值都位于 0 到 1 区间内，如果某个特征的值位于 0 到 500 区间内，您就可以通过将每个值除以 500 来缩放该特征。
另请参阅标准化。
scikit-learn
一个热门的开放源代码机器学习平台。请访问 www.scikit-learn.org。
半监督式学习 (semi-supervised learning)
训练模型时采用的数据中，某些训练样本有标签，而其他样本则没有标签。半监督式学习采用的一种技术是推断无标签样本的标签，然后使用推断出的标签进行训练，以创建新模型。如果获得有标签样本需要高昂的成本，而无标签样本则有很多，那么半监督式学习将非常有用。
序列模型 (sequence model)
一种模型，其输入具有序列依赖性。例如，根据之前观看过的一系列视频对观看的下一个视频进行预测。
会话 (tf.session)
封装了 TensorFlow 运行时状态的对象，用于运行全部或部分**图**。在使用底层 TensorFlow API 时，您可以直接创建并管理一个或多个 tf.session 对象。在使用 Estimator API 时，Estimator 会为您创建会话对象。
S 型函数 (sigmoid function)
一种函数，可将逻辑回归输出或多项回归输出（对数几率）映射到概率，以返回介于 0 到 1 之间的值。S 型函数的公式如下：

在逻辑回归问题中， 非常简单：

换句话说，S 型函数可将转换为介于 0 到 1 之间的概率。
在某些神经网络中，S 型函数可作为激活函数使用。
大小不变性 (size invariance)
在图像分类问题中，即使图像的大小发生变化，算法也能成功地对图像进行分类。例如，无论一只猫以 200 万像素还是 20 万像素呈现，该算法仍然可以识别它。请注意，即使是最好的图像分类算法，在大小不变性方面仍然会存在切实的限制。例如，对于仅以 20 像素呈现的猫图像，算法（或人）不可能正确对其进行分类。
另请参阅平移不变性和旋转不变性。
softmax
一种函数，可提供多类别分类模型中每个可能类别的概率。这些概率的总和正好为 1.0。例如，softmax 可能会得出某个图像是狗、猫和马的概率分别是 0.9、0.08 和 0.02。（也称为完整 softmax。）
与候选采样相对。
稀疏特征 (sparse feature)
一种特征向量，其中的大多数值都为 0 或为空。例如，某个向量包含一个为 1 的值和一百万个为 0 的值，则该向量就属于稀疏向量。再举一个例子，搜索查询中的单词也可能属于稀疏特征 - 在某种指定语言中有很多可能的单词，但在某个指定的查询中仅包含其中几个。
与密集特征相对。
稀疏表示法 (sparse representation)
一种张量表示法，仅存储非零元素。
例如，英语中包含约一百万个单词。表示一个英语句子中所用单词的数量，考虑以下两种方式：
要采用密集表示法来表示此句子，则必须为所有一百万个单元格设置一个整数，然后在大部分单元格中放入 0，在少数单元格中放入一个非常小的整数。
要采用稀疏表示法来表示此句子，则仅存储象征句子中实际存在的单词的单元格。因此，如果句子只包含 20 个独一无二的单词，那么该句子的稀疏表示法将仅在 20 个单元格中存储一个整数。
例如，假设以两种方式来表示句子“Dogs wag tails.”。如下表所示，密集表示法将使用约一百万个单元格；稀疏表示法则只使用 3 个单元格：

稀疏性 (sparsity)
向量或矩阵中设置为 0（或空）的元素数除以该向量或矩阵中的条目总数。以一个 10x10 矩阵（其中 98 个单元格都包含 0）为例。稀疏性的计算方法如下：

特征稀疏性是指特征向量的稀疏性；模型稀疏性是指模型权重的稀疏性。
空间池化 (spatial pooling)
请参阅池化。
平方合页损失函数 (squared hinge loss)
合页损失函数的平方。与常规合页损失函数相比，平方合页损失函数对离群值的惩罚更严厉。
平方损失函数 (squared loss)
在线性回归中使用的损失函数（也称为 L2 损失函数）。该函数可计算模型为有标签样本预测的值和标签的实际值之差的平方。由于取平方值，因此该损失函数会放大不佳预测的影响。也就是说，与 L1 损失函数相比，平方损失函数对离群值的反应更强烈。
静态模型 (static model)
离线训练的一种模型。
平稳性 (stationarity)
数据集中数据的一种属性，表示数据分布在一个或多个维度保持不变。这种维度最常见的是时间，即表明平稳性的数据不随时间而变化。例如，从 9 月到 12 月，表明平稳性的数据没有发生变化。
步 (step)
对一个批次的向前和向后评估。
步长 (step size)
与学习速率的含义相同。
随机梯度下降法 (SGD, stochastic gradient descent)
批次大小为 1 的一种梯度下降法。换句话说，SGD 依赖于从数据集中随机均匀选择的单个样本来计算每步的梯度估算值。
结构风险最小化 (SRM, structural risk minimization)
一种算法，用于平衡以下两个目标：
期望构建最具预测性的模型（例如损失最低）。
期望使模型尽可能简单（例如强大的正则化）。
例如，旨在将基于训练集的损失和正则化降至最低的函数就是一种结构风险最小化算法。
如需更多信息，请参阅 http://www.svms.org/srm/。
与经验风险最小化相对。
步长 (stride)
在卷积运算或池化中，下一个系列的输入切片的每个维度中的增量。例如，下面的动画演示了卷积运算过程中的一个 (1,1) 步长。因此，下一个输入切片是从上一个输入切片向右移动一个步长的位置开始。当运算到达右侧边缘时，下一个切片将回到最左边，但是下移一个位置。

前面的示例演示了一个二维步长。如果输入矩阵为三维，那么步长也将是三维。
下采样 (subsampling)
请参阅池化。
总结 (summary)
在 TensorFlow 中的某一步计算出的一个值或一组值，通常用于在训练期间跟踪模型指标。
监督式机器学习 (supervised machine learning)
根据输入数据及其对应的标签来训练模型。监督式机器学习类似于学生通过研究一系列问题及其对应的答案来学习某个主题。在掌握了问题和答案之间的对应关系后，学生便可以回答关于同一主题的新问题（以前从未见过的问题）。请与非监督式机器学习进行比较。
合成特征 (synthetic feature)
一种特征，不在输入特征之列，而是从一个或多个输入特征衍生而来。合成特征包括以下类型：
对连续特征进行分桶，以分为多个区间分箱。
将一个特征值与其他特征值或其本身相乘（或相除）。
创建一个特征组合。
仅通过标准化或缩放创建的特征不属于合成特征。

T
目标 (target)
与标签的含义相同。
时态数据 (temporal data)
在不同时间点记录的数据。例如，记录的一年中每一天的冬外套销量就属于时态数据。
张量 (Tensor)
TensorFlow 程序中的主要数据结构。张量是 N 维（其中 N 可能非常大）数据结构，最常见的是标量、向量或矩阵。张量的元素可以包含整数值、浮点值或字符串值。
张量处理单元 (TPU, Tensor Processing Unit)
一种 ASIC（应用专用集成电路），用于优化 TensorFlow 程序的性能。
张量等级 (Tensor rank)
请参阅等级。
张量形状 (Tensor shape)
张量在各种维度中包含的元素数。例如，张量 [5, 10] 在一个维度中的形状为 5，在另一个维度中的形状为 10。
张量大小 (Tensor size)
张量包含的标量总数。例如，张量 [5, 10] 的大小为 50。
TensorBoard
一个信息中心，用于显示在执行一个或多个 TensorFlow 程序期间保存的摘要信息。
TensorFlow
一个大型的分布式机器学习平台。该术语还指 TensorFlow 堆栈中的基本 API 层，该层支持对数据流图进行一般计算。
虽然 TensorFlow 主要应用于机器学习领域，但也可用于需要使用数据流图进行数值计算的非机器学习任务。
TensorFlow Playground
一款用于直观呈现不同的超参数对模型（主要是神经网络）训练的影响的程序。要试用 TensorFlow Playground，请前往 http://playground.tensorflow.org。
TensorFlow Serving
一个平台，用于将训练过的模型部署到生产环境。
测试集 (test set)
数据集的子集，用于在模型经由验证集的初步验证之后测试模型。
与训练集和验证集相对。
tf.Example
一种标准协议缓冲区，旨在描述用于机器学习模型训练或推断的输入数据。
时间序列分析 (time series analysis)
机器学习和统计学的一个子领域，旨在分析时态数据。很多类型的机器学习问题都需要时间序列分析，其中包括分类、聚类、预测和异常检测。例如，您可以利用时间序列分析根据历史销量数据预测未来每月的冬外套销量。
训练 (training)
确定构成模型的理想参数的过程。
训练集 (training set)
数据集的子集，用于训练模型。
与验证集和测试集相对。
迁移学习 (transfer learning)
将信息从一个机器学习任务迁移到另一个机器学习任务。例如，在多任务学习中，一个模型可以完成多项任务，例如针对不同任务具有不同输出节点的深度模型。迁移学习可能涉及将知识从较简单任务的解决方案迁移到较复杂的任务，或者将知识从数据较多的任务迁移到数据较少的任务。
大多数机器学习系统都只能完成一项任务。迁移学习是迈向人工智能的一小步；在人工智能中，单个程序可以完成多项任务。
平移不变性 (translational invariance)
在图像分类问题中，即使图像中对象的位置发生变化，算法也能成功对图像进行分类。例如，无论一只狗位于画面正中央还是画面左侧，该算法仍然可以识别它。
另请参阅大小不变性和旋转不变性。
负例 (TN, true negative)
被模型正确地预测为负类别的样本。例如，模型推断出某封电子邮件不是垃圾邮件，而该电子邮件确实不是垃圾邮件。
正例 (TP, true positive)
被模型正确地预测为正类别的样本。例如，模型推断出某封电子邮件是垃圾邮件，而该电子邮件确实是垃圾邮件。
正例率（true positive rate, 简称 TP 率）
与召回率的含义相同，即：
正例率正例数正例数假负例数
正例率是 ROC 曲线的 y 轴。

U
无标签样本 (unlabeled example)
包含特征但没有标签的样本。无标签样本是用于进行推断的输入内容。在半监督式和非监督式学习中，在训练期间会使用无标签样本。
非监督式机器学习 (unsupervised machine learning)
训练模型，以找出数据集（通常是无标签数据集）中的规律。
非监督式机器学习最常见的用途是将数据分为不同的聚类，使相似的样本位于同一组中。例如，非监督式机器学习算法可以根据音乐的各种属性将歌曲分为不同的聚类。所得聚类可以作为其他机器学习算法（例如音乐推荐服务）的输入。在很难获取真标签的领域，聚类可能会非常有用。例如，在反滥用和反欺诈等领域，聚类有助于人们更好地了解相关数据。
非监督式机器学习的另一个例子是主成分分析 (PCA)。例如，通过对包含数百万购物车中物品的数据集进行主成分分析，可能会发现有柠檬的购物车中往往也有抗酸药。
请与监督式机器学习进行比较。

V
验证集 (validation set)
数据集的一个子集，从训练集分离而来，用于调整超参数。
与训练集和测试集相对。

W
权重 (weight)
线性模型中特征的系数，或深度网络中的边。训练线性模型的目标是确定每个特征的理想权重。如果权重为 0，则相应的特征对模型来说没有任何贡献。
宽度模型 (wide model)
一种线性模型，通常有很多稀疏输入特征。我们之所以称之为“宽度模型”，是因为这是一种特殊类型的神经网络，其大量输入均直接与输出节点相连。与深度模型相比，宽度模型通常更易于调试和检查。虽然宽度模型无法通过隐藏层来表示非线性关系，但可以利用特征组合、分桶等转换以不同的方式为非线性关系建模。
与深度模型相对。
原文出处：
https://developers.google.cn/machine-learning/glossary?hl=zh-CN

(微生物-疾病 关联分析)Nature子刊：我国学者实现基于粪便微生物标志物的结直肠癌真正早筛

据世界卫生组织（WHO）统计，结直肠癌（CRC）是全球范围内发病率排名第三、致死率排名第二的癌症类型，给患者和社会带来了极大的健康危害和经济负担【1】。结直肠癌可能发生在任何年龄，但大多只影响50岁以上的老年人，年轻人群中结直肠癌的发病率很低。

然而，近30年来，50岁以下的年轻人群中结直肠癌的发病率一直在上升。由于年轻人结直肠癌发病的增加，导致结直肠癌的平均诊断年龄从72岁提前到了66岁。

而就在今日，《美国医学会杂志》（JAMA）发布的最新指南已经建议将结直肠癌的筛查起始年龄降至45岁。这进一步凸显了结直肠癌早筛的重要性。

2021年5月24日，中山大学附属第六医院胃肠病研究所朱立新课题组等在 Nature子刊 Nature Communications 在线发表了题为：Identification of microbial markers across populations in early detection of colorectal cancer 的研究论文。该研究实现了基于粪便微生物标志物的结直肠癌（CRC）真正早筛，即结肠腺瘤（CRA）阶段的筛查。



目前肿瘤的临床治疗手段仍然十分有限，治疗效果并不理想，肿瘤防治的核心仍是“早发现、早诊断、早治疗”。


研究表明，对肿瘤患者在发病早期进行有效的筛查和早期干预，能显著提高患者的生存率（早期患者的5年生存率能达到90%），并节省医疗资源和时间【2】。因为结肠腺瘤（Colorectal Adenoma, CRA）是绝大多数CRC发生必经前体【3】（图1），因此，结肠腺瘤（CRA）阶段的筛查对结直肠癌（CRC）防治至关重要。


图1. CRC“腺瘤-癌”发生发展进程

结直肠癌（CRC）的筛查已受到国家高度重视，然而现有筛查手段（比如传统的影像学、内镜和肿瘤标志物筛查，图2）存在放射性损害、有创、灵敏度较低和依从性过低等局限性，迫切需要技术革新。

随着液态活检技术（无创）的日渐成熟，从血、尿和粪便样本中找寻合适的（分子）标记物已成为癌症筛查技术的主要发展方向。其中由于粪便是消化道代谢的直接产物，因此，对于消化道疾病，特别是结直肠癌（CRC），粪便样本相关检测被寄予厚望。粪便免疫化学检测（fecal immunochemical test, FIT）、粪便隐血检测（fecal occult blood test, FOBT）和现有的粪便微生物标志物检测在结直肠癌（CRC）的诊断中得到广泛应用。

虽然这些方法技术对中晚期结直肠癌（CRC）筛查效果较好，但遗憾的是，对于早期结直肠癌（CRC）的，特别是结肠腺瘤（CRA）灵敏度仍然较差（图2）。


图2. 现有CRC筛查手段的缺点

中山大学附属第六医院胃肠病研究所朱立新教授团队与其联合研究团队整合了来源于多个研究中心的健康对照、结肠腺瘤（CRA）和结直肠癌（CRC）共775例样本的粪便微生物测序数据，对基于微生物标记物的结直肠癌（CRC）早诊早筛进行了系统的研究。

考虑到混杂因素（confounders）对研究结果的影响，该研究首先通过控制混杂因素去除批次效应，确定出结直肠癌（CRC）发展过程中显著性差异且在跨地域的队列中一致变化的细菌。随后，使用递归特征消除法（iterative feature elimination, IFE）筛选具有最强疾病分辨能力的菌种标志物，同时为避免模型的过拟合，本研究构建了10折交叉验证随机森林模型。最终，基于11个（e.g. Christensenellaceae R-7 group sp., Ruminococcaceae UCG-005 sp., Veillonella parvula）和26个（e.g. Streptococcus thermophilus TH1435, Bacteroides dorei, [Clostridium] scindens）细菌相关标志物分别构建了健康/CRA（AUC=0.80）和CRA/CRC（AUC=0.89）分类诊断模型。

其中，健康/CRA模型较先前研究显著提高，例如：Thomas等研究表明由多种微生物构建的跨队列健康-腺瘤模型的AUC值仅为0.54【4】。此外，该模型的灵敏度为0.82，显著高于目前国内唯一获证的早筛IVD产品（诺辉健康的常卫清FIT-DNA检测，其腺瘤阶段早筛灵敏度为63.5%）【5】。

进一步，本研究还通过队列间study-to-study和leave-one-dataset-out（LODO）分析证明了挑选出的标志物具有跨地域队列的通用性。更重要的，健康/CRA和CRA/CRC模型在两套独立队列（验证队列1：CRA患者102例，健康对照70例；验证队列2：CRA患者57例，CRC患者52例）中同样具有优良的表现，其中独立验证队列1的AUC值为0.78，独立验证队列2的AUC值为0.84。此外，本研究所挑选出的标志物能显著地区分其它肠道菌群密切相关疾病：如UC（P=0.045）、CD（P=0.003）、IBS（P=0.009）、NAFLD（P=0.027）和T2D（P=0.020），表明鉴定出的结肠腺瘤（CRA）肠道菌群标志物具有较高的疾病特异性。总体而言，本研究得到的癌症菌种标志物具有强大的结直肠癌（CRC）精准早诊早筛的潜力。

进一步，本研究基于标记基因测序数据预测了微生物的潜在功能，发现：相对于健康状态，肠道菌群标志物Veillonella parvula参与的ADP-L-glycero-beta-D-manno-heptose（ADP-heptose）生物合成通路的丰度以及编码该通路关键酶hldE的基因表达在结肠腺瘤（CRA）中显著增加；进一步，相对于结肠腺瘤（CRA）状态，由肠道菌群标志物Bacteroides dorei参与的menaquinol-10（MK-10）生物合成通路丰度以及编码关键酶menH和menF基因的表达在结肠腺瘤（CRA）中显著增加（图3），该结果在新收集的独立队列中得到验证，为早期结直肠癌（CRC）的治疗提供了潜在靶点。


图3. 微生物标志物参与的CRA潜在发病机制


该研究实现了基于粪便微生物标志物的结直肠癌（CRC）真正早筛，即结肠腺瘤（CRA）阶段的筛查。特别需要强调的是，不同于其它研究使用多类别标记物构建模型，该研究所构建的诊断模型仅仅用到细菌该单一类型标记物，在临床推广上极具优势，将进一步加速结直肠癌（CRC）早筛的进程。此外，本研究确定的与结肠腺瘤（CRA）相关的肠道菌群介导的功能通路变化也为从肠道菌群角度干预和治疗结直肠癌（CRC）提供了新靶标。

中山大学附属第六医院朱立新教授，同济大学附属第十人民医院生物信息系朱瑞新教授、田川博士（朱瑞新课题组原硕士生、现美国礼来高级研究员）和上海交通大学公共卫生学院刘宁宁研究员为共同通讯作者。朱立新课题组博士后焦娜博士和同济大学附属第十人民医院朱瑞新课题组在读硕士生吴苑琪为并列第一作者。

论文链接: 
https://www.nature.com/articles/s41467-021-23265-y
参考文献：
1. Wong, SH and J Yu. Gut microbiota in colorectal cancer: mechanisms of action and clinical applications. Nat Rev Gastroenterol Hepatol, 2019, 16(11): 690-704
2. Ungerer, V, AJ Bronkhorst and S Holdenrieder. Preanalytical variables that affect the outcome of cell-free DNA measurements. Crit Rev Clin Lab Sci, 2020, 57(7): 484-507
3. Strum, WB. Colorectal Adenomas. N Engl J Med, 2016, 374(11): 1065-75
4. Thomas, AM, P Manghi, F Asnicar, et al. Metagenomic analysis of colorectal cancer datasets identifies cross-cohort microbial diagnostic signatures and a link with choline degradation. Nat Med, 2019, 25(4): 667-678
5. http://www.ioncol.com/article/NewsInfo.aspx?id=5623

JGG：肠道菌群与COVID-19重症风险密切关联(机器学习区分COVID-19重症-轻症患者肠道核心物种)

2021年5月3日，JGG 在线发表了西湖大学郑钜圣教授、郭天南教授、中山大学陈裕明教授、中国科学院微生物研究所王军研究员团队合作题为“Gutmicrobiota, inflammation and molecular signatures of host response to infection”的研究论文。该研究通过结合多维组学（蛋白质组、肠道微生物组及粪便代谢组）生物信息学分析，探索了肠道菌群、炎症因子以及COVID-19重症标记物间的关联。该研究为研究肠道菌群与宿主免疫之间的相互作用，评估感染COVID-19后的重症风险提供了新线索。
DOI: https://doi.org/10.1016/j.jgg.2021.04.002

新型冠状病毒 (COVID-19) 自2019年12月首次发现以来在全球范围内肆意传播。至2021年5月，全球逾1亿5千余万人感染。由于不同个体感染COVID-19后的重症风险差异较大，因此，确定影响COVID-19重症风险的因素对预防和治疗COVID-19具有重要意义。该团队在前期的研究中，通过对13名COVID-19重症患者和18名COVID-19轻症患者的血清进行分析，发现并验证了22个蛋白质和7个代谢物可有效区分COVID-19重症与轻症患者。基于COVID-19重症蛋白质标记物，该研究构建了重症风险评分 (Proteomic risk score, PRS)。经统计学分析验证，PRS越高，患者感染COVID-19后的重症风险也越高。而宿主感染COVID-19后刺激免疫系统产生大量的炎症因子，是轻症向重症转换的重要节点。该研究在一个逾1700人的非COVID-19老年人群队列中发现，在年龄较高的人群中(≥58)，COVID-19标记物PRS与炎症因子存在正相关。该结果与流行病观察性研究结果吻合。

1. 重症风险评分体系构建及验证；2.重症风险评分与炎症因子关联分析；3. 基于肠道菌群构建机器学习模型预测重症风险标记物；4. PSR相关肠道菌群与炎症因子关联分析；5. 核心菌种相关的粪便代谢物及代谢通路分析；6. 核心菌种相关的生活方式及表型分析
肠道菌群失调与多种疾病，如自身免疫病、过敏和慢性炎症等疾病的发生发展相关。已有研究表明，肠道菌群组成在COVID-19患者与健康个体间存在较大差异。为了探究肠道菌群与COVID-19重症风险间的关系，该研究采用机器学习发现了20个肠道核心菌种。该核心菌群不仅可以有效预测COVID-19重症蛋白质标记物，而且还与炎症因子高度相关。粪便代谢组学分析表明，核心肠道菌群主要与氨基酰基-tRNA生物合成途径、精氨酸生物合成途径以及缬氨酸、亮氨酸和异亮氨酸生物合成途径密切相关，这些途径对维持人体免疫稳态至关重要。

西湖大学苟望龙博士、付元庆博士、岳靓博士、蔡雪博士、帅梦雷博士、许凤喆博士，中山大学陈耿东博士为该论文的共同第一作者，郑钜圣教授、郭天南教授、陈裕明教授和王军研究员为共同通讯作者。相关研究得到西湖实验室，国家自然科学基金、浙江省“万人”计划建设经费的支持。 

(无监督聚类发现人类疾病与肠道微生物组相关性及因果性：人类基因能决定肠道微生物组)西湖大学/中山大学/微生物所合作揭示中国人群肠道菌群与复杂疾病的关系

近年来，有越来越多的研究表明，一个人在患病和健康的不同状态下，肠道菌群的构成是不一样的。那么，到底是微生物的变化导致疾病的发生，还是疾病产生后导致了微生物的变化？微生物与疾病，谁是因，谁是果？

近日，西湖大学郑钜圣课题组在关于中国人群中宿主遗传、肠道微生物以及复杂疾病的相互关系上有了新的发现。研究首次揭示了影响中国人群肠道菌群的基因变异位点，发现了部分疾病与肠道菌群的因果关系，并同时对几十种复杂疾病的肠道菌群进行了特征归类。
The interplay between host genetics and the gut microbiome reveals common and distinct microbiome features for complex human diseases. Microbiome 2020,8,145. https://doi.org/10.1186/s40168-020-00923-9

中国人群中，宿主基因也会影响肠道菌群
目前已经有较为详尽和完整的证据表明，肠道微生物组的组成或结构可能受到宿主遗传的影响。也就是说，由于我们的基因不同，身体内的某些肠道菌群的含量就有可能不同。然而，已有的相关研究大多在欧美人群中展开，在亚洲人群中，肠道微生物与宿方遗传变异这一相关性尚未被阐明。
不同人群之间，不论是基因还是肠道菌群，都存在相当大的差异，因此十分有必要着手在亚洲人群中开展相关研究。
为了填补这一领域的空白，郑钜圣课题组与合作者一起开展了一项大型中国人群队列研究。他们纳入约4000名中老年居民（45-75岁），收集到约2000份的粪便样本，并对其肠道菌群的组成进行了检测分析。
要在人群水平探索肠道菌群与复杂疾病之间的因果关系，首先要揭示影响菌群的遗传位点。就是说，我们需要了解人体的基因是如何来影响肠道菌群的多少。
研究团队对203个肠道微生物分类群进行了微生物组全基因组关联分析，首先明确在亚洲人群中，遗传基因同样是能够影响某些肠道菌群的——我们的基因不同，肠道菌群的含量就不同。
在发现了一系列能够影响肠道菌群变化的宿主基因之后，研究团队构建了遗传风险评分，以此来评价人体基因变异和肠道菌群的关联。随后，研究团队应用双向孟德尔随机化方法探究了肠道微生物与几十种人类复杂疾病或性状之间的相互因果关系。
*孟德尔随机化（Mendelian randomization）是在遗传流行病学研究中用遗传变异作为工具变量来调查暴露和结果之间的因果关系的一种方法。
课题组通过分析发现，微生物与疾病，并不能简单定义谁是因谁是果。在不同的复杂疾病中，肠道菌群时而为因，时而为果，需要个案分析。
比如，在东亚人群中，Saccharibacteria门（糖精杆菌，其中一类肠道菌群，也称为TM7门）可以通过影响肾功能生物标记物（即肌酐和肾小球滤过率），从而改善肾功能。在这里肠道菌群是因。
而房颤、慢性肾病和前列腺癌这些疾病，则会对部分肠道微生物含量产生直接影响。在这里，肠道菌群是果。
帕金森和结直肠癌的肠道微生物特征竟然很相似！？
在此研究过程中，除了部分疾病与肠道菌群的因果关联之外，课题组还利用独特的肠道微生物大数据发现，不同的复杂疾病所对应肠道菌群特征，存在各自的独特性，同时也存在很强的相似性。
在对基于微生物预测的疾病风险评分进行无监督聚类后，郑钜圣团队发现帕金森与结直肠癌之间，慢性粒白血病和系统性红斑狼疮之间具有相似的肠道微生物特征，并在另一个独立队列中重复了相关发现。 

基于肠道微生物预测的疾病聚类
以上一系列基于中国人群的原创性发现，为深入理解肠道菌群与复杂疾病的因果关联奠定了基础，不同疾病相似菌群和特殊菌群的特征分析可能有助于科学家理解人类复杂疾病之间的关系与发病机制，为进一步的新型药物开发提供启示。
参考文献：
Xu FZ, Fu YQ, Sun TY, Jiang ZL, Miao ZL, Shuai ML, Gou WL, Ling CW, Yang J, Wang J, Chen YM, Zheng JS. The interplay between host genetics and the gut microbiome reveals common and distinct microbiome features for complex human diseases. Microbiome 2020,8,145. https://doi.org/10.1186/s40168-020-00923-9

Cell Trends综述精选：人工智能在生物医学领域的应用

本综述精选重点介绍使用基于人工智能（AI）的方法来解决跨学科的一系列复杂问题，以及Trends in Chemistry本月关于机器学习（machine learning)在分子与材料研究中应用的特刊。本精选文章全部开放下载。
在过去的十年里，这一高度跨学科的领域取得了惊人的进展，这归功于来自生物学、化学、数学、计算机学以及其他学科的杰出科学家。然而，这些不同学科的结合对关键发现和能力的广泛传播提出了挑战。此跨学科综述精选探索基于AI的算法如何运用于不同的领域，这些领域是疾病监测、诊断及治疗、分子和材料的发现、合成及优化、对人类及机器在认知和神经计算中的了解、对大型多层数据集（例如基因组学和植物胁迫表型的数据集）的解释与理解。
个体化和精准医疗的使能技术

在这篇综述中，来自新加坡国立大学的Dean Ho研究团队将重点介绍发展使能技术所取得的关键突破，这些技术可以推动实现个体化和精准医疗的目标；此外还将揭示目前存在的挑战，这些挑战一旦解决，可能会形成前所未有的力量，有利于实现真正的个体化护理。

▲长按识别二维码阅读论文
机器学习给植物胁迫表型带来的挑战与机遇

在这篇综述中，美国爱荷华州立大学Arti Singh研究团队提出了一种利用机器学习（ML）技术的总体策略，该策略支持在不同类型的胁迫、程序目标和环境的多个尺度上系统地应用植物胁迫表型。

▲长按识别二维码阅读论文
下一代表型筛选在传染病早期药物研发中的运用

在这篇综述中，法国巴黎巴斯德研究所和韩国巴斯德研究所的Spencer L. Shorte等研究人员回顾了最先进和新兴的技术，这些技术不仅为从复杂的表型筛选中利用有效信息提出了新的策略，也为增强转化药物研发的强大效用提供了全新的思路。在细胞、分子和生物信息学技术领域所取得的进步已遥遥领先，复杂的表型筛选也许不会再被认为是一种障碍，而会被当作是发现传染病化疗方法的催化剂。

▲长按识别二维码阅读论文
黑匣子的开启：遗传学家对机器学习的阐释

在这篇综述中，来自美国密歇根州立大学、澳大利亚圣文森特医学研究所的Christina B. Azodi团队以及来自美国密歇根州立大学的Shin-Han Shiu的团队将讨论可解释机器学习 (ML) 的重要性、用于解释ML模型的不同策略，以及这些策略应用于实践的例子。最后，将指出可解释ML在遗传学和基因组学中所面临的挑战和未来的发展方向。

▲长按识别二维码阅读论文
机器的兴起：癌症诊断的深度学习进展

在这篇综述中，加拿大不列颠哥伦比亚大学Stephen Yip团队概述了深度学习应用于癌症诊断的当前进展和最新技术，并讨论了该技术广泛应用于临床部署所面临的挑战。

▲长按识别二维码阅读论文
大脑中的分布式强化学习

机器学习的最新进展揭示了一套生物学上合理的算法，可以让哺乳动物从经验中重建对奖赏预测的分布。在这篇综述中，哈佛大学医学院Jan Drugowitsch及哈佛大学脑科学中心Naoshige Uchida团队合作回顾了这些算法的数学基础以及它们神经生物学实现的初步证据。

▲长按识别二维码阅读论文
人工智能和动物的常识

常识问题仍然是人工智能发展的一大障碍。在这篇文章中，来自DeepMind、伦敦帝国理工学院、剑桥大学的Murray Shanahan等研究人员认为，人类的常识是建立在许多其他动物所拥有的一系列基本能力之上的，这些能力与理解物体、空间和因果关系有关。动物认知领域的研究可以为深度强化学习（RL）的进展提供启发。

▲长按识别二维码阅读论文
医学领域中人工智能缺失的部分

医疗保健系统的利益相关者都在寻求将人工智能（AI）纳入他们的决策过程。在这篇文章中，美国威尔康奈尔医学院Olivier Elemento团队对一些关键因素进行了讨论，这些因素应该被优先考虑，以使人工智能在整个医疗保健链中充分且成功地发挥其价值。研究人员特别强调了对模型可解释性的关注和在人工智能框架内整合不同类型数据的重要性。

▲长按识别二维码阅读论文
Trends in Chemistry 特刊：分子和材料的机器学习
这期特刊重点介绍机器学习为新分子和材料的合成、发现和优化循环系统提供信息、桥梁以及帮助的一些重要方法，如果没有这些方法，开展和分析这些工作将会极其困难、耗费成本高或人工密集。

面向机器学习增强的高通量实验

在这篇综述中，来自美国麻省理工学院的Klavs F. Jensen团队重点举例介绍了机器学习（ML）和高通量实验（HTE）的最新发展，表明了它们的集成效用。研究人员分析强调了这两个领域的互补性，同时也揭露了一些可以并且应该克服的障碍，以充分利用ML和THE的融合，加速化学研究的进展。

▲长按识别二维码阅读论文
定义和探索化学空间

设计具有理想性质的功能分子是一个具有挑战性的多目标优化问题。在这篇综述中，来自美国麻省理工学院的Connor W. Coley提供了一些定义和探索化学空间的算法方法的概述。研究人员强调了机器学习的潜在作用和对综合可行性的考虑，并总结了这些方法未来发展和评估的重要方向。

综述｜Water Research: 环境中抗生素抗性基因的溯源追踪--挑战、进展和展望

论文题目：Source tracking of antibiotic resistance genes in the environment — Challenges, progress, and prospects
期刊：Water Research
IF：9.13
发表时间：2020
摘要
抗生素耐药性已成为全球公共卫生关注的焦点，抗生素耐药性的流行使得临床上一些常见的感染感染性疾病无药可医。
鉴于这一现象的普遍性，人们将越来越多的注意力转向可能有助于抗生素抗性基因 (Antibiotic resistance genes, ARGs) 在临床领域之外传播的环境途中。
过去数十年的研究清楚地证明了，随着人类活动对环境影响强度的增加，ARG污染的的程度也随之增加。
但是，在这些前人的研究中，对ARG准确的溯源跟踪一直受到各种因素的影响，例如环境中原发性ARG水平、不同环境的时空差异、环境抗性组的复杂性以及方法学的局限性。
快速发展的宏基因组学结合基于机器学习的分类技术为追踪环境中ARG污染的来源开辟了一条新途径。
在这篇综述中，将讨论有关环境ARG污染溯源追踪研究中的挑战和进展，并且会特别关注近期宏基因组学的方法进展。
本篇综述提出了一个基于宏基因组学的综合框架，在该框架中结合了实验设计和宏基因组学分析，将有助于实现在ARG污染环境中进行可靠的溯源追踪。
引言
自从20世纪初期第一种有效的抗生素（即磺酰胺和青霉素）被发现到现今为止，在细菌和抗生素之间的“军备竞赛”中，人类医学开始衰落，细菌开始了反击。
世界银行警告说，到2050年，抗生素耐药性可能导致全世界每年死亡超过1000万人并会带来超过100万亿美元的全球经济损失。
然而，抗生素耐药性的控制策略尚处于起步阶段，目前，人们越来越关注生态环境是临床领域外导致抗生素耐药性问题的重要介质。
长期以来，研究人员一直提出将ARGs作为污染物，在环境的常规污染物监测中通常不包括ARGs，但是其有可能造成不利的生态和人类健康影响。
与其它化学污染物不同，ARG在环境中的行为通常与其生物学特征有关，例如垂直和水平的遗传流动。
实际上，直到21世纪初，将环境中的ARGs作为污染物来看待才得到普遍的认同。
Pei和Pruden等人（2006）首次在科罗拉多州北部的不同环境中将ARGs作为新兴污染物进行了研究，他们发现在城市/农业直接影响的环境中，几种四环素和磺酰胺抗性基因的浓度明显高于原始环境和受人类影响较小的环境。
随着更多的调查研究，人为活动与ARG环境污染之间正相关的证据被发现，因此，识别可能的来源并了解它们与ARG发生的关系对于控制ARG在环境中的传播至关重要。
多种方法已经被应用于鉴定环境中ARG污染的可能来源。
由于大部分环境细菌无法被分离培养以及不确定的基因型与表型的关系，依赖于培养的方法几乎无法有效追踪ARG的污染。
尽管这些缺点可以使用一些非培养的方法得以解决，例如基于PCR的技术，单由于需要专用的引物，因此只能从样品中检测或定量某些特定的ARG。
快速发展的高通量测序技术提供了一种全新的方法，可对样品中遗传元件的多样性和丰富性进行前所未有的观测，为通过统计学分析定量对环境中ARG污染水平进行溯源跟踪提供了机会。
鉴于环境中的抗生素耐药性与临床领域之间的联系日益实现，迫切需要一个ARG溯源追踪平台，来有效地评估和处理环境ARG污染。
在这篇综述中，我们提供了方法开发的概述，并重点介绍了环境中ARG污染定量来源跟踪的最新进展，讨论了作为一种新兴污染物的ARGs的相关特征以及在环境中追踪其归驱的挑战。
环境中的抗生素、抗生素耐药性和ARGs
作为微生物生长必不可少的防御体系，抗生素是微生物生态系统的的天然组成成分之一，在疾病控制发现并使用抗生素引发了医学革命，极大地促进了天然和合成抗生素的发现和生产，据估计，全球每年会生产约100,000吨的抗生素。
与抗生素一样，抗生素耐药性也是细菌中一种古老的保护机制，可以防御细菌自身或者群落中其它微生物产生的抗生素。
抗生素耐药性通过多种机制来抵抗抗生素，包括防止抗生素与靶标物质接触、直接对抗生素进行修饰或改变抗生素的靶标结构等等，这些抗生素抗性功能主要由突变或外源获得的ARG编码。
多年的研究表明，抗生素消耗量的增加会导致细菌中抗生素耐药性和ARG的快速发展，这说明抗生素、抗生素耐药性和ARGs之间可能存在明显的相关性。
然而，在复杂的环境中，这种关系可能会被人为或其自身相关的化学、生物学特征所破坏。具体而言，ARGs作为遗传单元可以通过由接合转移、转导和转化等遗传机制进行传播，此外，检测方法的局限性也可能会影响这种相互关系。
我们经常会在不同的研究中发现相互矛盾的陈述，一方面，在城市河流等典型的抗生素污染环境中，抗生素浓度与ARG含量之间存在正相关，另一方面，在有限的抗生素污染环境中，甚至在没有抗生素的情况下，也可以发现很高的ARG的多样性和污染水平。
这些研究表明，抗生素、抗生素耐药性和ARG并不总是直接相关的，因为它们之间的关系一直在环境中不断受到各种复杂条件的影响。
追踪环境ARGs的挑战
一旦将ARGs或携带ARG的细菌释放到环境中，它们将通过与原生微生物群落在生理和生态相互作用促进遗传物质的交换。此外，ARG的归驱将在一定的时空范围内受到动态环境条件的影响，所有这些条件或过程都会对环境中ARG的追踪造成困难。

原生ARG水平的干扰
抗生素耐药性是一种自然现象，其出现早于现代医疗中抗生素的使用，因此，某些类型或含量的ARGs是环境中天然存在的。为了精确地识别由其它来源导致的ARG污染，首先就必须要排除这些环境与安生ARG的干扰。
实际上，已经在自然环境中发现了具有临床重要性的ARG，在自然土壤甚至3万年前的永久冻土沉积物中，已经发现了编码β-内酰胺、四环素和糖肽抗生素抗性的基因。此外，在一项跨地理梯度的污染区域研究中，确定了背景环境中存在一个本地tet基因库，维持这些库的生态压力可能独立于污水处理厂和泻湖等污染源。
为了解决背景影响，Storteboom等人对位于落基山的Cache-La-Poudre河原始上游地区与下游受城市和农业影响的地区的ARG水平进行了比较。因此，在环境研究中包含本土区域的对照样本以区分本地和异源的ARG对ARGs污染的溯源跟踪至关重要。
此外，原始环境可能非常容易受人为干扰的影响，这种原始环境的微生物群落在受到低水平人类污染干扰的情况下就会发生快速的变化。因此，对环境ARG的来源跟踪需要进行全面调查以识别真正的本土原始环境对照。
具有任何人为干扰历史记录的环境都应排除在自然环境对照之外，因为就算污染已经被控制了很长时间，其造成的ARGs污染依然可能会持续存在。
时空差异的影响
在大多数ARG源追踪研究中，通常情况在是特定区域的特定时期内采集样品，作为覆盖污染梯度的代表性样本，为了确保这些样本中的ARGs反映了环境中的人为干扰程度，在实验设计阶段综合的考虑时间和空间的变化是非常有必要的。
气候效应和季节性人为活动等季节性因素可能会在某些地区稀释或富含ARG的过程中施加额外的作用力。此外，如果不考虑其他重要的水文地质特征，可能会高估或低估人为因素对ARG的影响。
除环境条件的变化外，生物量密度沿时空尺度的变化同样可能会对ARG的发展产生影响，当ARGs进入到高生物量密度的受体环境中时，强烈的微生物相互作用会增强水平基因转移和随后的ARG传播，但是在低生物量密度的环境中情况并非如此（微生物密度越大，互作越频繁，ARGs的水平转移越频繁）。
金属(共选择物质)可促进细菌多重耐药
近年来，其它共选择压力对ARG增殖和传播的重要作用也已被大众接受。
反复观察到的的遗传物质共存表明，金属暴露（例如铜，锌和汞）可以促进对多种抗生素的抗性。目前，已经提出了一些金属驱动的对抗生素抗性的共选择的机制，主要包括：i）共抗性，编码不同抗性表型的基因物理连接在一起；ii）当抗生素或金属可以触发常见的生理反应时可能发生的交叉耐药性；iii）共同调节，其可以协调对抗生素和金属的抗性的转录和翻译应答。
此外，杀菌剂(如季铵化合物)，在作用机理上显示出与抗生素的相似之处，这也可能促进ARG在环境中的持续存在。
这些关联性的研究表明，抗生素以外的其它污染物可能代表了广泛的、顽固的选择压力，从而导致环境中ARG的传播和维持。
这种现象已在污水处理厂和农业污染地区的ARG污染源追踪中得到了例证，其中ARG的增殖与非抗生素药物显着相关，而ARG对应的抗生素却几乎未被检测到。
因此，由于基因单元携带的基因不仅只是ARG还包含多样的其它基因，任何能够对这些其它基因发生选择的压力都会间接选择AEG，所以ARG的富集并不总是需要抗生素的选择压力，取而代之的是，其它因素的供选择可能是ARG在环境中普遍存在和持续的主要驱动力。
环境ARG污染的遗传多样性
过去，对环境中的ARG污染进行溯源追踪的重点一直是环境中流行的某些特定ARG，但是，在大多数情况下，这些研究都忽略了环境中抗生素抗性组的总体情况。
在一个给定环境中，整体抗生素耐药性是所有ARGs的集合，被称为“抗生素抗性组”，既包括固有的和获得的ARG（通过固有的微生物特征编码具有固有耐药性的蛋白质以及通过突变或水平转移获得耐药性基因），也包括前体和隐性ARGs（编码具有适度抗生素抗性活性的蛋白质和未表达或表达不足的抗生素抗性的蛋白质）。
基于宏基因组学的大规模研究已经证明了临床环境意外的ARGs具有极高的深度和生态位，特别是在承受人为压力的环境中通常具有大量的ARG。与其他环境相比，这些环境中相对较高的生物密度大大增加了ARG的转移和增殖的机会。
基于标志基因的环境ARG污染溯源
自从在21世纪初意识到环境ARG污染对公共卫生的潜在威胁以来，在过去的几十年中，针对鉴定ARG可能来源和促进风险管理方法的开发一直都没有停歇。
由于缺乏详尽的工具来测试涵盖所有抗生素类别的抗性基因，因此通常采用传统PCR方法检测特定标志基因的频率，以指示某个区域内的总体ARG污染。
目前，已经提出了几种ARG标志基因（即用于环境检测的经过充分研究的ARG），例如使用tetW和tetM指示四环素(tetracycline)抗性基因、sul1和sul2指示磺胺(sulfa)抗性基因、blaCTX-M和blaTEM指示β-内酰胺(β-Lactam)抗性基因等等。最近，有研究人员提出临床第1类整合子的整合酶基因intI1可以作为人类活动ARG污染的指示者。

但是，传统qPCR技术在通量上的限制很大程度上阻碍了人们对各种不同环境中抗生素抗性组的全面了解。最近发展的qPCR芯片技术允许同时对数百个目标基因进行定量，并大大提高了定量能力，它已成功地用于多种环境体系的研究中。
除此之外，Gatica等人开发了一种高通量的扩增子技术，可以对环境样品中所有整盒子的基因盒基因进行表征，并将其应用于污水处理厂废水对下游受体环境的影响的研究中。
在使用单个或多重标志ARG的技术进行ARG污染溯源追踪时，一个主要问题是对如何识别背景环境的干扰。理想情况下，此类标志基因应在人为来源中丰富，而在天然生态系统中则很少，然而，由于在各个生境中报告了重叠的ARG库，并且在原始地区频繁出现一些典型的标志ARG，因此，在一定的时空范围内，缺乏对标记ARG分布图的严格预先测量，可能会在很大程度上造成混淆溯源跟踪结果。
除了检测频率外，还可以通过从潜在的污染源与受影响的环境之间的标志ARG系统发育关系来解决对ARG污染的跟踪。
在微进化的背景下，独特的环境选择性压力可以产生特定的ARG辩题，从而为ARG污染溯源提供了有用的遗传信息，以区分环境中不同的人为干扰所引入的ARGs。
这一假说最近在一项比较受污染和未受污染的河流抗生素抗行足的研究中得到了证实，研究人员发现blaCTX-M基因的核苷酸序列在被污染的和未被污染的抽水生态系统之间存在明显不同。此外，污水处理厂和牲畜场特有的典型标记ARG的序列变体tetW和sulI也已用于识别下游/邻近环境中相应的ARG污染情况。
为了明确地识别来源，各种统计分析已应用于基于标记基因观察结果的解释。除了基本的丰度增加/减少和不同环境的主坐标分析之外，目前还开发了强大的工具来解释生物/非生物现象引起的ARG变化。
通用线性回归（GLR）模型被证明是一种有效的工具，在一项应用GLR模型追踪河流ARG的研究中，对sul1和tetW基因的数量进行了建模，通过评估详尽的GLR模型集解决了上游人类活动与河流ARG之间的相关性，最终该研究的研究人员建议使用tetW：sul1的比率(即四环素抗性基因与磺胺抗性基因的比率)作为评估河流上游污染的指标。
高通量qPCR芯片(大于350个样本，可用于检测易感基因SNP位点)
高通量qPCR芯片的应用也鼓励使用合适的高通量工具来同时处理标记ARG，例如RandomForest预测分析。
尽管做出了这些努力，在ARG污染溯源跟踪研究中，精确的定量仍然是重大挑战。而且，在标记物-ARG定量分析中，PCR方法固有的偏差（如扩增效率和引物特异性）是不可避免的，迫切需要有效的新型工具来准确追踪ARG来源。
新的方法和未来的路
高通量测序技术的发展可以大规模并行定量捕获样本的遗传信息，其可以规避传统基于PCR的方法的局限性。

通过在序列处理和功能注释中生物信息学工具结合，基于高通量测序的宏基因组学技术已被广泛用于各种生境的ARG分析。并且，多项宏基因组学研究表明，ARG的组成特征与人为干扰水平显着相关，证实了宏基因组学有能力跟踪区域性ARG污染。
宏基因组学在环境污染跟踪中的成功应用，以及不断发展的计算方法和流程，建立精确的ARG溯源跟踪框架铺平了道路。
宏基因组溯源的早期工作
在探索宏基因组学在源跟踪中的早期工作中，主要关注于可能的ARG来源（如污水处理厂和农场）的存在或不存在相关基因丰度的增加或减少之间的关系。
将宏基因组学纳入框架中以监测环境中ARG的水平也为早期风险管理提供了一种新颖的方法。科学家探索使用宏基因组数据进行环境健康检测的潜力。他们使用了基于宏基因组流行病学的方法来开发了一种多变量指数，以量化水生环境的抗生素耐药性的潜在风险。
机器学习在定量溯源中的可能应用
尽管宏基因组学在评估和跟踪ARG污染方面得到了早期的示范效用，但仍然没有充分利用大量的信息数据，并且溯源跟踪中的定量框架问题仍然有待解决。
为了通过宏基因组学实现精确的来源判定，需要一种新颖的统计上稳健的分类方法，以解密嵌入在包含数千个ARG的丰度矩阵中的定量信息。
通过利用全面的数据来获取样本唯一的模式，机器学习分类方法已成为当今大数据时代的宝贵工具。最近，随机森林、SouceTracker和FEAST等机器学习的方法已经成功的应用在微生物群落研究中。特别是其中的SourceTracker方法 (是一个R语言脚本，用于做微生物来源分析：https://vlambda.com/wz_x3V6nwZ58A.html)，已经被成功的应用于ARG污染溯源追踪的研究中，因此，使用机器学的方法对环境ARG污染进行溯源追踪是十分有前景的。
由于机器学习分类使用源库作为训练数据集来识别不同源之间的唯一模式，因此，进行仔细的实验设计以包括代表性的高质量源样本是进行精确源跟踪的前提，应该进行包括地理和时间可变性在内的详细调查，以明确识别可能的污染源。
任何源跟踪分析中的一个主要干扰是背景噪声，这会导致假阳性信号和不必要的关注，通过在源库中包含原始环境，可以识别本地ARG的背景污染信号，在下游溯源分析中将其排除。
基于源库的训练，可以通过体外实验进一步检查和改进机器学习分类的性能，通过按特定比例混合源样品和测试环境样品最为分析方法的输入，可以有效的评估方法的准确性和有效性。
为提高分析结果的准确性，应该考虑SourceTracker预测的源贡献比率的相对标准偏差，尤其是对于对样本的贡献较低的源，并且应该以多次运行为基础来确定结果。
诸如增加稀疏深度之类的参数设置更改已显示出可增强运行一致性并区分假阳性来源，但是，仍然需要更严格的评估，包括重新启动次数、吉布斯采样的burn-in、稀疏深度、alpha和beta Dirichlet超参数等等，在不久的将来对这些技术参数进行系统评估，将可以充分利用机器学习分类的潜力，以高特异性和高灵敏度从而更好的量化ARG污染的多源贡献。
其它基于宏基因组的溯源研究
除了总体ARG图谱之外，宏基因组学还提供了环境样本中基因组水平上每个个体ARG的高分辨率结果，从而指导基于指示ARG的分析。此处，指示ARG的定义是通过统计方法，例如使用R包“labdsv”(主要用来做微生物指示种分析) 中的“indval”函数对频率和丰度进行分析，从而从宏基因组数据集中检索到的特定ARG。
为了识别表征针对每个不同污染源的指示剂ARG，首先要排除那些在不同源中同等流行或目标源样品中偶发检测到的ARGs。因此，应同时考虑来源检测的特异性和敏感性，仔细选择合适的统计方法。
与传统标志基因相比，从宏基因组学数据中选择的ARG指标在区分不同来源方面效率更高。作为一种补充方法，这些指标ARG将有助于解决ARG污染溯源追踪所面对的一些问题。
在污染源占比比较低的污染环境中，使用SourceTracker的结果通常会有很大的变异幅度，基于这些指示ARG的PCR技术可以作为高灵敏度的补充方法。此外，SourceTracker在区分相似成分的来源时可能会遇到一些问题，该问题可以使用具有高度特异性的经过统计验证的指标ARG来解决。
通过使用适当的工具，宏基因组学可以通过将短读长片段组装到连续片段，来检索完整的基因及关联的上下游信息，因而具有特定核苷酸改变和系统发生关系的污染源源特异性ARG变体可以在污染的环境中进行被识别和跟踪，无需依靠额外的繁琐的PCR和测序，宏基因组学就快速的完成基于ARG变体的溯源追踪过程。
此外，组装的contig可以在编码的ARG的附近提供基本的上下游信息，可以检索出可移动的遗传单元和与宿主相关的特征，从而进一步评估ARG污染的风险，其中含有高可移动性和致病性的ARGs污染在环境风险评估中将成为需要特别关注的问题。
通过进一步的序列处理，可以获得样品中存在的高质量的ARG的完整或原始宿主基因组，一次来追中携带ARG的种群。
宏基因组ARG分析的工具和数据库
不断降低的测序成本以及生物信息学分析技术的发展极大地促进了宏基因组学在各个领域的广泛应用，在宏基因组学的ARG分析中，对ARG基因的准确注释对于下游分析至关重要，这一过程现在可以在reads和contigs水平上进行。
可以使用诸如BLAST、BWA或Bowtie2之类的比对工具将测序平台产生的reads直接与参考数据库进行比对，然而，这种方法可能会增加注释的假阳性比例，因为从局部序列同源性出发，一个特定ARG的reads可能会与其他ARG出现虚假的匹配（相当赞同，比如我就将很多低质量的带有polyC的短reads误注释到了人类疱疹病毒上）。
仔细的比对参数测试将有助于减少这种注释偏差，其中相似度之类的参数如果过高（例如>90％）会减少检测基因同源性，而过低（例如<70％）会增加基因注释背景噪音。
随着多种拼接工具（MetaSPAdes、IDBA-UD、MEGAHIT）的发展，使得通过长连续contig进行深入的遗传特性分析成为可能。但是，不恰当的装配，尤其是在测序深度不足的样品中，基于拼接的方法可能会丢失那些低丰度的ARG（不赞同，可能是我的深度太足了？发现基于组装的方法鉴定的物种数量和准确度反而出乎意料地优于reads注释）。
与基于拼接的分析相比，基于reads的方法可以更广泛获得复杂群落中丰度较高和较低的ARGs，并且，基于reads的分析通常需要更少的分析时间和计算资源，这对于ARG污染的快速检测和管理是至关重要的。
在ARG污染的溯源跟踪研究中，作者建议利用基于reads的注释捕获整个样品中ARG的总体图谱，之后进一步使用基于拼接的分析来考察特定的ARG。
无论是基于reads的还是基于拼接的ARG分析在很大程度上都要依赖于经过整理的数据库来进行ARG注释，因此，开发和维护高质量的公共ARG数据库对于在ARG污染中实现有效的溯源追踪至关重要。
通用的并持续更新的数据库，例如CARD和SARG涵盖了大量不同类型的ARGs，此外，还有专门针对特定基因家族的特定ARG数据库，例如专注于β-内酰胺酶的LacED、Lahey和CBMAR。还有诸如PointFinder和CARD-RGI之类的工具，用于检测能提供细菌耐药性的染色体突变。
鉴于在各种功能宏基因组学研究中发现了越来越多的新型基因，FARME和ResFinderFG中包含一些其它数据库中没有的经过筛选的ARGs。
最近，使用隐马尔可夫模型进行ARG分配，显着提高了检索那些具有抗生素抗性功能域而通过相似性又无法识别的的基因的能力，SARGfam和Resfam中包含着ARG家族的蛋白质及其隐马尔可夫模型。
在不同的数据库中，所包含的ARG的数量和多样性可能相差非常多，这将在很大程度上影响注释的结果，在分析环境和临床起源的样本时，这一点尤其明显。因此，建立数据库和开发具有足够检索能力的算法对于检查环境抗生素耐药性至关重要。
在以上数据库中，作者建议研究人员根据样本类型和研究目的选择合适的数据库，全面的数据库包含所有ARG变体，在从复杂的环境群落中识别ARG时特别有用，而专门的数据库可以更好地为表征特定和新型ARG服务。
结论
本文提出一个基于宏基因组学的综合框架，用于对ARG污染进行溯源跟踪，宏基因组是框架中的核心部分。

在建立定量溯源追踪框架中有3个关键的问题：
1) 需要非常仔细的确定研究的地理和时间尺度，从而确定研究中作为源和用于溯源的样品，并且还要认为的制作特定比例混合的测试样品用于方法的验证；
2) 使用高质量的参考数据库对宏基因组数据集进行注释，对样本的ARG的丰度矩阵以进行机器学习分类，使用源样品训练分类模型，使用人造样品评估分类模型，之后应用其分析污染环境样品，在此过程中要进行多次的机器学习并不断优化调整参数；
3) 使用宏基因组分析识别指示ARG、靶标ARG的序列变体和携带ARG的基因组，基于这些特定基因/基因组的互补分析将有助于解决在跟踪低比例和重叠源上的挑战。

Briefings in Bioinformatics: 电子科技大学邹权组研发自动构建基于肠道微生物平衡的疾病预测模型及微生物生物标志物发掘平台

DisBalance: 自动构建基于肠道微生物平衡的疾病预测模型及微生物生物标志物发掘平台
DisBalance: a platform to automatically build balance-based disease prediction models and discover microbial biomarkers from microbiome data
Article，2021-04-08
Briefings in Bioinformatics, [IF9.101]
DOI：https://doi.org/10.1093/bib/bbab094
第一作者：Fenglong Yang (杨凤龙)
通讯作者：QuanZou(邹权)
主要单位：
电子科技大学基础与前沿研究院(Institute of Fundamental and Frontier Sciences, University of Electronic Science and Technology of China, Chengdu, Shahe Campus:No.4, Section 2, North Jianshe Road, 610054,China)
GutBalance:一个基于人类肠道菌群进行疾病预测和解决了组成问题的生物标志物发现的服务器
Briefings in Bioinformatics[IF:8.99]
① 如何最好地对高维菌群数据进行分类仍然是一个悬而未决的问题，部分原因是难以正确处理组成性数据；② 通过将DBA远端平衡用作基于菌群的监督疾病分类的特征，作者从GMrepo数据库中开发了一个基于平衡的模型存储库。存储库中的疾病模型可以预测新提交的样本的疾病风险；③ 作者强调了平衡-疾病关联的概念，并开发了GBDAD。通过将平衡-疾病关联与MicroPhenoDB中已证明的微生物-疾病关联联系起来，可以可靠地推断出新的物种-疾病关联。
GutBalance: a server for the human gut microbiome-based disease prediction and biomarker discovery with compositionality addressed01-30, doi: 10.1093/bib/bbaa436
【主编评语】组成变换属于菌群数据的监督学习，是决定疾病分类器性能和可靠性的关键步骤。人们重视远端判别平衡分析（DBA）方法的出色性能，该方法在处理高维菌群数据分类时选择细菌对和三个的远端平衡。通过将该方法应用于GMrepo数据库中所有疾病表型的物种水平丰度，作者建立了基于平衡的模型库，用于人类肠道菌群相关疾病的分类。该模型存储库支持对新样本的疾病风险进行预测。更重要的是，作者强调了平衡-疾病关联的概念，而不是传统的微生物-疾病关联，并建立了人类肠道平衡-疾病关联数据库(GBDAD)。模型存储库和GBDAD数据库部署在GutBalance服务器上，该服务器支持对疾病模型、与疾病相关的平衡和感兴趣的疾病相关物种进行交互可视化和系统的询问。并发现基于平衡的物种-疾病关联性将加速胃肠道微生态学研究和临床试验中新的微生物-疾病关联性假说的产生。（@刘永鑫-中科院-宏基因组）
引言
通过高通量测序以及相关数据库注释后获得的微生物组丰度数据包括功能分类单元（代谢途径，蛋白家族等）或物种分类单元（比科属种OTU等）等谱矩阵。谱矩阵描述了样本中功能或所属物种的丰度信息。谱矩阵常常是成分数据（compositional data），各成分（component）间相加总和为常数，即具有“定和限制”的特性，其中一个成分的减少，必然伴随其他一个或多个成分的增加。微生物组数据的组成性问题常常被忽视，进而导致误导性的发现、相互矛盾的解释和不可复现的分析结果。正确使用相应的数学理论分析微生物组数据对挖掘与人类健康疾病真正相关联的生物学标记和特征具有重要的意义。
应用监督学习可以从微生物组数据中识别可预测人类疾病表型的微生物特征（也称为生物标志物）。这些微生物生物标志物可用于个人健康监测，疾病预防治疗等。然而，如何对高维且具有组成性特点的微生物组数据进行正确的特征提取是决定疾病分类模型性能和可靠性的关键步骤。Quinn和Erb 评估了数据变换（包括原始的比例数据和5种“对数比”变换方法）对高维生物标志物成分数据的分类准确性的影响。对数比变换包括中心对数比变换（CLR）和4种基于Balance变换的方法（DBA、RDA、ARA、PBA）。结果表明，CLR 和4种balance变换方法在大多数分类任务中表现相当出色，均优于基于相对丰度特征的分类结果。
解决成分数据问题的研究通常应用加法对数比（the additive log-ratio，ALR）、中心化对数比（centred log-ratio，CLR）、等距测对数比（isometric log-ratio，ILR）来解决在微生物组数据的问题。成分数据的样本空间是一个受约束的单形空间（simplex），在同维度的单形空间和欧式空间之间找到等距映射（样本相对距离保持不变）对于多种多元统计工具在成分数据上的应用至关重要。由于ALR变换不是等距变换，CLR变换也没有用单形空间的一组正交基来表示，所以这个问题只能通过ILR变换来解决。
ILR变换表示为单形空间中的标准正交基，在成分数据分析中最经常被推荐使用。ILR变换方式的不同不会导致随后分析结果的不一致。ILR变换常用顺序二进制划分方法（sequential binary partition，SBP）来表示。然而，基于任意ILR变换的分析结果通常难以解释，为了克服这一困难，Egozcue和Pawlowsky-Glahn等人提出了“Balance”变换（一种特殊的ILR变换）。Balance变换的目的不是寻找与目标表型相关的单一特征，而是以可解释的方式寻找重要的特征比率（如细菌丰度比率）。虽然一些临床微生物组的研究中强调了将Balance作为生物标志物的重要性，但只有少数研究将Balance作为疾病预测等监督学习的输入特征。“selbal”方法首次成功地将Balance思想应用于基于微生物组数据的机器学习分类任务中，该方法旨在筛选出由多个组分构成的分类表现良好的唯一一个Balance特征。为了进一步提高当前基于Balance特征提取方法的分类性能与可解释性，Quinn和Erb应用了计算高效的判别平衡分析（discriminative balance analysis，DBA-distal）方法来筛选由2个或3个组分构成的，具有优良分类判别能力的Balances特征。针对基于微生物组数据的表型分类问题，DBA-distal方法在可解释性、运算时间和分类准确性方面的表现均优于selbal方法。
我们前期发表的工作（图1，GutBalance），成功应用DBA-distal方法对GMrepo数据库中数十种人类肠道微生物组有关的疾病数据集进行了深入分析，我们构建了基于distalbalances特征的数十种疾病的预测模型数据库（GutBalance），并开发了在线疾病预测平台以及疾病微生物标志物数据库GBDAD，系统解决了基于肠道微生物组数据进行疾病预测和生物标志物发掘的问题。

图1 GutBalance，基于Balance特征的疾病预测模型及生物标志物数据库（http://lab.malab.cn/soft/GutBalance）。
由于基于肠道微生物组的人类疾病分类预测研究普遍缺乏对成分数据特点的考虑，我们强调“平衡-疾病”关联的概念，而不是传统的“微生物-疾病”关联，并研发了一站式基于成分数据的疾病预测建模综合分析平台（图2，DisBalance）。该平台集成并简化了基于微生物组数据的疾病二分类建模、风险预测和疾病相关生物标志物发现的分析流程，可以快速方便地构建新的疾病预测模型，辅助与人类肠道菌群相关的疾病诊断和预防，促进模型驱动和知识驱动的机制发现。

图2 DisBalance，基于微生物组数据的Balance变换及疾病预测建模分析平台（http://lab.malab.cn/soft/DisBalance）。
DisBalance数据分析平台介绍
分析流程
DisBalance平台采用DBA-distal方法选择一组具有疾病判别能力的Balances作为正则化逻辑回归（LR）建模的输入特征，平台内置了从MicroPhenoDB数据库中提取的2111个疾病-微生物关联证据，提供多种挖掘微生物标志物的策略，分析流程见图3。DisBalance在线平台，对于微生物-疾病关联研究中疾病预测建模、风险预测和微生物标志物发现等都具有重要的意义和临床应用价值。

图3DisBalance平台由三个功能模块组成：1.Logistic regression（LR）模型构建，2.疾病风险预测 3.生物标志物发现。执行LR建模流程，只需输入两个文件：表示特征丰度信息的谱矩阵文件和包含样本表型分类信息的样本元数据文件。首先，过滤低组内覆盖度的特征（默认20%），采用多重简单置换的方法替换零值；然后采用DBA-distal方法生成SBP矩阵，从该矩阵中选择组成末端Balances的物种，并通过ILR（等距对数比）变换获得Balance谱数据（Balance-样本矩阵）；应用SMOTE采样技术处理类不平衡的数据集后，输入正则化的LR模型，利用嵌套交叉验证的方法进行训练得到优化后的疾病预测模型。根据构建好的疾病预测模型，一方面，可以提交新的物种丰度数据来预测新样本的患病风险；另一方面，有四种策略可以帮助发现微生物生物标志物：1）通过筛选LR系数绝对值最大的topbalances作为候选生物标志物，2）top balances相关的微生物也可以作为候选生物标志物，3）top balances相关的微生物可以通过已发表的疾病关联证据（如MicroPhenoDB微生物-疾病关联数据库）进行佐证，4）新的微生物-疾病关联可以通过结合top balances和实证的疾病相关微生物来进行推断，DisBalance平台内置MicroPhenoDB数据库，也支持上传自定义的微生物-疾病关联证据。
DisBalance平台的Web应用程序使用基于Python语言的Django前端框架和Plotly Dash框架实现。该平台部署在阿里巴巴弹性计算云服务器上。由于服务器的存储有限，用户提交的数据和平台生成的数据需要及时下载，所有数据将在任务完成后24小时后自动删除。
平台输出的可视化结果

图4以UC（D003093）疾病模型为例，展示DisBalance建模结果的可视化。包括基于物种丰度（A）和Balance（B）的tSNE样本分布图、混淆矩阵热图（C）以及模型评估ROC曲线（D）。很明显，基于Balance特征可以更好地区分疾病与健康样本。
图4展示了平台分析结果模型的可视化，其他分析结果见服务器Demo部分介绍（http://lab.malab.cn/soft/DisBalance）。
疾病-微生物关联推断
疾病预测模型筛选出来的topbalances特征（按LR系数绝对值大小排序的topbalances）和与topbalances相关的微生物都可以作为候选的疾病标志物。Topbalance相关的微生物与疾病的关联可以从MicroPhenoDB数据库中获得佐证。新的疾病-微生物关联可以通过结合疾病-Balance关联和实证的疾病-微生物关联来进行推断（图5B）。从图5A中，我们可以很方便的找到balances对应的微生物是否与MicroPhenoDB数据库中的相关疾病有关。

图5疾病-微生物关联推断方法（以UC数据集为例）。A.Top 10 balances的物种划分情况以及MicroPhenoDB中与top10 balance相关的微生物与疾病关联的证据，红色代表正相关，蓝色代表负相关。Balance中分子分母处的物种分别用圆圈和三角形表示。B. 结合疾病-Balance关联和实证的疾病-微生物关联推断新的疾病-微生物关联。对于与UC疾病正（红色）或负（蓝色）相关的2-part（B1）或3-part（B2）的balances，各自有四种情况可以唯一推断出新的疾病-微生物关联。以UC的2-part balance（28118_39486）为例（B3），模型训练结果显示该balance与UC负相关，balance中的分母物种Dorea formicigenerans（NCBI id: 39486）被证明与UC负相关，那么推断出分子物种Odoribacter splanchnicus（NCBI id：28118）与UC负相关。C. DisBalance平台的推断结果为在线交互表格，包含了topbalances信息、与疾病相关的LR系数、MicroPhenoDB中实证的疾病-微生物关联信息以及新的疾病-微生物关联推断结果。
为了简化推论过程，我们设计了一个功能模块，可以自动从topbalances和实证的疾病-微生物关联中推断出新的疾病-微生物关联。DisBalance默认使用MicroPhenoDB中的疾病-微生物关联证据，也支持用户上传自定义的疾病-微生物关联证据进行推论。推断结果可以通过输出界面进行交互式探索（图5C）。
小结
本研究开发了针对基于微生物组数据进行疾病分类研究的一站式在线分析平台DisBalance。该平台整合了微生物组与疾病研究中涉及的疾病模型构建、疾病风险预测和疾病微生物标志物挖掘等分析内容，可以快速方便地对提交的数据进行模型构建和疾病风险预测。为了促进模型驱动的知识发现，DisBalance提供了四种策略来帮助自动挖掘疾病相关的微生物标志物。UC实例数据的分析证明了流程的可靠性及使用方法。
我们强调Balance-疾病关联研究框架，并提供了端到端的相应研究方案与在线软件工具，系统解决了微生物组数据组成性问题对分析结果可靠性的影响。基于Balance研究框架，我们进一步创新性地提出了基于Balance建模结果的疾病-微生物关联推断方法和大规模数据挖掘结果，为疾病-微生物关联的实证研究与临床实践提供了指导。
讨论
影响疾病预测模型可信度的因素都会影响微生物-疾病关联的推断。这些因素包括：疾病健康样品的代表性、高通量测序方法及物种注释方法的选择、零值替换以及微生物-疾病关联证据的可靠性等。
1.微生物稀疏矩阵大量零值的替换。目前有不同的方法可以替换稀疏矩阵的零值，本研究使用了简单的多重置换方法。零值的处理方式将直接影响基于对数比的特征选择结果，进而影响训练模型的准确度，而如何最好地替换零值仍然是一个待解决的问题。
2.预测模型及模型推测的balance-疾病关联的可靠性。我们选择正则化的LR而不是其他机器学习算法来解决基于肠道微生物数据的疾病分类预测问题。LR算法的优势在于两个方面。首先，LR 模型具有高度可解释性，LR变量权重系数可直接用作衡量特征重要性的指标以及与表型的相关性。其次，通过应用弹性网正则化以及使用嵌套交叉验证的方法来训练LR模型可以有效规避模型对数据的过拟合问题。
3.微生物-疾病关联证据的可靠性。虽然本研究使用的MicroPhenoDB是与人类微生物-疾病关联证据收集最新的数据库，但规模仍然有限。构建全面的微生物-疾病关联的知识数据库有望加速人类微生物-疾病关联数据的整合。新的微生物-疾病关联研究的不断推进也将促进GBDAD数据库中与疾病相关的balances或微生物致病机理的揭示。
目前，DisBalance是解决基于微生物组数据进行疾病二分类建模的最佳解决方案。基于DBA-distal的对数比变换方法尚未完全解决微生物组数据的计数（countdata）问题，因此建议将DisBalance应用于样本量较大的数据集。除了我们在本研究中关注的微生物物种特征外，DisBalance还可以用于分析其他类型的成分数据，如基因、代谢物和mRNA等。
参考文献
Fenglong Yang, Quan Zou, DisBalance: a platform to automatically build balance-based disease prediction models and discover microbial biomarkers from microbiome data, Briefings in Bioinformatics, 2021, Doi:10.1093/bib/bbab094.

AI加入疫情研究一线！用深度学习算法寻找肺炎病毒宿主
原创机器之能机器之能2020-01-30

武汉新型冠状病毒肺炎的疫情仍在不断扩散。截至2020年1月30日7时，确诊病例达到7201例，确诊病例数已经超过2003年非典。随着确诊人数的增多，需要尽快确定可能感染武汉2019年新型冠状病毒（2019-nCoV）的潜在宿主与中间宿主，切断病毒传播链。
近期一篇研究论文指出，基于深度学习的病毒宿主预测方法，检测出蝙蝠和水貂可能是新型冠状病毒的两个潜在宿主，其中水貂可能为中间宿主。这种方式区别于其他传统检测方法，可视为AI技术在病毒检测中的重大突破。
基于AI技术的深度学习推测病毒宿的方法已经有所应用，可以减少病毒检测过程中的重复工作，或可视为AI在对抗疫情的重要突破。
近期一篇研究论文指出，基于深度学习的病毒宿主预测方法，检测出蝙蝠和水貂可能是新型冠状病毒的两个潜在宿主，其中水貂可能为中间宿主。这种方式区别于其他传统检测方法，可视为AI技术在病毒检测中的重大突破。
北京大学工学院教授朱怀球团队一篇题为《深度学习算法预测新型冠状病毒的宿主和感染性》的研究发于1月25日表于bioRxiv预印版平台。

该研究提出一种基于深度学习的病毒宿主预测方法，用于检测以DNA序列为输入的病毒能感染哪种宿主，并将其应用于武汉2019年新型冠状病毒（2019-nCoV）。
为了构建病毒宿主预测VHP（virual host prediction）模型，朱怀球团队使用了双路卷积神经网络（BiPathCNN），其中每个病毒序列分别由其碱基和密码子的一个热矩阵表示。
所谓双路卷积神经网络（BiPathCNN），即针对相同构造的卷积神经网络输入同样的数据集也会提取到不同特征的情况,为利用该差异挖掘图像的深层特征,提出一种双路卷积神经网络模型的图像分类算法。
考虑到输入序列长度的差异，该研究分别建立了两个BiPathCNN（BiPathCNN-A和BiPathCNN-B），分别用于预测100bp到400bp和400bp到800bp的病毒序列宿主。
朱怀球团队将病毒的宿主分为五类，包括植物、细菌、无脊椎动物、脊椎动物和人类。
在病毒序列的实际应用中，通过输入病毒核苷酸序列，VHP将输出每种宿主类型，分别反映每种宿主类型内的感染性。
研究推测，与感染其他脊椎动物的冠状病毒相比，蝙蝠冠状病毒与新型冠状病毒具有更相似的感染模式。此外，通过比较所有宿主在脊椎动物上的病毒传染模式，发现水貂病毒的传染性模式更接近新型冠状病毒。
研究表明，新型冠状病毒的6个基因组都极有可能感染人类。预测结果提示，新型冠状病毒具有与严重急性呼吸综合征冠状病毒（SARS-CoV）、蝙蝠SARS样冠状病毒（Bat SARS-like CoV）和中东呼吸综合征冠状病毒（MERS-CoV）一样强大的病毒感染力。
基于AI技术的深度学习推测病毒宿的方法已经有所应用，可以减少病毒检测过程中的重复工作，或可视为AI在对抗疫情的重要突破。
2018年11月，英国格拉斯哥大学研究团队发布了一项最新人工智能研究报告：科学家借助全新的机器学习算法，可以从基因层面预测埃博拉和寨卡等病毒的天然宿主，从而采取措施预防这些病毒传播到人类身上。
目前而言，人类对疾病的认知程度相当有限，由于病毒与疾病种类的复杂程度，现阶段还很难用人工智能完全替代，大部分情况下，AI在处理复杂数据过程中占据优势，得出的结论无法得到完全保证，最后的诊断与判定最终仍需要人来确认。
附北京大学工学院教授朱怀球团队发表论文的主要内容
报告名称：深度学习算法预测新型冠状病毒的宿主和感染性
报告版本：报告于1月25日发表至医学研究论文预印本发布平台 medRxiv
研究发现：
研究推测，与感染其他脊椎动物的冠状病毒相比，蝙蝠冠状病毒与新型冠状病毒具有更相似的感染模式。此外，通过比较所有宿主在脊椎动物上的病毒传染模式，发现水貂病毒的传染性模式更接近新型冠状病毒。
研究表明，新型冠状病毒的6个基因组都极有可能感染人类。预测结果提示，新型冠状病毒具有与严重急性呼吸综合征冠状病毒（SARS-CoV）、蝙蝠SARS样冠状病毒（Bat SARS-like CoV）和中东呼吸综合征冠状病毒（MERS-CoV）一样强大的病毒感染力。
研究方法：
研究使用基于深度学习算法开发的VHP（Virushost prediction，病毒宿主预测）方法报告了2019-nCoV宿主的预测结果。2018年之前发布的病毒序列数据用于构建训练集，而2018年之后发布的则用于测试。用于训练和测试的数据集包括所有DNA病毒的基因组、所有RNA病毒的编码序列及其在GenBank中的宿主信息。在VHP对2019-nCoV的预测结果中，数值反映了新病毒的感染性，得分模式和p值模式反映了新病毒的感染性模式。
随着全基因组序列的在线发布，朱怀球团队预测了2019-nCoV的潜在宿主，以及NCBI refseq中的其他44种冠状病毒和GenBank中的4种蝙蝠SARS样冠状病毒。结果表明，2019年nCoV的6个基因组均具有很高的感染人类的可能性（p值<0.05）。

除此之外，大多数报告的人类感染性冠状病毒的p值均为VHP法预测的最低值。2019-nCoV和其他人类冠状病毒的相似概率说明了2019-nCoV的高风险。
VHP方法以及算法的验证：
为了构建VHP模型，朱怀球团队使用了双路卷积神经网络（BiPathCNN），其中每个病毒序列分别由其碱基和密码子的一个热矩阵表示。
考虑到输入序列长度的差异，分别建立了两个BiPathCNN（BiPathCNN-A和BiPathCNN-B）用于预测100bp到400bp和400bp到800bp的病毒序列宿主。
用于训练和测试的数据集包括所有DNA病毒的基因组、所有RNA病毒的编码序列及其在GenBank中的宿主信息。为了开发新病毒潜在宿主类型预测的方法专家，使用2018年之前发布的病毒序列数据构建训练集，而使用2018年之后发布的病毒序列数据进行测试。
将病毒的宿主分为五类，包括植物、细菌、无脊椎动物、脊椎动物和人类。

表2详细列出了这五种类型中包含的宿主子类型。在病毒序列的实际应用中，通过输入病毒核苷酸序列，VHP将输出每种宿主类型，分别反映每种宿主类型内的感染性。此外，VHP提供了5个p值，用于统计感染与非感染事件的区别。
为了评估VHP的性能，朱怀球团队比较了blast和VHP的AUC（曲线下面积）。比较结果表明，VHP的平均AUC较高（见表3）。

本报告中预测了2019年nCoV感染人类的可能性，并暗示了2019年nCoV的风险。
报告也显示，VHP模型可以在公共卫生服务中发挥重要作用，为预防可能感染人类的新型病毒提供强有力的帮助，从而提供可靠的预测宿主和感染人类的潜力

收藏 | 图解最常用的10个机器学习算法！
机器学习研究组订阅3月23日
在机器学习领域，有种说法叫做“世上没有免费的午餐”，简而言之，它是指没有任何一种算法能在每个问题上都能有最好的效果，这个理论在监督学习方面体现得尤为重要。举个例子来说，你不能说神经网络永远比决策树好，反之亦然。模型运行被许多因素左右，例如数据集的大小和结构。
因此，你应该根据你的问题尝试许多不同的算法，同时使用数据测试集来评估性能并选出最优项。当然，你尝试的算法必须和你的问题相切合，其中的门道便是机器学习的主要任务。打个比方，如果你想打扫房子，你可能会用到吸尘器、扫帚或者拖把，但你肯定不会拿把铲子开始挖坑吧。

01  线性回归

线性回归可能是统计学和机器学习中最知名和最易理解的算法之一。
由于预测建模主要关注最小化模型的误差，或者以可解释性为代价来做出最准确的预测。 我们会从许多不同领域借用、重用和盗用算法，其中涉及一些统计学知识。
线性回归用一个等式表示，通过找到输入变量的特定权重（B），来描述输入变量（x）与输出变量（y）之间的线性关系。

Linear Regression
举例：y = B0 + B1 * x
给定输入x，我们将预测y，线性回归学习算法的目标是找到系数B0和B1的值。
可以使用不同的技术从数据中学习线性回归模型，例如用于普通最小二乘和梯度下降优化的线性代数解。
线性回归已经存在了200多年，并且已经进行了广泛的研究。 如果可能的话，使用这种技术时的一些经验法则是去除非常相似（相关）的变量并从数据中移除噪声。 这是一种快速简单的技术和良好的第一种算法。
02 逻辑回归

逻辑回归是机器学习从统计领域借鉴的另一种技术。 这是二分类问题的专用方法（两个类值的问题）。
逻辑回归与线性回归类似，这是因为两者的目标都是找出每个输入变量的权重值。 与线性回归不同的是，输出的预测值得使用称为逻辑函数的非线性函数进行变换。
逻辑函数看起来像一个大S，并能将任何值转换为0到1的范围内。这很有用，因为我们可以将相应规则应用于逻辑函数的输出上，把值分类为0和1（例如，如果IF小于0.5，那么 输出1）并预测类别值。

Logistic Regression
由于模型的特有学习方式，通过逻辑回归所做的预测也可以用于计算属于类0或类1的概率。这对于需要给出许多基本原理的问题十分有用。
与线性回归一样，当你移除与输出变量无关的属性以及彼此非常相似（相关）的属性时，逻辑回归确实会更好。 这是一个快速学习和有效处理二元分类问题的模型。
03 线性判别分析

传统的逻辑回归仅限于二分类问题。 如果你有两个以上的类，那么线性判别分析算法（Linear Discriminant Analysis，简称LDA）是首选的线性分类技术。
LDA的表示非常简单。 它由你的数据的统计属性组成，根据每个类别进行计算。 对于单个输入变量，这包括：
每类的平均值。
跨所有类别计算的方差。

Linear Discriminant Analysis
LDA通过计算每个类的判别值并对具有最大值的类进行预测来进行。该技术假定数据具有高斯分布（钟形曲线），因此最好先手动从数据中移除异常值。这是分类预测建模问题中的一种简单而强大的方法。
04 分类和回归树
决策树是机器学习的一种重要算法。
决策树模型可用二叉树表示。对，就是来自算法和数据结构的二叉树，没什么特别。 每个节点代表单个输入变量（x）和该变量上的左右孩子（假定变量是数字）。

Decision Tree
树的叶节点包含用于进行预测的输出变量（y）。 预测是通过遍历树进行的，当达到某一叶节点时停止，并输出该叶节点的类值。
决策树学习速度快，预测速度快。 对于许多问题也经常预测准确，并且你不需要为数据做任何特殊准备。
05 朴素贝叶斯

朴素贝叶斯是一种简单但极为强大的预测建模算法。
该模型由两种类型的概率组成，可以直接从你的训练数据中计算出来：1）每个类别的概率; 2）给定的每个x值的类别的条件概率。 一旦计算出来，概率模型就可以用于使用贝叶斯定理对新数据进行预测。 当你的数据是数值时，通常假设高斯分布（钟形曲线），以便可以轻松估计这些概率。

Bayes Theorem
朴素贝叶斯被称为朴素的原因，在于它假设每个输入变量是独立的。 这是一个强硬的假设，对于真实数据来说是不切实际的，但该技术对于大范围内的复杂问题仍非常有效。
06 K近邻

KNN算法非常简单而且非常有效。KNN的模型用整个训练数据集表示。 是不是特简单？
通过搜索整个训练集内K个最相似的实例（邻居），并对这些K个实例的输出变量进行汇总，来预测新的数据点。 对于回归问题，新的点可能是平均输出变量，对于分类问题，新的点可能是众数类别值。
成功的诀窍在于如何确定数据实例之间的相似性。如果你的属性都是相同的比例，最简单的方法就是使用欧几里德距离，它可以根据每个输入变量之间的差直接计算。

K-Nearest Neighbors
KNN可能需要大量的内存或空间来存储所有的数据，但只有在需要预测时才会执行计算（或学习）。 你还可以随时更新和管理你的训练集，以保持预测的准确性。
距离或紧密度的概念可能会在高维环境（大量输入变量）下崩溃，这会对算法造成负面影响。这类事件被称为维度诅咒。它也暗示了你应该只使用那些与预测输出变量最相关的输入变量。
07 学习矢量量化

K-近邻的缺点是你需要维持整个训练数据集。 学习矢量量化算法（或简称LVQ）是一种人工神经网络算法，允许你挂起任意个训练实例并准确学习他们。

Learning Vector Quantization
LVQ用codebook向量的集合表示。开始时随机选择向量，然后多次迭代，适应训练数据集。 在学习之后，codebook向量可以像K-近邻那样用来预测。 通过计算每个codebook向量与新数据实例之间的距离来找到最相似的邻居（最佳匹配），然后返回最佳匹配单元的类别值或在回归情况下的实际值作为预测。 如果你把数据限制在相同范围（如0到1之间），则可以获得最佳结果。
如果你发现KNN在您的数据集上给出了很好的结果，请尝试使用LVQ来减少存储整个训练数据集的内存要求。
08 支持向量机

支持向量机也许是最受欢迎和讨论的机器学习算法之一。
超平面是分割输入变量空间的线。 在SVM中，会选出一个超平面以将输入变量空间中的点按其类别（0类或1类）进行分离。在二维空间中可以将其视为一条线，所有的输入点都可以被这条线完全分开。SVM学习算法就是要找到能让超平面对类别有最佳分离的系数。

Support Vector Machine
超平面和最近的数据点之间的距离被称为边界，有最大边界的超平面是最佳之选。同时，只有这些离得近的数据点才和超平面的定义和分类器的构造有关，这些点被称为支持向量，他们支持或定义超平面。在具体实践中，我们会用到优化算法来找到能最大化边界的系数值。
SVM可能是最强大的即用分类器之一，在你的数据集上值得一试。
09 bagging和随机森林

随机森林是最流行和最强大的机器学习算法之一。 它是一种被称为Bootstrap Aggregation或Bagging的集成机器学习算法。
bootstrap是一种强大的统计方法，用于从数据样本中估计某一数量，例如平均值。 它会抽取大量样本数据，计算平均值，然后平均所有平均值，以便更准确地估算真实平均值。
在bagging中用到了相同的方法，但最常用到的是决策树，而不是估计整个统计模型。它会训练数据进行多重抽样，然后为每个数据样本构建模型。当你需要对新数据进行预测时，每个模型都会进行预测，并对预测结果进行平均，以更好地估计真实的输出值。

Random Forest
随机森林是对决策树的一种调整，相对于选择最佳分割点，随机森林通过引入随机性来实现次优分割。
因此，为每个数据样本创建的模型之间的差异性会更大，但就自身意义来说依然准确无误。结合预测结果可以更好地估计正确的潜在输出值。
如果你使用高方差算法（如决策树）获得良好结果，那么加上这个算法后效果会更好。
10 Boosting和AdaBoost
Boosting是一种从一些弱分类器中创建一个强分类器的集成技术。 它先由训练数据构建一个模型，然后创建第二个模型来尝试纠正第一个模型的错误。 不断添加模型，直到训练集完美预测或已经添加到数量上限。
AdaBoost是为二分类开发的第一个真正成功的Boosting算法，同时也是理解Boosting的最佳起点。 目前基于AdaBoost而构建的算法中最著名的就是随机梯度boosting。

AdaBoost
AdaBoost常与短决策树一起使用。 在创建第一棵树之后，每个训练实例在树上的性能都决定了下一棵树需要在这个训练实例上投入多少关注。难以预测的训练数据会被赋予更多的权重，而易于预测的实例被赋予更少的权重。 模型按顺序依次创建，每个模型的更新都会影响序列中下一棵树的学习效果。在建完所有树之后，算法对新数据进行预测，并且通过训练数据的准确程度来加权每棵树的性能。
因为算法极为注重错误纠正，所以一个没有异常值的整洁数据十分重要。
初学者在面对各种各样的机器学习算法时提出的一个典型问题是“我应该使用哪种算法？”问题的答案取决于许多因素，其中包括：
数据的大小，质量和性质; 
可用的计算时间; 
任务的紧迫性; 
你想要对数据做什么。
即使是一位经验丰富的数据科学家，在尝试不同的算法之前，也无法知道哪种算法会表现最好。 虽然还有很多其他的机器学习算法，但这些算法是最受欢迎的算法。 如果你是机器学习的新手，这是一个很好的学习起点。

15小时虚拟筛选10亿分子，《Nature》+HMS验证云端新药研发未来

一种新药从开发到获得批准平均成本为20亿-30亿美元，至少耗时10年。
这句话，药物研发领域的人大概都听累了。
为什么这么难？
1. 湿实验昂贵而费时；
2. 初始化合物命中率低；
3. 临床前阶段的高损耗率。
今年3月，哈佛大学医学院（HMS）的研究人员在《Nature》杂志发表了论文《An open-source drug discovery platform enables ultra-large virtual screens》，描述了一个叫做VirtualFlow的开源药物发现平台，能通过云端整合海量的CPU对超大规模化合物库进行基于结构的虚拟筛选，提高药物发现效率。
论文作者Christoph Gorgulla称，在一个CPU上筛选10亿种化合物，每个配体的平均对接时间为15秒，全部筛完大概需要475年，而HMS利用VirtualFlow的平台，调用160000个CPU对接10亿个分子仅耗时约15小时，10000个CPU则需要两周。

听起来非常吸引人。
抱着给某CRO公司虚拟筛选的7.8亿个分子，我们心里有点高兴。
当时，我们调用了云上几万个core来筛选，计算时长也仅花费了3-13个小时（每个Core上所需时间不一样）。
限于算力，或者高效灵活地调用大规模计算集群的能力，当前的虚拟筛选通常仅采样百万到千万个分子，而事实上目前可用于药物发现的有机分子已经超过10的60次方。
注：湿实验室（Wet Lab）主要靠的是做实验，干实验室（Dry Lab）主要是计算机模拟和计算。
HMS的论文主要论证了两点：
1、虚拟筛选的规模越大，筛选的化合物越多，真阳性率越高；
2、线性扩展能力+云平台=无限可能。
超大规模筛选可提高真阳性率
论文推导了真阳性率与所筛选化合物数量的的函数关系的概率模型，证明：化合物的最高打分随着规模增加而提高。
作者分别从10万、100万、1000万、1亿、10亿个化合物中进行了5次筛选，挑选了得分最高的前50个化合物进行对比，从图中可以很清楚地看到筛选的规模越大，得分越高（位置越靠上）。

分子化合物的质量会随着虚拟筛选规模的扩大而提升
图片来源：《Nature》
虚拟筛选规模可以通过两种不同的方式提高初始命中的质量：
1. 通过识别具有更紧密结合亲和力的化合物，从而降低剂量，减少脱靶效应；
2. 通过发现具有更好的药代动力学和/或更少固有细胞毒性的化合物。
为了验证大规模筛选的准确性，研究人员选择了肿瘤研究领域热门的KEAP1蛋白作为虚拟筛选靶点，对含有13亿配体的数据库进行了虚拟筛选。通过两个阶段的筛选，HMS选出了约1万个打分优秀的分子。
随后，研究人员从成药性、配体效率、化学多样性以及获取难度等方面在这约1万个候选分子中挑选了590个苗头化合物进行活性验证，最终给出了两个活性达到毫微摩尔级的代表性化合物iKEAP 1和iKEAP 2的多种实验结果，验证了VirtualFlow在对接10亿以上分子量时的高效性。
线性扩展+云平台=无限可能
可线性扩展的意思是说，处理器数量增加一倍，筛选能力也会增加一倍。
为了论证这一点，HMS在本地和云端均进行了测试：
本地计算集群LC1由18,000个CPU（分别为Intel Xeon和AMD Opteron处理器的不同型号）异构组成；本地集群LC2上则有30,000个英特尔Xeon8268处理器。
云端则选择了GCP和AWS，最多调用了160,000万个CPU（作者并未阐述在云端使用的CPU型号）。实验表明VirtualFlow在多种情况下均体现了良好的线性可扩展性，具体可看下图（图中并未描述平台在AWS上的表现）。

VirtualFlow在不同情况下呈现出的线性可扩展性
图片来源：《Nature》
而这种近乎无限的线性扩展性意味着什么？
即便在今后的实际应用中并行数百万个内核，VirtualFlow的性能效率也不会受到其他因素的拖累。如果你拥有一个300核的计算机集群，你可以在六周内筛选1亿个化合物，而如果你有1,000核，那么两个星期内就可以完成筛选。
这个开源的VirtualFlow平台到底是个啥？
这个项目由哈佛大学医学院牵头，整体仍处于较新的阶段。VirtualFlow平台旨在利用超级计算能力并行筛选潜在的有机化合物结构，以寻找有希望的新药物分子。

VirtualFlow在虚拟筛选中的作用过程
图片来源：《Nature》
VirtualFlow平台主要分为VFLP（配体制备）和VFVS（虚拟筛选）两个模块，VFLP负责分析目标的化学空间构型（图中上半部分的蓝色箭头），再由VFVS根据事先预设好的靶点经过一次或多次虚拟筛选之后，最终获得先导化合物。
目前已知的平台特性包括：
1. 用Bash编写，完全开源、免费；
2. 目前支持的应用包括：AutoDock Vina、QuickVina 2、Smina、AutoDockFR、QuickVina-W、VinaXB和Vina-Carb；
3. 暂时不支持GPU；
4. 支持AWS、GCP、Azure在内的主流云计算平台。
这很棒。
但开源平台，不是你想拥抱就能拥抱。
VirtualFlow拥有较高的使用门槛，你可能需要懂点代码，懂点调度器，再懂点集群，还要熟悉各个云平台等等。

VirtualFlow使用界面
药物研发向来是皇冠上的明珠，HMS这篇论文验证了应用云平台的确能带来新药研发效率的提升，时间和金钱成本上的大量缩减。
中国人自己的创新药+云，我们可以期待一下。

机器学习模型评估教程！【机器学习基础】

如何在投入生产前评估机器学习模型性能？

想象一下，你训练了一个机器学习模型。也许，可以从中选几个候选方案。
你在测试集上运行它，得到了一些质量评估。模型没有过度拟合，特征也有意义。总的来说，在现有的有限数据下，它们的表现尽善尽美。
现在，是时候来决定它们是否好到可以投入生产使用了。如何在标准性的质量评估外，评估和比较你的模型呢？
在本教程中，我们将通过一个案例，详细介绍如何评估你的模型。
案例：预测员工流失情况
我们将使用一个来自Kaggle竞赛的虚构数据集，目标是识别哪些员工可能很快离开公司。（数据集与代码下载，在后台回复日期"210411 "获取）
这个想法听起来很简单：有了预警，你可能会阻止这个人离开。一个有价值的专家会留在公司——无需寻找一个新的员工，再等他们学会工作技巧。
让我们试着提前预测那些有风险的员工吧！

首先，检查训练数据集。它是为方便我们使用而收集的。一个经验丰富的数据科学家会产生怀疑！我们将其视为理所当然，并跳过构建数据集的棘手部分。
我们有1470名员工的数据。
共35个特征，描述的内容包括：
员工背景（教育、婚姻状况等）。
工作细节（部门、工作级别、是否出差等）。
工作历史（在公司工作年限、最后一次晋升日期等）。
薪酬（工资、股票意见等）。
以及其他一些特征。
还有一个二元标签，可以看到谁离开了公司。这正是我们所需要的！我们将问题定义为概率分类任务。模型应该估计每个员工属于目标 "流失 "类的可能性。

在研究模型时，我们通常会将数据集分成训练和测试数据集。我们使用第一个数据集来训练模型，用其余的数据集来检查它在未知数据上的表现。
我们不详细介绍模型训练过程。这就是数据科学的魔力，我们相信你是知道的！
假设我们进行了很多次实验，尝试了不同的模型，调整了相应的超数，在交叉验证中进行了区间评估。
我们最终得到了两个合理的模型，看起来同样不错。
接下来，检查它们在测试集上的性能。这是我们得到的结果。
随机森林模型的ROC AUC值为0. 795分
梯度提升模型的ROC AUC评分为0.803分。
ROC AUC是在概率分类的情况下优化的标准指标。如果你寻找过Kaggle这个用例的众多解决方案，这应该是大多数人的做法。

我们的两个模型看起来都不错。比随机拆分好得多，所以我们肯定可以从数据中得到一些线索。
ROC AUC分数很接近。鉴于这只是一个单点估计（single-point estimate），我们可以认为性能相似。
那么问题来了，两者之间我们应该选哪个呢？
同样的质量，不同的特点
让我们更详细地分析这些模型。
我们将使用Evidently开源库来比较模型并生成性能报告。
如果你想一步一步地跟着做，可以在文末打开这个完整的upyter Notebook。
首先，我们在同一个测试数据集上训练两个模型并评估性能。
接下来，我们将两个模型的性能日志准备成两个pandas数据框(DataFrames）。每个包括输入特征、预测类和真实标签。
我们指定了列映射来来定义目标的位置，预测的类别，以及分类和数字特征。
然后，我们调用evidently tabs来生成分类性能报告。在单个的dashboard中显示两个模型的性能，以便我们可以比较它们。
comparison_report = Dashboard(rf_merged_test, cat_merged_test, column_mapping = column_mapping, tabs=[ProbClassificationPerformanceTab]) 
comparison_report.show()

我们将较简单的随机森林模型作为基准，对于这个工具来说，它成为 "参考（Reference）"。第二个梯度提升被表示为 "当前（Current）"的评估模型。
我们可以快速看到两个模型在测试集上的性能指标汇总。

现实情况不是Kaggle，所以我们并不总是关注第二位数。如果我们只看准确率和ROC AUC，这两个模型的性能看起来非常接近。
我们甚至可能有理由去偏爱更简单的随机森林模型，源于它更强的可解释性，或者更好的计算性能。
但F1-score的差异暗示可能还有更多故事。模型的内部运作方式各不相同。
重温类别不平衡问题
精明的机器学习者知道其中的窍门。两个类别的规模远不相等。在这种情况下，准确度的衡量标准是没有太大意义的。即使这些数字可能在 "论文"上看起来很好。
目标类通常是一个次要的类。我们希望预测一些罕见但重要的事件，比如：欺诈、流失、辞职。在我们的数据集中，只有16%的员工离开了公司。
如果我们做一个朴素的模型，只是把所有员工都归类为 "可能留下"，我们的准确率是84%！

ROC AUC并不能给我们一个完整的答案。相反，我们必须找到更适合预期模型性能的指标。
何谓"好的"模型？
你知道其中的答案：这要视情况而定。
如果一个模型能简单地指出那些即将辞职的人，并且永远是对的，那就太好了。那么我们就可以做任何事情了！一个理想的模型适合任何用例——但这往往不会在现实中出现。
相反，我们处理不完美的模型，使它们对我们的业务流程有用。根据应用，我们可能会选择不同的标准来评估模型。
没有一个单一的衡量标准是理想的。但模型并不是存在于真空中——希望你可以从提出问题开始！
让我们考虑不同的应用场景，并在此背景下评估模型。
示例1：给每个员工贴标签
在实践中，我们可能会将该模型集成到一些现有的业务流程中。
假设我们的模型用于在内部人力资源系统的界面上显示一个标签，我们希望突出显示每个具有高损耗风险的员工。当经理登录系统时，他们将会看到部门中每个人的 "高风险"或 "低风险"标签。

我们希望为所有员工显示标签。我们需要的模型尽可能的 "正确"。但我们知道，准确度指标隐藏了所有重要的细节。我们将如何评估我们的模型呢？
超越准确度
让我们回到evidently的报告中，更深入地分析两种模型的性能。
我们能够很快注意到，两个模型的混淆矩阵看起来是不同的。

我们的第一个模型只有两个假阳性（false positives）。听起来很不错？的确，它没有给我们太多关于潜在辞职的错误提醒。
但是，另一方面，它只正确识别了6次辞职。其他的53个都被漏掉了。
第二个模型错误地将12名员工标记为高风险。但是，它正确预测了27个辞职。它只漏掉了32人。
按类别划分的质量指标图总结了这一点。让我们看看 "yes"类别。

但第二个模型在召回率中胜出！它发现45%的人离开了公司，而第一个模型只有10%。
你会选择哪个模型呢？
最有可能的是，在目标 "辞职"类别中，召回率较高的那一个会赢。它可以帮助我们发现更多可能离职的人。
我们可以容忍一些假阳性，因为解释预测的是经理。人力资源系统中已有的数据也提供了额外的背景。
更有可能的是，在其中增加可解释性是必不可少的。它可以帮助用户解释模型预测，并决定何时以及如何做出反应。
总而言之，我们将基于召回率指标来评估我们的模型。作为一个非ML标准，我们将添加经理对该功能的可用性测试。具体来说，要把可解释性作为界面的一部分来考虑。
示例2：发送主动警报
让我们想象一下，我们期望在模型上有一个特定的行动。
它可能会与同一个人力资源系统进行整合。但现在，我们将根据预测发送主动通知。
也许，给经理发送一封电子邮件，提示安排与有风险的员工会面？或者是对可能的留用环节提出具体建议，比如额外的培训等等。

在这种情况下，我们可能对这些假阳性有额外的考虑。
如果我们过于频繁地给经理们发送邮件，他们很可能会被忽略。不必要的干预也可能被视为一种负面结果。
我们该怎么办？
如果没有任何新的有价值的功能可以添加，我们就只能使用现有的模型。我们无法榨取更多的精度。但是，可以限制采取行动的预测数量，目标是只关注那些预测风险较高的员工。
精度-召回率权衡
概率模型的输出是一个介于0和1之间的数字，为了使用预测，我们需要在这些预测的概率上分配标签。二元分类的 "默"方法是以0.5为切入点。如果概率较高，标签就是 "yes"。
我们可以选择一个不同的阈值，也许，0.6甚至0.8？通过设置更高的阈值，我们将限制假阳性的数量。
但这是以召回率为代价的：我们犯的错误越少，正确预测的数量也越少。
这个来自evidently报告的类别分离图（Class Separation）让这个想法非常直观。它在实际标签旁边显示了各个预测概率。

我们可以看到，第一个模型做出了几个非常自信的预测。稍微 "上调"或 "下调"阈值，在绝对数字上不会有太大的差别。
然而，我们可能会欣赏一个模型，并挑选出几个具有高置信度的案例的能力。例如，如果我们认为假阳性的成本非常高。在0.8处做一个分界点，就能得到100%的精度。我们只做两个预测，但都是正确的。
如果这是我们喜欢的行为，我们可以从开始就设计这样一个 "决定性 "的模型，它将强烈地惩罚假阳性，并在概率范围的中间做出较少的预测。(事实上，这正是我们在这个演示中所做的！）。

第二个模型的预测概率比较分散。改变阈值会产生不同的情况。我们只需看一下图示，就能做出大致的估计。例如，如果我们将阈值设置为0.8，就会让我们只剩下几个假阳性。
更具体地说，让我们看看精度-召回表。它旨在类似情况下选择阈值。它显示了top-X预测的不同情况。

例如，我们可以只对第二个模型的前5%的预测采取行动。在测试集上，它对应的概率阈值为66%。所有预测概率较高的员工都被认为有可能离职。
在这种情况下，只剩下18个预测。但其中14个将是正确的！召回率下降到只有23.7%，但现在的精度是77.8%。我们可能更喜欢它，而不是原来的69%精度，以尽量减少误报。
为了简化概念，我们可以想象一下类别分离图上的一条线。

在实践中，我们可以通过以下两种方式之一进行限制：
只对topX预测采取行动
将所有概率大于 X 的预测分配给正类。
第一种方案可用于批量模型。如果我们一次生成所有员工的预测，我们可以对它们进行分类，比如说，前5%。
如果我们根据要求进行个别预测，那么选取一个自定义的概率阈值是有意义的。

这两种方法中的任何一种都可以工作，这取决于具体用例。
我们也可以决定用不同的方式来可视化标签。例如，将每个员工标记为高、中、低流失风险。这将需要基于预测概率的多个阈值。
在这种情况下，我们会额外关注模型校准的质量，这一点从类别分离图上可以看出。
综上所述，我们会考虑精度-召回率的权衡来评估我们的模型，并选择应用场景。
我们不显示每个人的预测，而是选择一个阈值。它帮助我们只关注流失风险最高的员工。
示例3：有选择地应用模型
我们还可以采取第三种方法。
当看到两个模型的不同图谱时，一个明显的问题出现了。图上的小点背后的具体员工是谁？两种模型在预测来自不同角色、部门、经验水平的辞职者时有什么不同？
这种分析可能会帮助我们决定什么时候应用模型，什么时候不应用模型。如果有明显的细分市场，模型失效，我们可以将其排除。或者反过来说，我们可以只在模型表现好的地方应用模型。
在界面中，我们可以显示 "信息不足 "这样的内容。这可能比一直错误要好！

低性能部分
为了更深入地了解表现不佳的片段，我们来分析一下分类质量表（Classification Quality Table）。对于每个特征，它将预测的概率与特征值一起映射。
这样，我们就可以看到模型在哪些方面犯了错误，以及它们是否依赖于单个特征的值。
我们举个例子。
这里有一个工作等级（Job Level）特征，它是角色资历的特定属性。

如果我们对1级的员工最感兴趣，那么第一个模型可能是一个不错的选择！它能以高概率做出一些有把握的预测。例如，在0.6的阈值下，它在这个群体中只有一个假阳性。
如果我们想预测3级的辞职情况，第二个模型看起来要好得多。
如果我们希望我们的模型对所有级别都有效，我们可能会再次选择第二个模型。平均而言，它在1、2、3级中的表现可以接受。
但同样有趣的是，这两个模型在第4级和第5级上的表现**。**对这些群体中的员工做出的所有预测，概率都明显低于0.5。两种模型总是分配一个 "负"的标签。
如果我们看一下真实标签的分布，我们可以看到，在这些工作级别中，辞职的绝对数量相当低。很有可能在训练中也是如此，模型并没有发现任何有用的模式。

由于我们比较的是同一测试数据集上的性能，所以分布是相同的。
如果我们要在生产中部署一个模型，我们可以构建一个简单的业务规则，将这些片段（segments）从应用中排除。

我们也可以利用这个分析的结果，把我们的模型放在一个 "性能改进计划"上。也许，我们可以添加更多的数据来帮助模型。
例如，我们可能有一些 "旧的"数据，而这些数据是我们最初从训练中排除的。我们可以有选择地增强训练数据集，用于表现不佳的部分。在这种情况下，我们会添加更多关于4级和5级员工辞职的旧数据。
综上所述，我们可以识别出模型失败的特定细分片段，我们仍然显示出对尽可能多的员工的预测。但知道模型远非完美，我们只对表现最好的那部分员工进行应用。
模型知道什么？
这张表同样可以帮助我们更详细地了解模型的行为。我们可以探索误差、离群值，并了解模型的学习情况。
例如，我们已经看到，第一个模型只预测了少数有把握的辞职。第二个模型从我们的数据中 "捕捉"到了更多有用的信号。它是从哪里来的呢？
如果通过我们的特征，可以得到一个提示。
比如说，第一个模型只成功预测了那些相对新进公司的人的辞职，第二个模型可以检测出有10年以下工作经验的潜在离职者。我们可以从这个图中看到。

我们可以从股票期权等级（stock options level）中看到类似的情况。
第一个模型只成功预测了0级的员工。即使我们有不少重新加入的员工，至少也是1级的！第二种模型捕获到了更多等级较高的离职者。

但如果我们看加薪（即最近的加薪），我们会发现没有明显的细分，无论哪个模型的效果都更好或更差。
除了第一种模型的一般特征外，并没有具体的 "倾斜"，做出较少的置信预测。

类似的分析可以帮助在模型之间进行选择，或者找到改进模型的方法。
就像上面工作级别的例子一样，我们可能有办法来增强我们的数据集。我们可能会添加其他时期的数据，或者包含更多的特征。在类别不平衡的情况下，我们可以尝试给特定例子更多的权重。作为最后的手段，我们也可以添加业务规则。
我们找到了赢家！
回到我们的例子：第二种模式是大多数情况下的赢家。

但谁会只看ROC AUC就被信服了呢？我们必须超越单一的指标来深入评估模型。
它适用于许多其他用例。性能比准确度更重要。而且并非总是可以为每种错误类型分配简单的"成本"来对其进行优化。像对待产品一样对待模型，分析必须更加细致入微。
关键是不要忽视用例场景，并将我们的标准与之挂钩。可视化可能有助于那些不以ROC AUC思考的业务相关者进行沟通。
提示：本教程不用于辞职预测，而是模型分析
如果你想解决类似的用例，我们至少要指出这个数据集的几个限制。
我们缺乏一个关键的数据点：辞职的类型。人们可以自愿离职、被解雇、退休、搬到全国各地等等。这些都是不同的事件，将它们归为一类可能会造成模糊的标签。如果把重点放在 "可预测"的辞职类型上，或者解决多类问题来代替，会比较合理。
关于所从事的工作，没有足够的上下文。一些其他数据可能会更好地表明流失情况：绩效评估、特定项目、晋升规划等。这种用例需要与领域专家一起精心构建训练数据集。
没有关于时间和辞职日期的数据。我们无法说明事件的顺序，并与公司历史上的特定时期有关。
最后提醒一点：像这样的用例可能是高度敏感的。
你可能会使用类似的模型来预测一线人员的流动率。目标是预测招聘部门的工作量和相关的招聘需求。不正确的预测可能会导致一些财务风险，但这些很容易考虑进去。
但如果该模型用于支持对单个员工的决策，其影响可能会更加关键。例如，考虑分配培训机会时的偏差。我们应该评估使用案例的道德规范（the ethics of the use case），并审核我们的数据和模型是否存在偏见和公平（bias and fairness）。
参考链接：
本文所用数据集地址：https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset
Evidently开源库：https://github.com/evidentlyai/evidently
Jupyter notebook地址：https://github.com/evidentlyai/evidently/blob/main/evidently/examples/ibm_hr_attrition_model_validation.ipynb
原文地址：https://evidentlyai.com/blog/tutorial-2-model-evaluation-hr-attrition

UC Berkeley 马毅：深度学习的第一性原理

【专栏：研究思路】我们认为，人工智能进入了新的拐点。在一个后深度学习时代，不同的学者对未来智能发展道路的理解逐渐清晰，当然也逐渐开始分化，由此导致了开展布局完全不同的研究工作。智源社区将系统分析全球范围内知名学者对未来研究布局的「研究思路」，以帮助大家思考人工智能的未来。

智源导读：寻找深度学习的普适理论一直是学界关注的焦点。在深度学习的工作中，我们常常使用许多经验性的方法，例如选择不同的非线性层，样本的归一化，残差链接，卷积操作等等。这样的方法为网络带来了优秀的效果，经验性的理解也为深度学习发展提供了指导。但似乎我们对其理解仅限于此，由于网络的黑盒性质，这些方法究竟从理论上如何工作，为何需要加入网络，我们似乎难以回答。
近日UC Berkeley的马毅教授的报告“Deep Networks from First Principle”提供了一种系统性的理论观点。
报告中，马毅教授阐述了最大编码率衰减（Maximal Coding Rate Reduction,   MCR^2）作为深度模型优化的第一性原理的系列工作。此外，马毅介绍了近期的工作：通过优化 MCR^2 目标，能够直接构造出一种与常用神经网络架构相似的白盒深度模型，其中包括矩阵参数、非线性层、归一化与残差连接，甚至在引入「群不变性」后，可以直接推导出多通道卷积的结构。该网络的计算具有精确直观的解释，受到广泛关注。
正如费曼所说「What I cannot create I do not understand」。该工作表明，为了学习到线性划分的样本表示，所有这些常用方法都能够精确推导出来，都是实现该目标所必须的。因此，通过该工作，可以更加直观细致地理解神经网络中的常用方法。
本文整理自该报告的部分内容，原报告链接如下：
https://www.youtube.com/watch?v=z2bQXO2mYPo
01
深度学习的第一性原理
Learn to Compress, Compress to Learn!
——马毅
聚类和分类是两种主要的目标，很多任务都可以归类为将数据划分成不同的部分。马毅教授提出，分类和聚类代表的学习任务，与数据压缩（Compression）有关，而这样的任务，通常是在寻找高维目标的低秩结构，且深度网络能够适应于这样的压缩场景。

我们引入一个假设，在数据处理中，通常面对的是具有低维结构的高维数据。在这样的情况下，学习的目标通常会包含三个基本问题：
Interpolation，我们寻找样本之间的相似关系，这体现为聚类或分类任务；
Extrapolation，当获得第一个阶段任务的信息后，我们就可以对新的样本进行归类，判断未知样本的结构。
Representation，我们能够了解数据的信息，并建模描述它。
深度学习则将上述数据分析的任务“塞”进黑箱运算中。例如在神经网络分类任务中，我们将输入与输出的标签相互对应，然而足够大的深度网络能够拟合任何给定标签。
尽管在实践中取得了很好的效果，但是理论上来说，训练深度神经网络并不能保证稳定和最优，且我们无法从中了解到模型究竟学到了什么。
IB Theory 提供了一种理解方法，但是，从信息视角理解网络会碰到一个问题，即在高维数据上，传统信息论的统计量是无法定义的，高维数据常常是退化分布的，无法完成有效测量。
02
通过压缩来聚类和分类
传统聚类方法通常采用最大化相似度的方法进行，而应用在高维退化分布的数据上时，相似度难以定义。因此，我们从更基础的问题出发，为什么需要聚类划分数据？

从压缩角度，我们可以看出，能够划分的数据具有更小的空间，通过划分能够获得对数据更有效的表示。如果能找到编码长度的有效度量，就可以设计相应的优化目标。
熵是度量编码长度的工具，但在高维数据上，熵的测量非常困难，马毅教授采用率失真理论来度量这样的表示，提出了编码长度函数（Coding Length Function）：

有上图的度量后，我们就能描述聚类或划分的现象，即划分前的数据所须的编码长度，大于划分后的编码长度。这样的划分不需要标签，而是可以通过一些贪心算法，比较不同划分之间的编码长度，获得使划分后编码长度最小的划分。结果展现了这样的方法有非常好的聚类效果，能够找到全局最优的划分，并对离群点非常鲁棒。
同样的方法可以应用于分类任务，通过比较将新数据划分到不同类别增加的编码长度，选取使编码长度增加最少的类别，作为该样本最合适的分类，这种方法依旧来源于最小划分后编码长度的理论。这种方法可以理解为，将新样本划分到合适的类别分类后，所带来的存储开销应当最少，通过正确分类，可以得到最优的表示效率。结果显示，比较传统方法，MICL能够找到更加紧的边界，并且与分类不同的是，其决策边界更接近于数据本身的结构特征。

03
通过最大编码率降低来表示
在完成了 Interpolation(聚类)与 Extrapolation(分类)后，从压缩的视角，还能够实现对数据的表示。当数据符合某种低秩结构时，优秀的表达的目标可以被理解为，最大限度地学习到该结构特征，即，在让同一结构样本靠近的同时，使样本表达能力最大；同时，将不同结构数据间的差异尽可能清晰地体现出来。
具体来说，有三条原则：
1. 压缩同类别数据；
2. 区分不同类别数据；
3. 每类数据能够表达的范围尽可能大。
上文中的Coding Length Function同样为最优表达提供了度量。当样本表达最优时，其表达所占的空间能够最大化，即整体的样本集拥有最大的编码长度。而对于混合类别的数据，令其划分后所须的编码长度最小化，即使其能够让属于不同结构的样本相互靠近。基于这样的目标，CLF描述了如下的学习目标。

这被称为Maximum Coding Rate Reduction ( MCR^2 )，下图展示了其直观的解释：

为了使不同范围的样本进行比较，针对每个样本需要进行归一化操作。这与归一化的通常理解相符，使模型能够比较不同范围的样本。
04
从优化编码率降低来构建深度网络
通过对 MCR^2 目标进行梯度下降优化，我们甚至可以利用这一原理构造一个新的深层网络ReduNet。下图展示了详细的推导过程。对该目标求梯度后，获得了两个操作矩阵E、C，所求梯度就是其分别与样本乘积的和。
而观察E、C两个操作矩阵，会发现其与样本乘积的结果天然带有几何的解释，即样本Z对于其余样本，和各划分类别样本的残差。因此，若需要扩展样本空间的大小，只需加上E与样本相乘获得的残差，若要压缩各类别子空间的大小，仅需减去与C进行相同操作的结果。

梯度下降过程可以表现为如下结构的网络，其中，每个C对应一个划分的类别，E为求梯度得到的扩张操作矩阵，而梯度下降的更新过程体现为原样本加上得到的梯度，即梯度加上原样本的残差I：

对比常用的神经网络结构，可以发现其与ReduNet有许多相似之处，例如残差链接，C的多通道性质，非线性层等。同时，ReduNet所有参数均能够在前向传播中计算得到，因此网络无需BP优化。

05
从平移不变性得到卷积网络
通过引入组不变性，将cyclic shift后的样本视为同一组，每次将一组样本编码到不同低秩空间，ReduNet可以实现识别的平移不变性。
同时，类似卷积的网络性质也随之而来。在引入平移不变的任务要求后，网络使用循环矩阵表示样本，因而在与E，C矩阵进行矩阵乘时，网络的操作自然地等价于循环卷积。
但考虑不变性时，另一个问题出现了。当存在无数种shift可能时，若样本是稠密的，则其可以通过变换生成任意信号，因此，样本的稀疏性和不变性是不可兼得的，这体现为“不变性”与“稀疏性”的Trade-off。通常深度网络可能隐含了样本的稀疏化过程，而ReduNet则使用了随机卷积核提取样本的稀疏编码。

可以看到E，C在考虑不变性后，自动产生了卷积效果，且求逆计算使得通道间的操作相互关联。上述计算还可以通过频域变换来加速计算效率。

构造的ReduNet也可以通过反向传播训练，且前向传播计算得到的参数，为反向传播训练提供了非常好的参数初始化，通过该初始化得到的参数，经过BP训练后，结果比随机初始化并BP训练的结果有显著提升
马毅教授指出，在构造ReduNet的过程中发现，深度神经网络中常用的操作，稀疏编码，频域计算，卷积，归一化，非线性等等，都是为了实现优化 MCR^2 目标，学习一个可线性划分的表示所必须的操作，且可以在构造网络的过程中推导出来。教授在报告中引用的费曼这句话"What I cannot create I do not understand."，深刻地揭示了该工作的意义。当深度网络中曾经广泛使用的操作能够真正被构造出来时，我们才真正理解了他们。
06
总  结
基于“First Principle”的理论，报告中的工作展现了广泛的前景。报告中拓展了许多未来方向，其中包括基础的关于压缩与学习关联的理论，关于 MCR^2 准则的研究，以及对ReduNet网络的进一步优化工作。

尽管上文中算法有诸多变化，其核心都是基于“压缩”的概念。聚类，划分，表征，这些学习任务都可以被表述成压缩任务。我们希望学习到样本的知识，是期望能够更高效地表示样本，因此我们学习类别，提取特征，抽象概念。 MCR^2 原理基于率失真理论，描述了划分和压缩的过程，并能够基于压缩，完成包括聚类，分类，表示学习，构造网络等等任务，体现了作为学习的一般原理的泛用性能。

随机森林(决策树)超易懂原理
决策树的每一个决策节点向下分类的过程中，所依据的分类原则是找到变量的阈值，使该分类层级的Gini impurity（分类得到的子集中任选一个点，这个点是错误分类的概率）最小：
有个数据框：
x	y	color
0	1	blue
0.5	0.5	blue
1.1	1.5	blue
1.8	2.1	red
1.9	2.8	red
2	2	green
2.5	2.2	green
3	3	green
3.6	3.3	green
3.7	3.5	green
第一个决策节点为整个数据集，假设分类变量的阈值为y=2，则整个数据集可被分为：
0	1	blue
0.5	0.5	blue
1.1	1.5	blue
和
1.8	2.1	red
1.9	2.8	red
2	2	green
2.5	2.2	green
3	3	green
3.6	3.3	green
3.7	3.5	green
右边数据框有2红5绿，任取一点判为错误的概率(即Gini impurity)为：
P抽到红*P判为绿 + P抽到绿*P判为红 = 2/7*5/7 + 5/7*2/7 = 20/49
左边数据框全部为3蓝，任取一点判为错误的概率(即Gini impurity)为：
0
进行加权：Gy-root = 0.7*20/49 + 0.3*0 = 0.286
假设分类变量的阈值为x=2，则整个数据集可被分为：
0	1	blue
0.5	0.5	blue
1.1	1.5	blue
1.8	2.1	red
1.9	2.8	red
和
2	2	green
2.5	2.2	green
3	3	green
3.6	3.3	green
3.7	3.5	green
右边数据框全部为5绿，任取一点判为错误的概率(即Gini impurity)为：
0
左边数据框为3蓝2红，任取一点判为错误的概率(即Gini impurity)为：
P抽到蓝*P判为红 + P抽到红*P判为蓝 = 3/5*2/5 + 2/5*3/5 = 12/26
进行加权：Gx-root = 0.5*12/26 + 0.5*0 = 0.231
以x=2为分类变量的阈值可以获得更小的Gini impurity，故舍弃y=2这一分类变量
https://mp.weixin.qq.com/s/lBRvQJwix2H8Up29vSI0Ig

(随机森林找关键环境因子) 微生物多样性驱动陆地生态系统的多功能性
1 文献信息：
Delgado-Baquerizo, M., Maestre, F. T., Reich, P. B., Jeffries, T. C., Gaitan, J. J., Encinar, D., … Singh, B. K. (2016). Microbial diversity drives multifunctionality in terrestrial ecosystems. Nature Communications, 7, 10541. doi:10.1038/ncomms10541.
2 内容概览：
尽管微生物群落对生态系统服务的重要性，但全球范围内的微生物多样性与多种生态系统功能和服务之间的关系（即多功能性） 还有待评估。在这里，我们使用两个独立的大规模数据库，其地理覆盖范围对比鲜明（分别来自78个全球旱地和179个苏格兰各地），并发现了土壤微生物多样性与陆地生态系统的多功能性呈正相关。即使同时考虑多种多功能驱动因素（气候、土壤非生物因素和空间预测因素），微生物多样性也对多功能性表现出直接积极的影响作用。我们的发现表明微生物多样性的损失可能会减少生态系统多功能性，从而对提供气候调节、土壤肥力以及陆地生态系统的粮食生产等服务产生负面影响。
在这里，我们假设微生物多样性：（一)促进陆地生态系统中的多功能性；(二）其与土壤pH、气候和空间预测因子等作为多功能变化的驱动因素一样重要。我们发现，土壤微生物多样性与旱地的多功能性呈正相关。微生物多样性对多功能性的积极影响，即使同时考虑多个气候、非生物和空间预测因子的多功能性。我们的研究提供了经验证据，表明微生物多样性与全球范围内陆地生态系统的多功能性呈正相关，并进一步表明微生物的任何损失都与全球范围内的陆地生态系统呈正相关；多样性可能会降低陆地生态系统中维持多种生态系统功能和服务的。
微生物多样性和生态系统多功能性。我们用潜在净氮(N)矿化、硝态氮、铵态氮、DNA浓度、有效磷(P)和植物生产力来表征生态系统多功能性。alpha多样性，系统发育多样性和物种丰富度也与多种功能密切相关。据我们所知，我们的结果首次提供了证据，表明微生物多样性在全球范围内与陆地生态系统中的多功能呈正相关。
生态系统多功能性的驱动者。我们使用随机森林模型来确定多功能性最重要的预测因子(经纬度、海拔、平均年温(MAT)、平均年降水量(MAP)、土壤pH和微生物多样性)；结构方程建模(SEM)用来分析在考虑多种多功能的驱动因子后，微生物多样性与多功能性之间的关系是否能够继续维持。我们的随机森林模型表明，微生物多样性与其他多功能预测因子一样重要或更重要，其次是年平均温度和降水量。我们的SEM分别解释了旱地和苏格兰数据集生态系统多功能性中53%和38%的方差。
3 主图图表：

Figure 1 | Relationships between microbial diversity and ecosystem multifunctionality.Results are shown for the Drylands (bacteria (a) and fungi (b))and Scotland (bacteria (c)) data sets.

Figure 2 | Main predictors of ecosystem multifunctionality.The fifigure shows the Random Forest mean predictor importance (% of increase of MSE) ofenvironmental drivers and microbial diversity (Shannon index, bits) on ecosystem multifunctionality for the Drylands (a) and Scotland (b) data sets.

Figure 3 | Direct and indirect effects of space, climate, soil pH and microbial diversity on ecosystem multifunctionality.Structural equation models areshown for the Drylands (a,c) and Scotland (b,d) data sets.

Figure 4 | Direct and indirect effects of space, climate, soil pH, plant richness and microbial diversity on ecosystem multifunctionality in globaldrylands

随机森林分析理论与实践
原创红皇后学术红皇后学术2020-05-27
随机森林 (Random Forest)
随机森林是机器学习算法的一种，最早由 Leo Breiman 和 Adele Cutler提出。
机器学习是一种智能的数据挖掘技术，更具已有的知识建立预测模型来识别大数据中的有用信息。
随机森林的目的就是根据已有的数据建立模型，从而实现对数据的分类和回归。
如果目标变量是分类变量，随机森林可以进行分类；如果目标变量是连续变量，随机森林可以进行回归预测。
通过随机森林分析，可以找出能够区分不同组样本间差异的关键物种或OTU。
随机森林的优势
其在目前所有的分类算法中，具有最高的准确性；
能够有效的处理大规模数据；
能够处理具有数千个变量的数据；
能够给出哪些变量对于分类更加重要；
能够有效的处理具有许多missing data的数据；
产生的分类方法可以用于分析后续的其它数据；
能够进行聚类并定位异常值。
分类树的构建
随机森林的最基础构建是分类树，其主要工作就是选取特征对数据集进行划分，最后把数据贴上不同的标签，从而达到对数据集进行分类的目的。
构建好的决策树呈树形结构，可以认为是if-then规则的集合，其结构有点类似多元回归树，使用，使用一个层级的特征变量对数据集进行区分，直到每一个数据集都有其明确的分类为止。

分类树的“种植”与“生长”过程
输入的样本有N个不同的分类，随机有放回的在所有样本中选择N个个体，建立分类树的训练集；
输入的样本有M个属性变量，选择一个远小于M的数值m，从M个属性变量中随机无放回的抽取m个变量作为分裂属性；
从抽取的m个属性中采用某种策略选择一个属性对样本进行分裂；
不断的重复2和3步骤，直到分类树无法分裂为止。
这样就得到了一个分类树，这种分类树会尽最大程度的生长，并且没有剪枝过程。
随机森林的原理
对输入的样本不断的重复构建分类树的过程，最后一定数量的分类树就组成了一个森林，就是我们所说的随机森林模型。
随机森林基于同时构建了非常多的分类树，在数据输入到随机森林模型中之后，首先将其放置到每一个分类树中，每一个分类树会给出一个分类结果，这类似一个投票，之后在整个森林中进行计票，最终得到目标的准确分类。
随机森林的准确性
随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。
随机森林可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。
由于随机森林建立分类树的抽样方法，在建立每一颗分类树时，相当于只有2/3的样本被用所训练集，此时另外1/3的剩余样本就可以当作测试集对分类树的准确性进行检验。
它的计算方式如下：
对每个样本，计算它作为测试样本时树对它的分类情况（约1/3的树）；
然后以所有该样本的分类的投票结果作为该样本的分类；
最后用误分个数占样本总数的比率作为随机森林的错误比例。
随机森林分类的准确性受到两个因素的控制。
森林中任意两个树的相关性 (correlation)，这种相关性越高，分类的错误率也越高；
每一个单独分类树的强度 (strength)，强度越高，分类的准确性越高。
在建立分类数时，分裂变量m的数量会影响这两个因素，降低m的数目，会同时降低相关性和强度指标，因此随机森林分析中最重要的就是确定最适合m值的范围。
分析实战
我们使用R语言的randomForest包进行随机森林分析，首先安装并载入该R包。
install.packages("randomForest")
library(randomForest)
还是照例先来介绍一下主函数的各项参数。
randomForest(formula, data=NULL, ..., subset, na.action=na.fail)
randomForest(x, y=NULL,  xtest=NULL, ytest=NULL, ntree=500,
             mtry=if (!is.null(y) && !is.factor(y))
             max(floor(ncol(x)/3), 1) else floor(sqrt(ncol(x))),
             replace=TRUE, classwt=NULL, cutoff, strata,
             sampsize = if (replace) nrow(x) else ceiling(.632*nrow(x)),
             nodesize = if (!is.null(y) && !is.factor(y)) 5 else 1,
             maxnodes = NULL,
             importance=FALSE, localImp=FALSE, nPerm=1,
             proximity, oob.prox=proximity,
             norm.votes=TRUE, do.trace=FALSE,
             keep.forest=!is.null(y) && is.null(xtest), corr.bias=FALSE,
             keep.inbag=FALSE, ...)
各参数意义：
data，用于分析的数据，数据框格式，行为样本，列为变量；
subset，使用向量指定哪些行用于分析；
na.action，规定如果发现缺失值如何处理；
x，formula，使用数据框、矩阵或者公式形式给出用于预测模型的因变量；
y，响应变量，如果是因子，则进行分类，如果是连续变量，则进行回归；
xtext，ytext，规定因变量和响应变量；
ntree，森林中包含分类树的数目；
mtry，上文提到的m值，用于建模的变量数目；
replace，个体采集是是否放回；
importance，是否评估因变量的重要性；
proximity，是否计算样本间的近邻测度；
其它参数有的我没看懂，不过默认就好。
接下来我们使用mtcars数据进行一个示例，首先载入数据文件。
⚠️要先将用于分类的变量设置为因子。
data(mtcars)
mtcars$cyl <- as.factor(mtcars$cyl)
进行随机森林分析并查看结果。
mtcars.rf <- randomForest(cyl~., data = mtcars,importance = TRUE,
                          proximity = TRUE,ntree = 1000, mtry = 3)
print(mtcars.rf)
⚠️这里的mtry = 3是根据第一次默认的结果设置的。

这里最重要的结果是m的数值以及模型预测的错误比例。
预测准确性结果可视化
接下来用一个热图来展示模型预测的准确性。
library(pheatmap)
pheatmap(mtcars.rf$confusion[,1:length(unique(mtcars$cyl))],cluster_rows = FALSE,
         cluster_cols = FALSE,legend = FALSE,display_numbers = TRUE,
         number_format = "%o",fontsize = 20,number_color = "black",
         main = paste("RF classifier (",(1-mtcars.rf$err.rate[1000,1])*100,"% correct)",sep = ""),
         fontsize_number = 26)

这里只是给出一个简单的绘图示例代码，大家可以参考其它热图的绘制方法自行美化一下结果图，比如画成下面这样。

分类贡献率结果可视化
使用下面的代码可以得到不同变量对于样本分类的贡献率，并绘制一个基本的图像。
round(importance(mtcars.rf), 2)
varImpPlot(mtcars.rf)

结果中的MeanDecreaseAccuracy，为平均减少准确度，即没有这个变量时，分类准确度下降的程度，相当于我们常用的分类贡献度的概念。
接下来将不同变量的importance值提取出来单独绘制一个图像。
library(ggplot2)
imp <- data.frame(ID = rownames(mtcars.rf$importance),
                  importance = as.data.frame(mtcars.rf$importance)$MeanDecreaseAccuracy)
imp$ID <- factor(imp$ID, levels = imp[order(imp$importance),1])
imp <- imp[order(imp$importance,decreasing = TRUE),]
imp$m <- c(rep(1,mtcars.rf$mtry),rep(2,length(colnames(mtcars))-mtcars.rf$mtry[1]-1))
imp$m <- as.factor(imp$m)
ggplot(imp,aes(ID,importance,fill = m)) +
    geom_bar(stat = "identity",width = 0.7) + coord_flip() +
    theme_bw()+ theme(panel.grid=element_blank()) +
    theme(panel.border = element_blank()) +
    theme(panel.background=element_rect(fill='transparent', color='black'),
          plot.margin = unit(c(3,5,1,1),"mm")) +
    scale_y_continuous(limits = c(0,max(imp$importance*1.1)),expand = c(0,0)) +
    theme(axis.text.y = element_text(size = 12,face = "bold",colour = "black"),
          axis.text.x = element_text(size = 12,face = "bold",colour = "black"),
          axis.title.x = element_text(size = 16,face = "bold",colour = "black")) +
    xlab("") +ylab("Importance") +
    scale_fill_manual(values = c("navy blue","grey")) +
    theme(legend.position = "none")

机器学习算法 - 随机森林
机器学习算法 - 随机森林之决策树初探（1）
原创生信宝典生信宝典1月10日

随机森林是基于集体智慧的一个机器学习算法，也是目前最好的机器学习算法之一。
随机森林实际是一堆决策树的组合（正如其名，树多了就是森林了）。在用于分类一个新变量时，相关的检测数据提交给构建好的每个分类树。每个树给出一个分类结果，最终选择被最多的分类树支持的分类结果。回归则是不同树预测出的值的均值。
要理解随机森林，我们先学习下决策树。
决策树 - 把你做选择的过程呈现出来
决策树是一个很直观的跟我们日常做选择的思维方式很相近的一个算法。
如果有一个数据集如下：
data <- data.frame(x=c(0,0.5,1.1,1.8,1.9,2,2.5,3,3.6,3.7), color=c(rep('blue',5),rep('green',5)))
data

##      x color
## 1  0.0  blue
## 2  0.5  blue
## 3  1.1  blue
## 4  1.8  blue
## 5  1.9  blue
## 6  2.0 green
## 7  2.5 green
## 8  3.0 green
## 9  3.6 green
## 10 3.7 green
那么假如加入一个新的点，其x值为1，那么该点对应的最可能的颜色是什么？
根据上面的数据找规律，如果x<2.0则对应的点颜色为blue，如果x>=2.0则对应的点颜色为green。这就构成了一个只有一个决策节点的简单决策树。

决策树常用来回答这样的问题：给定一个带标签的数据集(标签这里对应我们的color列)，怎么来对新加入的数据集进行分类?
如果数据集再复杂一些，如下，
data <- data.frame(x=c(0,0.5,1.1,1.8,1.9,2,2.5,3,3.6,3.7),
                   y=c(1,0.5,1.5,2.1,2.8,2,2.2,3,3.3,3.5),
                   color=c(rep('blue',3),rep('red',2),rep('green',5)))
data

##      x   y color
## 1  0.0 1.0  blue
## 2  0.5 0.5  blue
## 3  1.1 1.5  blue
## 4  1.8 2.1   red
## 5  1.9 2.8   red
## 6  2.0 2.0 green
## 7  2.5 2.2 green
## 8  3.0 3.0 green
## 9  3.6 3.3 green
## 10 3.7 3.5 green
如果x>=2.0则对应的点颜色为green。
如果x<2.0则对应的点颜色可能为blue，也可能为red。
这时就需要再加一个新的决策节点，利用变量y的信息。

这就是决策树，也是我们日常推理问题的一般方式。
训练决策树 - 确定决策树的根节点
第一个任务是确定决策树的根节点：选择哪个变量和对应阈值选择多少能给数据做出最好的区分。
比如上面的例子，我们可以先处理变量x，选择阈值为2(为什么选2，是不是有比2更合适阈值，我们后续再说)，则可获得如下分类：

我们也可以先处理变量y，选择阈值为2，则可获得如下分类：

那实际需要选择哪个呢？
实际我们是希望每个选择的变量和阈值能把不同的类分的越开越好；上面选择变量x分组时，Green完全分成一组；下面选择y分组时，Blue完全分成一组。怎么评价呢？
这时就需要一个评价指标，常用的指标有Gini inpurity和Information gain。
Gini Impurity
在数据集中随机选择一个数据点，并随机分配给它一个数据集中存在的标签，分配错误的概率即为Gini impurity。
我们先看第一套数据集，10个数据点，5个blue，5个green。从中随机选一个数据点，再随机选一个分类标签作为这个数据点的标签，分类错误的概率是多少？如下表，错误概率为0.25+0.25=0.5(看下面的计算过程)。
probility <- data.frame(Event=c("Pick Blue, Classify Blue",
                                "Pick Blue, Classify Green",
                                "Pick Green, Classify Blue",
                                "Pick Green, Classify Green"), 
                        Probability=c(5/10 * 5/10, 5/10 * 5/10, 5/10 * 5/10, 5/10 * 5/10),
                        Type=c("Blue" == "Blue",
                               "Blue" == "Green",
                               "Green" == "Blue",
                               "Green" == "Green"))
probility

##                        Event Probability  Type
## 1   Pick Blue, Classify Blue        0.25  TRUE
## 2  Pick Blue, Classify Green        0.25 FALSE
## 3  Pick Green, Classify Blue        0.25 FALSE
## 4 Pick Green, Classify Green        0.25  TRUE
我们再看第二套数据集，10个数据点，2个red，3个blue，5个green。从中随机选一个数据点，再随机选一个分类标签作为这个数据点的标签，分类错误的概率是多少？0.62。
probility <- data.frame(Event=c("Pick Blue, Classify Blue",
                                "Pick Blue, Classify Green",
                                "Pick Blue, Classify Red",
                                "Pick Green, Classify Blue",
                                "Pick Green, Classify Green",
                                "Pick Green, Classify Red",
                                "Pick Red, Classify Blue",
                                "Pick Red, Classify Green",
                                "Pick Red, Classify Red"
                              ),
                        Probability=c(3/10 * 3/10, 3/10 * 5/10, 3/10 * 2/10, 
                                      5/10 * 3/10, 5/10 * 5/10, 5/10 * 2/10,
                                      2/10 * 3/10, 2/10 * 5/10, 2/10 * 2/10),
                        Type=c("Blue" == "Blue",
                               "Blue" == "Green",
                               "Blue" == "Red",
                               "Green" == "Blue",
                               "Green" == "Green",
                               "Green" == "Red",
                               "Red" == "Blue",
                               "Red" == "Green",
                               "Red" == "Red"
                               ))
probility

##                        Event Probability  Type
## 1   Pick Blue, Classify Blue        0.09  TRUE
## 2  Pick Blue, Classify Green        0.15 FALSE
## 3    Pick Blue, Classify Red        0.06 FALSE
## 4  Pick Green, Classify Blue        0.15 FALSE
## 5 Pick Green, Classify Green        0.25  TRUE
## 6   Pick Green, Classify Red        0.10 FALSE
## 7    Pick Red, Classify Blue        0.06 FALSE
## 8   Pick Red, Classify Green        0.10 FALSE
## 9     Pick Red, Classify Red        0.04  TRUE

Wrong_probability = sum(probility[!probility$Type,"Probability"])
Wrong_probability

## [1] 0.62
Gini Impurity计算公式：
假如我们的数据点共有C个类，p(i)是从中随机拿到一个类为i的数据，Gini Impurity计算公式为：
$$ G = \sum_{i=1}^{C} p(i)*(1-p(i)) $$ 

对第一套数据集，10个数据点，5个blue，5个green。从中随机选一个数据点，再随机选一个分类标签作为这个数据点的标签，分类错误的概率是多少？错误概率为0.25+0.25=0.5。

对第二套数据集，10个数据点，2个red，3个blue，5个green。
从中随机选一个数据点，再随机选一个分类标签作为这个数据点的标签，分类错误的概率是多少？0.62。

决策树分类后的Gini Impurity
对第一套数据集来讲，按照x<2分成两个分支，各个分支都只包含一个分类数据，各自的Gini IMpurity值为0。
这是一个完美的决策树，把Gini Impurity为0.5的数据集分类为2个Gini Impurity为0的数据集。Gini Impurity== 0是能获得的最好的分类结果。


第二套数据集，我们有两种确定根节点的方式，哪一个更优呢？
我们可以先处理变量x，选择阈值为2，则可获得如下分类：

每个分支的Gini Impurity可以如下计算：

当前决策的Gini impurity需要对各个分支包含的数据点的比例进行加权，即

我们也可以先处理变量y，选择阈值为2，则可获得如下分类：

每个分支的Gini Impurity可以如下计算：

当前决策的Gini impurity需要对各个分支包含的数据点的比例进行加权，即

两个数值比较0.24<0.29，选择x作为第一个分类节点是我们第二套数据第一步决策树的最佳选择。
前面手算单个变量、单个分组不算麻烦，也是个学习的过程。后续如果有更多变量和阈值时，再手算就不合适了。下一篇我们通过暴力方式自写函数训练决策树。
当前计算的结果，可以作为正对照，确定后续函数结果的准确性。

机器学习算法-随机森林之决策树R 代码从头暴力实现（2）
原创生信宝典生信宝典1月11日

前文（机器学习算法 - 随机森林之决策树初探（1））讲述了决策树的基本概念、决策评价标准并手算了单个变量、单个分组的Gini impurity。是一个基本概念学习的过程，如果不了解，建议先读一下再继续。
本篇通过 R 代码（希望感兴趣的朋友能够投稿这个代码的Python实现）从头暴力方式自写函数训练决策树。之前计算的结果，可以作为正对照，确定后续函数结果的准确性。
训练决策树 - 确定根节点的分类阈值
Gini impurity可以用来判断每一步最合适的决策分类方式，那么怎么确定最优的分类变量和分类阈值呢？
最粗暴的方式是，我们用每个变量的每个可能得阈值来进行决策分类，选择具有最低Gini impurity值的分类组合。这不是最快速的解决问题的方式，但是最容易理解的方式。
定义计算Gini impurity的函数
data <- data.frame(x=c(0,0.5,1.1,1.8,1.9,2,2.5,3,3.6,3.7),
                   y=c(1,0.5,1.5,2.1,2.8,2,2.2,3,3.3,3.5),
                   color=c(rep('blue',3),rep('red',2),rep('green',5)))

data

##      x   y color
## 1  0.0 1.0  blue
## 2  0.5 0.5  blue
## 3  1.1 1.5  blue
## 4  1.8 2.1   red
## 5  1.9 2.8   red
## 6  2.0 2.0 green
## 7  2.5 2.2 green
## 8  3.0 3.0 green
## 9  3.6 3.3 green
## 10 3.7 3.5 green
首先定义个函数计算每个分支的Gini_impurity。
Gini_impurity <- function(branch){
  # print(branch)
  len_branch <- length(branch)
  if(len_branch==0){
    return(0)
  }
  table_branch <- table(branch)
  wrong_probability <- function(x, total) (x/total*(1-x/total))
  return(sum(sapply(table_branch, wrong_probability, total=len_branch)))
}
测试下，没问题。
Gini_impurity(c(rep('a',2),rep('b',3)))

## [1] 0.48
再定义一个函数，计算每次决策的总Gini impurity.
Gini_impurity_for_split_branch <- function(threshold, data, variable_column, 
                                           class_column, Init_gini_impurity=NULL){
  total = nrow(data)
  left <- data[data[variable_column]<threshold,][[class_column]]
  left_len = length(left)
  left_table = table(left)
  left_gini <- Gini_impurity(left)

  right <- data[data[variable_column]>=threshold,][[class_column]]
  right_len = length(right)
  right_table = table(right)
  right_gini <- Gini_impurity(right)
  total_gini <- left_gini * left_len / total + right_gini * right_len /total

  result = c(variable_column,threshold, 
             paste(names(left_table), left_table, collapse="; ", sep=" x "),
             paste(names(right_table), right_table, collapse="; ", sep=" x "),
             total_gini)

  names(result) <- c("Variable", "Threshold", "Left_branch", "Right_branch", "Gini_impurity")

  if(!is.null(Init_gini_impurity)){
    Gini_gain <- Init_gini_impurity - total_gini
    result = c(variable_column, threshold, 
             paste(names(left_table), left_table, collapse="; ", sep=" x "),
             paste(names(right_table), right_table, collapse="; ", sep=" x "),
             Gini_gain)

    names(result) <- c("Variable", "Threshold", "Left_branch", "Right_branch", "Gini_gain")
  }

  return(result)
}
测试下，跟之前计算的结果一致：
as.data.frame(rbind(Gini_impurity_for_split_branch(2, data, 'x', 'color'), 
                            Gini_impurity_for_split_branch(2, data, 'y', 'color')))

##   Variable Threshold       Left_branch       Right_branch     Gini_impurity
## 1        x         2 blue x 3; red x 2          green x 5              0.24
## 2        y         2          blue x 3 green x 5; red x 2 0.285714285714286
暴力决策根节点和阈值
基于前面定义的函数，遍历每一个可能的变量和阈值。
首先看下基于变量x的计算方法：
uniq_x <- sort(unique(data$x))
delimiter_x <- zoo::rollmean(uniq_x,2)
impurity_x <- as.data.frame(do.call(rbind, lapply(delimiter_x, Gini_impurity_for_split_branch, 
                                    data=data, variable_column='x', class_column='color')))
print(impurity_x)

##   Variable Threshold                  Left_branch                 Right_branch     Gini_impurity
## 1        x      0.25                     blue x 1 blue x 2; green x 5; red x 2 0.533333333333333
## 2        x       0.8                     blue x 2 blue x 1; green x 5; red x 2             0.425
## 3        x      1.45                     blue x 3           green x 5; red x 2 0.285714285714286
## 4        x      1.85            blue x 3; red x 1           green x 5; red x 1 0.316666666666667
## 5        x      1.95            blue x 3; red x 2                    green x 5              0.24
## 6        x      2.25 blue x 3; green x 1; red x 2                    green x 4 0.366666666666667
## 7        x      2.75 blue x 3; green x 2; red x 2                    green x 3 0.457142857142857
## 8        x       3.3 blue x 3; green x 3; red x 2                    green x 2             0.525
## 9        x      3.65 blue x 3; green x 4; red x 2                    green x 1 0.577777777777778
再包装2个函数，一个计算单个变量为决策节点的各种可能决策的Gini impurity, 另一个计算所有变量依次作为决策节点的各种可能决策的Gini impurity。
Gini_impurity_for_all_possible_branches_of_one_variable <- function(data, variable, class, Init_gini_impurity=NULL){
  uniq_value <- sort(unique(data[[variable]]))
  delimiter_value <- zoo::rollmean(uniq_value,2)
  impurity <- as.data.frame(do.call(rbind, lapply(delimiter_value, 
                                     Gini_impurity_for_split_branch, data=data, 
                                     variable_column=variable, 
                                     class_column=class,
                                     Init_gini_impurity=Init_gini_impurity)))
  if(is.null(Init_gini_impurity)){
    decreasing = F
  } else {
    decreasing = T
  }
  impurity <- impurity[order(impurity[[colnames(impurity)[5]]], decreasing = decreasing),]
  return(impurity)
}

Gini_impurity_for_all_possible_branches_of_all_variables <- function(data, variables, class, Init_gini_impurity=NULL){
  one_split_gini <- do.call(rbind, lapply(variables,
                                          Gini_impurity_for_all_possible_branches_of_one_variable, 
                                          data=data, class=class,
                                          Init_gini_impurity=Init_gini_impurity))
  if(is.null(Init_gini_impurity)){
    decreasing = F
  } else {
    decreasing = T
  }
  one_split_gini[order(one_split_gini[[colnames(one_split_gini)[5]]], decreasing = decreasing),]
}
测试下：
Gini_impurity_for_all_possible_branches_of_one_variable(data, 'x', 'color')

##   Variable Threshold                  Left_branch                 Right_branch     Gini_impurity
## 5        x      1.95            blue x 3; red x 2                    green x 5              0.24
## 3        x      1.45                     blue x 3           green x 5; red x 2 0.285714285714286
## 4        x      1.85            blue x 3; red x 1           green x 5; red x 1 0.316666666666667
## 6        x      2.25 blue x 3; green x 1; red x 2                    green x 4 0.366666666666667
## 2        x       0.8                     blue x 2 blue x 1; green x 5; red x 2             0.425
## 7        x      2.75 blue x 3; green x 2; red x 2                    green x 3 0.457142857142857
## 8        x       3.3 blue x 3; green x 3; red x 2                    green x 2             0.525
## 1        x      0.25                     blue x 1 blue x 2; green x 5; red x 2 0.533333333333333
## 9        x      3.65 blue x 3; green x 4; red x 2                    green x 1 0.577777777777778
两个变量的各个阈值分别进行决策，并计算Gini impurity,输出按Gini impurity由小到大排序后的结果。根据变量x和阈值1.95(与上面选择的阈值2获得的决策结果一致)的决策可以获得本步决策的最好结果。
variables <- c('x', 'y')
Gini_impurity_for_all_possible_branches_of_all_variables(data, variables, class="color")

##    Variable Threshold                  Left_branch                 Right_branch     Gini_impurity
## 5         x      1.95            blue x 3; red x 2                    green x 5              0.24
## 3         x      1.45                     blue x 3           green x 5; red x 2 0.285714285714286
## 31        y      1.75                     blue x 3           green x 5; red x 2 0.285714285714286
## 4         x      1.85            blue x 3; red x 1           green x 5; red x 1 0.316666666666667
## 6         x      2.25 blue x 3; green x 1; red x 2                    green x 4 0.366666666666667
## 41        y      2.05          blue x 3; green x 1           green x 4; red x 2 0.416666666666667
## 2         x       0.8                     blue x 2 blue x 1; green x 5; red x 2             0.425
## 21        y      1.25                     blue x 2 blue x 1; green x 5; red x 2             0.425
## 51        y      2.15 blue x 3; green x 1; red x 1           green x 4; red x 1              0.44
## 7         x      2.75 blue x 3; green x 2; red x 2                    green x 3 0.457142857142857
## 71        y       2.9 blue x 3; green x 2; red x 2                    green x 3 0.457142857142857
## 61        y       2.5 blue x 3; green x 2; red x 1           green x 3; red x 1 0.516666666666667
## 8         x       3.3 blue x 3; green x 3; red x 2                    green x 2             0.525
## 81        y      3.15 blue x 3; green x 3; red x 2                    green x 2             0.525
## 1         x      0.25                     blue x 1 blue x 2; green x 5; red x 2 0.533333333333333
## 11        y      0.75                     blue x 1 blue x 2; green x 5; red x 2 0.533333333333333
## 9         x      3.65 blue x 3; green x 4; red x 2                    green x 1 0.577777777777778
## 91        y       3.4 blue x 3; green x 4; red x 2                    green x 1 0.577777777777778


机器学习算法-随机森林之决策树R 代码从头暴力实现（3）

前文 (机器学习算法 - 随机森林之决策树初探（1）) 讲述了决策树的基本概念、决策评价标准并手算了单个变量、单个分组的Gini impurity。是一个基本概念学习的过程，如果不了解，建议先读一下再继续。
机器学习算法-随机森林之决策树R 代码从头暴力实现（2）通过 R 代码从头暴力方式自写函数训练决策树，已决策出第一个节点。后续......
再决策第二个节点、第三个节点
第一个决策节点找好了，后续再找其它决策节点。如果某个分支的点从属于多个class，则递归决策。
递归决策终止的条件是：
再添加分支不会降低Gini impurity
某个分支的数据点属于同一分类组 (Gini impurity = 0)
定义函数如下：

brute_descition_tree_result <- list()
brute_descition_tree_result_index <- 0

# 递归分支决策
brute_descition_tree <- function(data, measure_variable, class_variable, type="Root"){

  # 计算初始Gini值
  Init_gini_impurity <- Gini_impurity(data[[class_variable]])

  # 确定当前需要决策的节点的最优变量和最优阈值
  brute_force_result <- Gini_impurity_for_all_possible_branches_of_all_variables(
    data,  variables, class=class_variable, Init_gini_impurity=Init_gini_impurity)

  # 输出中间计算结果
  print(brute_force_result)

  # 根据最优决策变量、阈值和Gini增益
  split_variable <- brute_force_result[1,1]
  split_threshold <- brute_force_result[1,2]
  gini_gain = brute_force_result[1,5]
  # print(gini_gain)



  # 判断此次决策是否需要保留
  if(gini_gain>0){
    brute_descition_tree_result_index <<- brute_descition_tree_result_index + 1
    brute_descition_tree_result[[brute_descition_tree_result_index]] <<-  
                                    c(type=type, split_variable=split_variable,
                                         split_threshold=split_threshold)
    # print(brute_descition_tree_result_index)
    # print(brute_descition_tree_result)

    # 决策左右分支
    left <- data[data[split_variable]<split_threshold,]
    right <- data[data[split_variable]>=split_threshold,]

    # 分别对左右分支进一步决策
    if(length(unique(left[[class_variable]]))>1){
      brute_descition_tree(data=left, measure_variable, class_variable,
                           type=paste(brute_descition_tree_result_index, "left"))

    }
    if(length(unique(right[[class_variable]]))>1){
      brute_descition_tree(data=right, measure_variable, class_variable,
                           type=paste(brute_descition_tree_result_index, "right"))
    }
  }
  # return(brute_descition_tree_result)
}

调用函数，并输出中间计算结果
brute_descition_tree(data, variables, "color")

根节点评估记录
##    Variable Threshold                  Left_branch                 Right_branch          Gini_gain
## 5         x      1.95            blue x 3; red x 2                    green x 5               0.38
## 3         x      1.45                     blue x 3           green x 5; red x 2  0.334285714285714
## 31        y      1.75                     blue x 3           green x 5; red x 2  0.334285714285714
## 4         x      1.85            blue x 3; red x 1           green x 5; red x 1  0.303333333333333
## 6         x      2.25 blue x 3; green x 1; red x 2                    green x 4  0.253333333333333
## 41        y      2.05          blue x 3; green x 1           green x 4; red x 2  0.203333333333333
## 2         x       0.8                     blue x 2 blue x 1; green x 5; red x 2              0.195
## 21        y      1.25                     blue x 2 blue x 1; green x 5; red x 2              0.195
## 51        y      2.15 blue x 3; green x 1; red x 1           green x 4; red x 1               0.18
## 7         x      2.75 blue x 3; green x 2; red x 2                    green x 3  0.162857142857143
## 71        y       2.9 blue x 3; green x 2; red x 2                    green x 3  0.162857142857143
## 61        y       2.5 blue x 3; green x 2; red x 1           green x 3; red x 1  0.103333333333333
## 8         x       3.3 blue x 3; green x 3; red x 2                    green x 2              0.095
## 81        y      3.15 blue x 3; green x 3; red x 2                    green x 2              0.095
## 1         x      0.25                     blue x 1 blue x 2; green x 5; red x 2 0.0866666666666667
## 11        y      0.75                     blue x 1 blue x 2; green x 5; red x 2 0.0866666666666667
## 9         x      3.65 blue x 3; green x 4; red x 2                    green x 1 0.0422222222222223
## 91        y       3.4 blue x 3; green x 4; red x 2                    green x 1 0.0422222222222223

第二层节点评估记录
##    Variable Threshold       Left_branch      Right_branch         Gini_gain
## 3         x      1.45          blue x 3           red x 2              0.48
## 31        y       1.8          blue x 3           red x 2              0.48
## 2         x       0.8          blue x 2 blue x 1; red x 2 0.213333333333333
## 21        y      1.25          blue x 2 blue x 1; red x 2 0.213333333333333
## 4         x      1.85 blue x 3; red x 1           red x 1              0.18
## 41        y      2.45 blue x 3; red x 1           red x 1              0.18
## 1         x      0.25          blue x 1 blue x 2; red x 2              0.08
## 11        y      0.75          blue x 1 blue x 2; red x 2              0.08

最终选择的决策变量和决策阈值
as.data.frame(do.call(rbind, brute_descition_tree_result))

最终选择的决策变量和决策阈值
##     type split_variable split_threshold
## 1   Root              x            1.95
## 2 2 left              x            1.45

运行后，获得两个决策节点，绘制决策树如下：

从返回的Gini gain表格可以看出，第二个节点有两种效果一样的分支方式。

这样我们就用暴力方式完成了决策树的构建。

机器学习算法-随机森林之理论概述

前面我们用 3 条推文从理论和代码角度讲述了决策树的概念和粗暴生成。




















随机森林的算法概述
决策树我们应该都暴力的理解了，下面我们看随机森林。
随机森林实际是一堆决策树的组合（正如其名，树多了就是森林了）。
这是一个简化版的理解，实际上要复杂一些。具体怎么做的呢？
假设有一个数据集包含n个样品,p个变量，也就是一个n X p的矩阵，采用下面的算法获得一系列的决策树：
从数据集中有放回地取出n个样品作为训练集，训练1棵决策树。假如数据集有6个样品[a,b,c,d,e]，第一次有放回地取出6个样品可能是[a,a,c,d,d,e]，第二次有放回地取出6个样品可能是[a,c,c,d,e,e]。每棵树的训练集是不完全相同的，每个训练集内部包含重复样本。这一步称为Bagging (Bootstrap aggregating) 自助聚合。
每棵决策树构建时，通常随机抽取m (m<<p)个变量进行每个节点决策变量的选择，m在选择每一层节点时不变。
每棵决策树野蛮生长，不剪枝。
重复第1,2,3步t次，获得t棵决策树。
通过聚合t棵决策树做出最终决策：
分类问题：选择t棵决策树中支持最多的类作为最终分类 (服从多数，majority vote)
回归问题：计算t棵决策树预测的数值的均值作为最终预测值

为什么要这么做呢？
在数据科学中，很大数目的相对不相关的模型的群体决策优于任何单个模型的决策。
随机森林的错误率取决于两个因素：
不同树之间的相关性越高，则错误率越大。这要求m尽可能小。
每个决策树的分类强度越大，则错误率越小。这要求m尽可能大。
m通常为或。也可以通过oob (out-of-bag) error rate (自助聚合错误率)进行迭代调参选择分类效果最好的m。
随机森林工作机制
通过有放回采样的方式构建训练集时，通常有1/3的样品没有被选中。这些样品可以用来估计该训练集获得的决策树的分类错误率 (OOB), 也可以用来评估变量的重要性。
每棵决策树构建好后，用所有的数据作为输入获得每个样品在每个决策树的分类结果，然后计算每一对样品 (c1, c2)之间的相似度 ()。如果两个样品分类到相同的分类节点，它们彼此的相似性加一。最终的相似度除以决策树的总数 (t)目获得任意一对样品标准化后的的相似度值。这个值可后续用于缺失值填充、异常样品鉴定和对数据进行低维可视化。(i表示第i棵树)

oob (out-of-bag) error rate (聚合错误率)
应用随机森林时，无需交叉验证或额外的测试集来评估错误率，而是在构建随机森林时，可自行评估：
每一个决策树构建时使用到的样本是不同的。因为是有放回地随机采样，在构建每棵树时，大约1/3的样品不会被用到。
这些没有被抽样到用于构建决策树的样品用来测试该决策树的分类能力。理论上每个样品在1/3的决策树中可作为测试样品获得一个分类结果和分类错误率。
聚合每个没有被抽样到的样品在所有其不被用于训练集构建的决策树的分类结果作为该样品的最终分类结果，与数据的原始分类一致则是分类正确，否则是分类错误。
所有分类错误的样品除以总样品即为构建的随机森林模型的错误率。(i表示第i棵树)

确定分类的关键变量
对于每一棵随机决策树，统计其分类OOB数据集中所有样品正确分类的次数  (O)。随机排列OOB数据集中变量m的值，再用该决策树分类，统计所有样品被正确分类的次数 (P)。这两个次数的差值(O-P)即为变量m在单棵决策树的分类重要性得分 (CS, classification score)，其在所有分类树中的均值即为该变量的整体重要性得分 (ACS, avraged classification score)。

通过大量数据测试发现，同一个变量不同决策树获得的CS的相关性很低，可以认为是彼此独立的。随后按照常规方式计算所有CS的标准差，ACS/CS获得Z-score值。假设Z-score服从正态分布，根据Z-score估计该变量的重要性程度。
如果变量数目很多，只在第一次用所有变量评估，后续只评估第一次选出的最重要的一部分变量。
筛选关键分类变量的另一个指标 Gini importance
Gini impurity得分是确定决策树每一层级节点和阈值的一个评判标准。每个变量 (m)贡献的Gini impurity的降低程度(GD(m))可以作为该变量重要性的一个评价标准。变量(m)在所有树中的GD(m)之和获得的变量重要性评估与上面通过随机置换数据获得的变量评估结果通常是一致的。
变量互作 Interactions
变量互作定义为，一个决策树通过变量m做了决策后的子节点可以更容易或更不容易通过变量k做决策。如果变量m与变量k相关，按m决策后，再按k决策就不容易分割因为已经被m分割了。
这也是基于两个变量m和k是独立的这一假设。通常通过每个变量作为决策节点时计算出的Gini impurity得分 (g(m)或g(k)，得分越低说明决策分割效果越少)作为评价依据。他们的差值g(m) - g(k)即为变量m和k的互作值，这个值越大，说明基于变量m分割后更有利于基于变量k分割。
相似性 Proximities
每棵决策树构建好后，用所有的数据评估决策树，然后计算每一对样品之间的相似度。如果两个样品分类到相同的分类节点，它们彼此的相似性加一。最终的相似度除以决策树的总数目获得任意一对样品标准化后的的相似度值 (具体见前面的公式)。这些值构成一个n X n(n为总样品数)的矩阵。这个值可后续用于缺失值填充、异常样品鉴定和对数据进行低维可视化。
如果数据集较大，n X n的矩阵可能需要消耗比较多内存。这是可以使用n X t的矩阵来存储 (t是随机森林中决策树的数目)。用户也可以指定每个样品只保留最相似的nrnn个样本，以加快运算速度。
如果有测试数据集，测试集中的样品也可以与训练集中的样品计算两两相似度。
降维展示 Scaling
样品i和j的相似度定义为prox(i,j),这是一个不大于1的正数 (具体见前面公式)。1-prox(i,kj)则是样品i和j的欧式距离。这样就构成一个最大为n X n的距离矩阵。
随后就可以通过MDS方式计算其内积，求解特征值和特征向量。可类比于PCA分析，获得几个新的展示维度。通常绘制第1,2维就可以比较好的展示样品的分布。
原型 Prototypes
Prototypes是一种展示变量与分类关系的方式。对第j类，选择基于相似性获得的K近邻样品中落在j类最多的样品。这k个样品中，计算每个变量的中位数、第一四分位数和第三四分位数。这个中位数就是class j的原型 (prototype), 第一四分位数和第三四分位数则是原型的置信范围。对于第二个原型，按之前的步骤计算，只是不考虑上一步用到的k的样品。输出原型时，如果是连续变量，则用原型值减去第5分位数并除以第95分位数和第5分位数的差值。如果是区域变量，原型就是出现次数最多的值。
训练集中的缺失值填充 Missing value replacement for the training set
随机森林有两种方式填充缺失值。
第一种方式速度快。如果某个样品属于class j，其变量m值缺失，如果变量m的值是数值型，则用class j中所有样品的变量m的值的中位数作为填充值；如果变量m的值是分类型，则用class j中所有样品出现次数最多的变量m的值作为填充值；
第二种方式需要更多计算量但可以给出更好的结果，即便是存在大量缺失数据时。它只通过训练集填充缺失值。首先对缺失值做一个粗略的、非精确的填充。然后计算随机森林和样本相似性。
定义样品i的变量m的值为v(m,i)，如果变量m为连续性数值变量,则其填充值为其它样品中变量m的值与该样品与样品i的相似性的乘积的均值 (n1是变量m不为缺失值的样品)。如果变量m为分类型变量, 则替换其为所有样品最频繁出现的值（计算频率时需要用根据每个样品与样品i的相似性进行加权）。

随后使用填充后的数据集迭代构建随机森林模型，计算新的填充值并且继续迭代，通常4-6次迭代即可。
替换测试集中的缺失值 Missing value replacement for the test set
取决于测试集是否自带样品标签（分类属性）有两种方式可以用于缺失值替换。
如果测试集有标签，训练集中计算出的填充值直接用于测试集填充。
如果测试集没有标签，则测试集中的每个样品都重复n_class次 (n_class为总的分类数)。第一次重复的样品设置其标签为class 1，并用class 1的对应值填充。第二次重复的样品设置其标签为class 2，并用class 2的对应值填充。
这个重复后的测试集用构建好的随机森林模型进行分类。在某个样品的所有重复中，获得最多分类的class则是该样品的标签，缺失值也根据对应class填充。
标签有误的样品 Mislabeled cases
训练集通常是人为判断设定的标签。在一些领域，误标记会常出现。很多误标记的样本可以通过异常值检测的方式鉴定出。
异常值 Outliers
异常样本定义为需要从主数据集中移除的样本。从数据上来说，异常样品就是与其它所有样品相似性 (proximity)都很低的样品。通常为了缩小计算量，异常样品是从每个分类内部计算鉴定的。class j的一个异常样本就是在class j中某一个或多个与其它样本相似性很低的样本。
class j中的样品 n与class j中其它样品(k)的平均相似性定义为

样品n是否为异常样品的度量方式为

The raw outlier measure for case n is defined as
如果平均相似度较低，这个度量值就会很高。计算这一组数据 (class j中每个样本 （J1, J2, Jn）与其它样本的平均相似度)的中位数和绝对偏差。每个平均相似度值减去中位数除以绝对偏差即获得最终的异常值度量标准。

无监督聚类 Unsupervised learning
无监督聚类问题中，每个样品有一些度量值但都没有分类标签（分类问题）或响应变量（回归问题）。这类问题没有优化的标准，通常只能获得模糊的结论。最常见的应用是对数据进行聚类，能聚成几类，每一类是否意义明确。
在随机森林中，所有原始数据都视为来源于class 1, 然后构建一个相同大小的合成数据集都视为来源于class 2。第二个合成数据集通过对原始数据进行单变量 (univariate distribution)随机采样构建。合成数据集每个样品构建方式举例如下：
样品第一个变量的值从该变量在n个样品中的值随机抽取一个。
样品第二个变量的值从该变量在n个样品中的值随机抽取一个。
以此类推
因此，Class 2数据有着独立的随机变量数据分布，并且每个变量的数据分布与原始矩阵的对应变量一致。Class 2数据打乱了原始数据的依赖结构。现在全体数据就有了两个分类，可以应用随机森林算法构建模型。
如果这两类数据的oob误分类率为40%或更高，说明随机森林the x -variables look too much like independent variables to random forests。变量的依赖在分组上贡献不大。如果误分类率低，则说明变量依赖发挥了重要作用。
把无标签数据集改造为包含两个分类的数据集给我们一些好处。缺失值可以有效填充。可以鉴定出异常样品。可以评估变量的重要性。可以做降维分析（如果原始数据有标签，非监督的降维 (`scaling)通常能保留下原始分类的结构）。最大的一个好处是使得聚类成为了可能。
平衡预测错误  Balancing prediction error
在一些数据集，不同分类的预测错误是很不均衡的。一些分类预测错误率低，另一些类预测错误率高。这通常发生在每个类的样品数目差别较大时。随机森林试图最小化总错误率，保持大的类有较低的分类错误率，较小的类有较高的分类错误率。例如，在药物发现时，一个给定的分子会分类为有活性或无活性。通常分类为有活性的的概率为1/10或1/100。这时分类为有活性组的错误率就会很高。
用户可以通过输出每种分类的错误率作为预测不平衡性的评估。为了说明这个问题，我们采用一个有20个变量的合成数据集。class 1的数据符合一个球形的高斯分布，class 2的数据符合另一个球形高斯分布。训练集包含1000个class 1样品和50个class 2样品。测试集包括5000个class 1样品和250个class 2样品。
包含500棵树的随机森林输出的错误率为500 3.7 0.0 78.4。总体错误率较低(3.7%),但class 2的错误率高，为(78.4%)。可以通过对每个class设置不同的权重来平衡错误率。一个class的权重越高，它的分类错误率降低的就越多。一个常规的设置权重的方式是与class中样品数成反比。设置训练集中class 1的权重为1,class 2的权重为20，再次构建模型，输出为500 12.1 12.7 0.0。class 2的权重为20有点太高了，降低为10，获得结果如下500 4.3 4.2 5.2。
这样两个分类的错误率差不多平衡了。如果要求两个分类错误率相等，则还可以再微调class 2的权重。
需要注意的是，在平衡不同分类错误率时，总的错误率升高了。这是正常的。
检测新样品 Detecting novelties
对于测试集异常样品的检测可用来寻找新的不能分类于之前鉴定好的类中的样品。
以下面的卫星图像数据集做例子。训练集有4435个样品，测试集有2000个样品，36个变量，6个分组。
在测试集中，等间距的选取了5个样品。被选中样品的每个变量的值用从训练集中同一个变量的值随机选取一个替换。采用参数noutlier=2; nprox=1作为运行参数，输出下图：

这展示了采用一个固定的训练集，可以检测测试集中的新样品。训练集构建的树可以存储起来用于检测后续的数据集。这个检测新样品的方法目前还处于试验阶段，还不能区分DNA检测中的新样本。
接下来我们就选几套数据集进行实战操作，边操作边理解概念了

微生物组学大数据：如何挖掘与利用？
宏基因组今天

编者按：
随着二代测序技术的成熟，微生物组领域蓬勃发展，并产生了大量数据，近年来研究所涉及的样本量和测序数据量更是快速增加，那么面对如此庞大的数据我们应该如何处理呢？可以利用这些数据做什么呢？
今天，我们特别共同关注微生物组领域中的数据科学，并对微生物组初创公司如何利用微生物组数据进行简要总结。希望本文能够为相关人士和诸位读者带来一些启发与帮助。
微生物组研究正在产生大量数据
并不是每一个人都认为人类基因组计划是个好主意。早在 20 世纪 80 年代末和 90 年代初，当这个计划仍处于筹划阶段时，一些著名的科学家认为，对整个人类基因组进行测序，是一件费力而不讨好的事情。
“我认为，这些信息将有不可估量的效用，但这一点并不那么显而易见。”麻省理工学院的生物学家 Robert Weinberg 如是写道1。
批评人士担心，该计划将会抽走原本属于各个独立实验室的宝贵经费，然后投入到一个大型的政府计划中，而这个计划可能不会产生很多重要且富有意义的成果。
然而，三十年后的今天，人类基因组计划不仅回了本2，而且几乎彻底改变了生物医学研究领域3，并为今天的生物技术产业奠定了基础。
该计划影响科学和技术发展的主要方式之一，是提供免费可用的参考数据集，研究人员可以利用这些数据集，开发新的计算工具和测序技术。因此，生物医学研究领域，现已成为最大的数据科学领域之一。
而肠道微生物组也有着类似的发展轨迹。肠道微生物组是指栖息在肠道中的亿万微生物，这些微生物对我们的健康而言至关重要，被看作是我们的“虚拟器官”4。研究发现，我们的微生物组对机体的新陈代谢、疾病的易感性乃至药物反应，都会产生微妙但普遍的影响。
然而，直到最近，微生物组中的大多数微生物物种依然是“不可见的”，因为它们无法在培养皿上生长。为了追踪这些微生物，科学家们依靠对从粪便样本中收集的 DNA 进行测序。
与人类基因组计划一样，研究人员正试图通过建立大型参考数据集，来促进微生物组的研究，这些参考数据集，是新的技术和数据分析工具的基础。
在微生物组研究中，数据分析的关键挑战之一，是将粪便样本中提取的 DNA 序列片段，组装成完整的基因组。这份工作就像是，利用垃圾箱里的书页碎片，重新将成千上万的书页拼装起来。因此，如果你没有原始书本作为参考，这是很难做到的。
但是，有了一台像样的电脑和一份文本的原版拷贝，这项工作就将变得十分容易。
这就是为什么大量的研究团队，最近醉心于收集数十万份人类肠道微生物组样本以建构参考基因组序列5,6，比如 2019 年 7 月刚发布的一组新数据7。这项研究汇编了数千种微生物的基因组，以及超过 1.7 亿条非人类基因序列。在人体内，细菌基因的数量，大约是人类基因数量的 1 万倍。

微生物组研究是一个数据科学问题
这些庞大的数据集对计算生物学家提出了新的挑战和机遇。这些计算生物学家为了人类健康，试图理解，甚至操纵人体微生物组。
其中一个关键的挑战是，细菌基因组本身并没有那么有用。它们需要与其他数据一起进行分析。
微生物组对我们很重要，因为它会随着年龄、饮食、药物甚至癌症等疾病的变化而变化。不仅如此，我们的肠道微生物还会与我们一起代谢食物，操纵我们的免疫系统，并与人体本身共同构成广泛的代谢网络。而为了做到这一切，肠道微生物表达了大量的基因。
为了弄清楚微生物组，研究人员需要追踪这些细菌基因的表达，是如何随着时间的推移而变化的，以及为什么在不同患者之间出现差异。
这通常涉及到，将微生物组信息与患者血液检测数据、表观遗传学数据、临床结果，甚至组织学图像联系起来。研究者正在建立整合这些不同数据类型的平台资源。
ColPortal8 是一个专注于结肠直肠癌样本的平台，其将不同的数据集以一种利于数据分析的形式整合在一起，使得数据分析师更容易回答医学问题，而不是花费大量力气将数据整合在一起。
另一个挑战是将最先进的分析方法，如机器学习，应用于成分混杂的大型微生物组数据集。
机器学习算法可以很好地根据复杂数据中存在的微妙模式，对样本进行分类。例如，微生物组研究的目标之一，是根据患者微生物组组成的特征变化，来预测早期癌症9。
如果这能成功，我们在 50 岁以后都应该做的常规结肠镜检查，可以被一种侵入性更小的筛查方法所取代，一种只需要粪便样本的筛查方法。
然而，机器学习手段一般不适合非专业人员。遗憾的是大多数微生物组学家，不是机器学习的专家，他们也没有理由成为这方面的专家。为了确保高质量的机器学习技术在这个问题上发挥作用，一些项目专注于为微生物组数据构建机器学习工具。
比如，欧盟资助的 ML4 Microbiome 项目10正在收集数据集，建立数据标准，并构建可广泛应用于研究社区的软件。而由明尼苏达大学的 Dan Knights 运营的“Microbiome Learning Repo”11，则是一个公开的机器学习工具库。
在不久之前，微生物组数据科学家还需要从零开始构建这样的工具。如今，他们却可以把工作重心放在数据分析上了。

初创公司如何利用微生物组数据？
这些新的微生物组平台资源，在实验室之外，又会产生怎样的影响呢？微生物组研究不仅仅是学术团队的课题；目前已有十多家生物科技初创公司，在这一领域开展工作，许多公司成立还不到五年。
初创公司采用的微生物组技术可以分为几种常见的手段，以下的每一种手段都依赖于微生物组 DNA 测序和数据分析：
微生物组移植：在治疗慢性胃肠道感染方面，利用健康捐赠者的粪便微生物组进行粪菌移植，取得了一定的成功。Rebiotix12 和 MaaT Pharma13 等公司，正在对细菌感染和溃疡性结肠炎等疾病的微生物组疗法，进行临床试验。
成功的关键之一，将是确切地了解一个“好的”微生物组是什么样子的——这只有通过分析微生物组测序数据，才能弄清楚。
“将细菌作为药物”：另一种手段是专注于特定种类的肠道微生物的代谢功能，而不是重现整个健康微生物组。Seres Therapeutics 公司14希望改善正在接受免疫疗法的转移性黑色素瘤（一种致死率很高的癌症）患者的治疗情况。
由于微生物组与免疫系统相互作用，Seres Therapeutics 公司开发了一种针对免疫系统的细菌混合物，目的是帮助这些患者对治疗产生更好的应答。想要了解细菌是如何控制人体免疫系统的，关键是要知道它们表达什么基因，并模拟这些基因是如何协同工作的。
微生物组工程：一种比较有野心的操纵微生物组的方法，是对其进行基因工程。法国公司 Eligo Biosciences15 正在利用一种来源于噬菌体的技术——CRISPR 基因编辑技术。具体地，该公司通过对肠道中的细菌进行基因编辑，让它们表达有益基因，或杀死传染性细菌。这项技术可能不会很快出现在临床上，但它依然可以从新的大型肠道微生物基因数据库中受益。
Eligo Biosciences 公司的技术，还可以针对感染性细菌中的抗生素耐药性基因——这种方法依赖于从数亿细菌基因中，识别出这些基因。
微生物组诊断：微生物组数据最有前景的应用之一，可能是在诊断上——尤其是对于癌症的诊断。肿瘤会产生很多不同寻常的代谢副产物，从而改变微生物组。
像 Metabiomics16 这样的公司，就是基于这样一种想法，即微生物组的变化，可以被用于早期癌症的发现——早在症状出现之前。这种方法要想成功，就需要依靠良好的模型，以从微生物组每天或每周的波动中，梳理出任何有风险的迹象。
微生物组数据非常复杂，即使以当今数据密集的生物医学科学的标准来看，也是如此。但就像大多数数据科学领域一样，研究的步伐正在加快，因为微生物组研究人员建立了新的工具和数据库，其他人可以使用这些工具和数据库来回答新的问题。
在这种情况下，在这个领域里，研究工作逐渐从实验室工作台上转移到了键盘上的数据分析。
参考资料：
（滑动下方文字查看）
1. https://pubmed.ncbi.nlm.nih.gov/3223969/
2.https://www.genome.gov/27544383/calculating-the-economic-impact-of-the-human-genome-project
3. https://genomemedicine.biomedcentral.com/articles/10.1186/gm483
4. https://pubmed.ncbi.nlm.nih.gov/23833275/
5. https://pubmed.ncbi.nlm.nih.gov/30867587/
6. https://pubmed.ncbi.nlm.nih.gov/30661755/
7. https://pubmed.ncbi.nlm.nih.gov/32690973/
8. https://colportal.imib.es/colportal/help.jsf
9. https://pubmed.ncbi.nlm.nih.gov/32647386/
10. https://www.ml4microbiome.eu/ml4-microbiome-overview/
11. https://bio.tools/ML_Repo
12. https://www.rebiotix.com/about-rebiotix/
13. https://www.maatpharma.com/technology/#gutprint
14. https://www.serestherapeutics.com/our-programs/
15. https://eligo.bio/
16. http://metabiomics.com/preventing-cancer/
原文网址：
https://builtin.com/data-science/microbiome-research-data-science

AI可以如何抗击新冠？WHO论文告诉你这三大场景大有可为
机器之心2020-03-24
随着新冠疫情的持续发展，全世界的研究者都在致力于疫情的缓解，其研究重点包括：追踪病毒传播、促进病毒检测、开发疫苗、寻找新的治疗方法、了解疫情的社会经济影响等。在这篇综述文章中，来自杜伦大学、蒙特利尔大学、WHO等机构的研究者探讨了 AI 相关技术在疫情中发挥的作用，总结出了 AI 在医疗、分子、社会三个层面的应用。

具体来说，分子层面包括药物挖掘等相关研究；医疗层面包括个体病人的诊断和治疗；社会层面包括流行病学和信息医学研究等。此外，论文还综述了当前可用的开源数据集和其他资源。
这篇综述的目的并非评估文中所述技术的重要性，也不做推荐之用，而是向读者展示当前 AI 技术在抗击疫情方面的应用范围。

论文链接：https://drive.google.com/file/d/1vDcb6HeS-hufNgqH0dDhIEGjuJpnnkzT/view
医疗层面：从诊断到结果预测
迄今为止，AI 在应对 COVID-19 上的应用大多集中在医学成像的诊断上。在近期多篇文献中，除了使用患者医学数据预测疾病进展的方法、用于病情监测的无创检测方法，还有 AI 协助计算机进行 CT 诊断的案例。
医学影像诊断
RT-PCR 测试是诊断 COVID-19 的关键方法，但这种方法仍存在样本采集、分析时间等局限性，所以人们越来越关注使用医学成像技术进行 COVID-19 诊断。COVID-19 具有特殊的放射学特征和图像模式，这些特征均可通过 CT 扫描的方式观察到，但对于放射学科的医务人员来说，识别这些图像也颇为费时，因此在 CT 扫描诊断过程中使用机器学习方法是一种理想的选择。
多项研究已经将诊断定为二元分类问题，即「健康」与「新冠病毒阳性」。
Wang 等人使用改进过的 Inception 神经网络架构，对医生确定过的区域进行训练，从而对健康患者和新冠患者进行二元分类。基于 259 位患者的约 1000 个图像切片的数据集，研究者训练出了能够识别疑似 COVID- 19 的模型，然后将结果提供给医生作进一步诊断。
Chen 等人也发现，在经由专业医生标记过的 6000 多张 CT 图像切片数据上训练 UNet++神经网络，其性能可接近专业医生的诊断水平。这一模型之后在武汉大学人民医院进行部署，用以协助医生加快对病例的分析及诊断，目前这一模型也已开源。
其他机器学习方法将诊断归结为 3 种分类任务：健康、COVID-19 患者及其他类型肺炎患者。
在 Xu 和 Song 的研究中，经典的 ResNet 架构可用于特征提取。Xu 等人添加了几个用于分类的全连接层，Song 等人则添加了特征金字塔网络（Feature Pyramid Network）和注意力模块，使网络更加复杂，但在图像细粒度方面表现更好。
这两项研究均表明，即使在诊断过程中可能存在多个疑似结果（包括非 COVID-19 的肺炎类型），这种方法也能够准确地区分开来。
此外，还有一些研究采用了融合型方法：将现有的软件与特定机器学习方法相结合，以实现更高的准确性。
在 Gozes 等人的研究中，商业医学影像程序可用来进行原始图像的处理，然后与一个 ML Pipeline 结合使用。这种两步式方法包含在肺异常医学影像数据上训练过的 U-Net 架构，以及在 ImagetNet 上训练过的 Resnet-50，其中图像分类已微调为「Coronavirus」和「健康」。
Shan 等人的研究中，采用了「human- in-the-loop」的方法减少机器学习架构所需的标记时间。研究者使用少量人工标记的数据来训练基于 V-Net 架构的初始模型。
该模型对新的 CT 扫描影像进行分割，之后经由专业医生校正，在迭代过程中不断反馈到模型中。这种方法使得基于深度学习技术的系统可用于自动分割和感染区域统计，以及评估患者 COVID-19 病情的严重程度。
研究表明，该模型的性能逐步提升，经过 200 个带注释的示例数据训练之后，将新图像分析所需的人工时间从开始的 30 分钟以上减少到 5 分钟以上。这个方法将机器学习的优势与人类的专业知识相结合，是一个前途广阔的研究方向。
疾病跟踪的非侵入式测量
另外一种不需要特殊医疗成像设备的原创性方法是，通过 Kinect 深度相机来识别病人的呼吸模式。
该方法基于最近对 COVID-19 患者症状的临床发现，即 COVID-19 患者的呼吸模式不同于其他流感或普通感冒，其较明显地表现出呼吸急促症状。
基于以上临床信息，研究者开发出一种具有注意力机制的双向 GRU 神经网络，并使用它来识别反常的呼吸模式。
研究者使用 20 名参与者的真实数据以及基于真实记录产生的大量仿真数据来训练该模型。虽然这些反常的呼吸模式并不一定与真实的 COVID-19 诊断相关，但对这些呼吸急促症状的预测可作为首要诊断特征，为大范围监控潜在患者提供帮助。
另外一些方案是使用手机来检测 COVID-19，有使用嵌入式传感器来识别 COVID-19 症状的，也有通过回答在手机调查问卷中的一些关键问题来排查高风险病人的。虽然以上方法都是在移动技术方面的重要尝试，但目前的研究并不足以评估这些方法的可行性与性能表现。
患者预测
Yan 等人提出一种基于患者临床数据与血样检测中特征的预测方法，该方法能够帮助临床医生尽早地识别出高风险患者，希望以此提高患者的预后以及减少重症患者的死亡率。
与此研究相类似的方法有，基于 XGBoost 算法的预测模型，其用于预测死亡风险和识别能够在医院中进行检测的关键测量特征。基于 375 名患者的数据，作者从 300 多个输入特征中筛选出三个关键临床指标，为预测患者死亡率提供了一种临床启发式的依据。该方法的一大优势是其具有良好的可解释性，因为筛选出的这三个指标与 COVID-19 病理学进展中的几个最重要因素相关，即细胞损伤、细胞免疫与发炎。
一个与此互补的研究是，在半自动标记的 CT 影像上训练一个 U-Net 变种，该方法旨在预测 COVID-19 患者是否需要长时间住院观察。这意味着一旦完成初期诊断，我们仍然可使用机器学习的方法来预测患者病情的严重程度以及是否需要长期住院。
分子层面：从蛋白质到药物挖掘
蛋白质结构预测
蛋白质具有的 3D 结构由它们的基因序列决定，并且该结构会影响蛋白质的功能与作用。一般而言，蛋白质结构通过 X 光晶体衍射图谱法等实验研究法来确定，但这些方法花费昂贵、耗费时间。
最近，计算模型已经被用来进行蛋白质结构的预测，主要有两种方式：一种是模板建模，它的原理是利用相似蛋白作为模板序列进而预测蛋白质结构；另一种是无模板建模，它主要预测那些无已知相似结构的蛋白质的结构。
2018 年底，谷歌 DeepMind 重磅推出 AlphaFold，它能够利用基因序列预测蛋白质结构。给定一种新的蛋白质，AlphaFold 利用神经网络来预测氨基酸对之间的距离，以及连接它们的化学键之间的角度。根据神经网络预测的两种物理属性，DeepMind 还训练了一个神经网络以预测蛋白质成对残基（residues）之间距离的独立分布，这些概率能组合成估计蛋白质结构准确率的评分。目前，AlphaFold 可以预测与 SARS-Cov-2 相关的 6 种蛋白质的结构，分别为 SARS-Cov-2 膜蛋白、蛋白 3a、Nsp2、Nsp4、Nsp6 和 papain-like 蛋白酶。
改进病毒 DNA 测试
当前，机器学习和新型基因组技术也用来提升 PT-PCR 的测试效果。Metsky 等人利用 CRISPR（一种通过割裂特定基因遗传代码链并利用酶来编辑基因组的工具）来进行检验分析设计，用以检测包括 SARS-CoV-2 在内的 67 种呼吸道病毒。此外，对于那些被预测为敏感性和特异性并且涵盖多种基因组的检测分析，有些 ML 模型可以加速它们的设计。
老药新用
发现当前药物可以用来治疗 COVID-19 的一种方法是生物医学知识图谱。生物医学知识图谱网络可以捕捉蛋白质与药物等不同实体之间的联系，从而可以进一步了解它们彼此之间的关联。
Richardson 等人利用生物医学知识图谱识别出了巴瑞替尼，这是一种通常用于治疗关节炎的药物，但由于它能够抑制 AP2 相关的蛋白激酶 1（AAK1），使得病毒很难进入宿主细胞，所以该药物可能适用于 COVID-19 的治疗。
Ge 等人也提出一种类似方法来构建关联人体蛋白、病毒蛋白和药物的知识图谱，它所使用的数据集捕捉了这些实体之间的关系。这种知识图谱用来预测可能有效的候选药物。作者已经识别出了多聚腺苷酸聚合酶抑制剂 CVL218，目前正处在临床试验阶段。
其他一些研究也利用创建的模型来预测蛋白配体的复合物亲和性，以解决老药新用的难题。Hu 等人使用多任务神经网络对亲和性进行广义预测。作者已经识别出了一系列 SARS-Cov-2 相关的蛋白质，如 RNA 依赖的核糖核酸聚合酶、3C-like 蛋白酶、解旋酶以及包膜蛋白等等，从而借助于 4895 种药物的数据集展开靶向治疗。他们推荐了 10 种可能有效果的药物以及这些药物的靶蛋白和复合物亲和性评分。为了提升模型的可解释性，他们还对每个靶蛋白可能出现结合的精确位置进行预测。
同样地，Beck 等人利用他们提出的 Molecule Transformer-Drug Target Interaction（MT-DTI）复合物亲和性模型，识别出美国食品及药物管理局（FDA）批准抗病毒药物中可能对 6 种冠状病毒蛋白质（分别为 3C-like 蛋白酶、RNA 依赖的核糖核酸聚合酶、解旋酶、3』-to-5』核酸外切酶、endoRNAse 和 2』-O-ribose 甲基转移酶）有效的药物。MT-DTI 模型以 SMILES 数据和氨基酸序列的形式输入串数据，并运用一种借鉴 BERT 算法的文本建模方法。此外，该模型识别的药物可能对上述蛋白具有靶向效果。
最后，Zhang 等人利用密集全连接神经网络，它在 PDBBind 数据集上被训练用于预测复合物亲和性，从而识别 3C-like 蛋白酶的潜在抑制剂。他们利用 SARS 病毒变体创建了靶蛋白同源（模板）模型，并探索现有复合物（如 ChemDiv 和 TargetMol）和三肽的数据集，从而找出对蛋白质具有靶向效果的治疗手段。
药物发现
一些研究者试图寻找新的化合物，用来治疗新冠肺炎。Zhavoronkov et al. (2020a) 等就使用了一个专有管道，寻找类 3C 的水解酶抑制剂。他们的模型使用了三种输入：蛋白质晶体结构、类晶体的例子，以及蛋白质模型本身。对于每个输入类型，研究者拟合了 28 种不同的模型，包括生成自编码器和生成对抗网络。研究者使用强化学习探索潜在的候选药物，其中有一个奖励函数和一些标准——药物相似性、新颖性、和多样性相联系。同时，他们确认识别出的候选化合物和已有的化合物不同，说明它们确实找到了不同的药物。
Tang et al. (2020) 也使用了强化学习来发现药物。研究者整理了 284 种已知的分子——能够抑制 SARS 类病毒。他们将这些蛋白质打碎成 316 个片段，然后使用高级深度 Q-learning 来组合，进行药物设计。这种强化学习的奖励函数有三个评价角度：药物相似性分数、加入的预定义「倾向使用的」片段和出现的已知药效集团（和化合物的功效有关的特定结构）。
结果，有 4922 个结果通过启发式搜索被过滤。最终有排名最前的 47 个化合物在分子模拟中进行评估。研究者会选择最可能有效的化合物，并进行生产和测试。
社会层面：流行病学和信息病学
流行病学
流行病学研究覆盖领域极其广泛，其流行的规模和相关性，以及数据的实时更新等多方面因素导致了研究工作必须进行多种类建模。但此次团队将专注于用机器学习去完成流行病学建模的案例。
鉴于流行病感染速度迅速，所以短期实时预测是作为提供信息的重要来源之一，同时模型必须兼备灵活性，以适应各种不断变化的协议或是程序。
Hu et al. (2020b)†收集了 WHO 以及其他预测参与者于 2020 年 1 月 11 日至 2 月 27 日期间收集的数据，用以开发创建一个新的关于中国国内累积或是新增确诊病例的数据集。这些信息主要用于训练调整后的自动编码器（MAE），以便实时预测新病例，并估计流行病的严重程度以及持续时间。
类似的，Al-qaness et al. (2020) 模型可以使用历史数据并提前十天预测确诊病例的总人数。而作者的模型是基于 neuro-fuzzy inference system (ANFIS) (Jang, 1993)，flower pollination algorithm (FPA) (Yang, 2012) 以及 salp swarm algorithm (SSA)(Mirjalili et al., 2017) 进而最优化模型里的参数。
而 Mizumoto et al. (2020) 通过 ML 的方法利用从钻石公主号游轮上所收集的感染数据来了解无症状病例的发生率。作者利用这些数据通过贝叶斯分析对时间序列进行建模，并使用了 Hamiltonian Monte Carlo (HMC) 以及 No-U-Turn- Sampler (Homan & Gelman, 2014) 进行调整模型参数，从而预估无症状感染者的可能性。尽管在这种封闭式环境中进行分析是非常重要的，但是否值得对外适用于更广泛的人群还有待观察。
信息学
当下社交媒体以及在线平台已成为疫情相关信息的主要传播渠道，而团队更看重的是「信息流行病」，如错误信息的信息或是谣言会越传越广。
Cinelli et al. (2020)†分析了与 COVID- 19 相关社交媒体的内容，作者从 Twitter, Instagram, YouTube, Reddit, 以及 Gab 中收集的 800 万条于 2020 年 1 月 1 日至 2 月 14 日间使用 COVID- 19 关键字的评论或帖子。作者预估了对 COVID- 19 话题的参与度，并横向比较了各平台间话题的发展进度。互动参与度是通过使用累积的贴子数以及 45 天内对帖子的反馈来反应的（如评论，点赞等）。作者采用 phenomenological (Fisman et al., 2013) 以及经典 SIR 模型来表示信息传播或复制的数量。
类似的，Mejova & Kalimeri (2020)† 研究对象是使用带有病毒相关内容的 Facebook 广告，通过使用「冠状病毒」以及「COVID- 19」等关键字去搜索所有广告，其范围覆盖了 34 个国家及地区，并收集了 923 余条结果。大部分位于美国和欧盟，而其中 5% 的广告是具有较强的误导信息。
此外，也有研究者着手于新冠病毒特定新闻内容的整理，并进行了人工和自动的真实性验证和相关性分析。Pandey et al. (2020)†开发了一个评估每日新闻头条和 WHO 建议之间相似度的渠道。如果相似度高于某个阈值，则这篇新文章就会在用户的时间线上出现，同时附有 WHO 的相关建议。其中相似度的阈值由人工审核确定，依据用户反馈不断更新。针对相互矛盾的信息，这种方法可以帮助大众识别准确可信赖的新闻报道，也能促使重要的指导性文章产生更广泛的影像，推动官方的关注与采纳建议。
数据集和其他资源 
利用 AI 抗击新冠病毒离不开各种开源数据集和其他资源，本文重点介绍了当前可用的案例数据、文本数据和生物医学数据。
案例数据
案例数据是指病例的数量和地域分布，这种数据对于追踪 COVID19 疫情的蔓延具有重要作用。这份综述列举的案例数据包括：
WHO COVID-2019 现状报告：https://www.who.int/emergencies/diseases/novel-coronavirus-2019/situation-reports
约翰·霍普金斯 CSSE：https://github.com/CSSEGISandData/COVID-19
nCoV2019 GitHub 项目：https://github.com/beoutbreakprepared/nCoV2019
人道主义社会交换项目：https://data.humdata.org/event/covid-19
专为医学专家开发的项目：https://github.com/CodeForPhilly/chime
意大利封锁后的移动变化数据：https://covid19mm.github.io/in-progress/2020/03/13/first-report-assessment.html
文本数据
NLP 方法在这次疫情研究中发挥了重要作用，利用该技术解读的大量文本信息可以帮助我们了解当前有哪些信息是已知的（如病毒传播、环境稳定性、风险因素等）。这部分的数据包括：
WHO 全球新冠病毒研究文献数据库：https://www.who.int/emergencies/diseases/novel-coronavirus-2019/global-research-on-novel-coronavirus-2019-ncov
当前最大的新冠相关文献开源数据集 CORD-19：https://pages.semanticscholar.org/coronavirus-research
Kaggle 开源数据集挑战赛：https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge
其他开源数据集：https://www.ncbi.nlm.nih.gov/research/coronavirus/；https://covid-19.dimensions.ai/
社交媒体数据集：https://github.com/echen102/COVID-19-TweetIDs；https://www.kaggle.com/smid80/coronavirus-covid19-tweets
生物医学数据
目前，用于诊断的开源数据集和模型还不是很多。上文中提到的一些 CT 扫描方法可以找到，但用于训练系统的方法并没有系统地开源。目前，这一方向的努力包括：
Covid Chest X-Ray Dataset：https://github.com/ieee8023/covid-chestxray-dataset
Data Against Covid-19：https://www.data-against-covid.org/
在基因组测序和药物挖掘方面，有几个数据集是基于之前存在的计划或专门为 COVID-19 从零开始创建的。这方面值得关注的项目包括：
GISAID Initiative：https://www.gisaid.org/epiflu-applications/next-hcov-19-app/
RCSB 蛋白质数据库：http://www.rcsb.org/news?year=2020&article=5e3c4bcba5007a04a313edcc
药物挖掘信息共享网站：https://ghddi-ailab.github.io/Targeting2019-nCoV/
跟踪新冠病毒遗传多样性的 Nextstrain：https://nextstrain.org/
蛋白质折叠游戏 Foldit：https://fold.it/

AI算法或使我们跟上新冠病毒的变异速度
ScienceAIScienceAI1月22日

随着新型冠状病毒的新变种像野火一样在全球范围内不断涌现，研究人员一直在夜以继日地进行试验以确定哪些新毒株可能击败我们的疫苗。
令人欣慰的是，人工智能（AI）可能会对这项工作有所帮助。在周五Science（《科学》）杂志上发表的一篇论文中，麻省理工学院的研究人员描述了一种机器学习算法，该算法可以预测哪些变异毒株对世界上刚研制出的疫苗构成的威胁最大。
该工具可用于快速缩小最有可能逃逸已接种疫苗或先前感染者的免疫系统的变异毒株，使得研究人员可以在实验室中对这些可疑变异株进行检测，并相应地更新疫苗。
论文的合著者、麻省理工学院的生物工程师Bryan Bryson说：「这是疫苗研发的实时伙伴，现在，我们使用模型能做的比在实验室中做要快得多。」
该工具在COVID-19大流行的关键时刻到来了。针对SARS-CoV-2（引起COVID-19的冠状病毒）研制的数百万剂疫苗终于向公众推出，超过百分之三的美国人已经接种了疫苗。
这些疫苗被设计用来训练我们的免疫系统识别冠状病毒的特殊毒株。
最近几周，来自英国、南非、加利福尼亚和其他地区的新病毒变种开始在全球范围内传播。这些难以应付的变种似乎比原来的病毒更具感染力，尽管让人高兴的是其致命性有所下降。多位专家公开表示，目前的疫苗仍能对抗这些新毒株。
「当然，会有更多的突变。」Bryson和他的同事声称他们的算法可以帮助疫苗生产商跟上这场竞赛，「这将减少目前用于监测此类突变实验所耗费的大量人力物力。」
论文的合著者、麻省理工学院的计算机科学家Bonnie Berger说：「这是一个告诉你何时展开调查的工具。随着新毒株的出现，我们可以标记出哪些毒株有逃逸的可能性值得去研究。」
许多基于AI的工具对COVID-19疫苗的早期研发都有所帮助，例如，人工智能帮助研究人员确定了病毒遗传密码的哪些部分最有可能发生突变以及某些突变是如何影响其物理结构的。麻省理工学院的新的机器学习算法通过将AI应用于病毒逃逸而延展了AI的功能。
该小组的模型最初是为机器语言理解而开发的，旨在同时查找语法和语义。将逃逸突变识别为保留病毒感染性但导致病毒看起来与免疫系统不同的突变，类似于保留句子语法但改变其含义的单词更改。利用两者之间的相似性，研究人员对其进行了创造性地修改，以感知病毒遗传密码的变化。

他们称这一过程为受限语义更改搜索（constrained semantic change search, CSCS）。当模型了解冠状病毒基因组时，它同时也开始了解该基因组可能发生何种变化。然后，它会生成一个可疑毒株的简短列表以在实验室中进行测试。
为了测试毒株，研究人员首先将产生一种假病毒，该假病毒携带由计算模型识别的可疑变异。然后，他们让假病毒经受从先前接种或感染过COVID-19的人群中收集的抗体。如果抗体不能中和病毒，则表明该新病毒株能够逃避免疫系统，因此需要更新的疫苗。然后再回到算法中，寻找更多可疑的变异毒株。
计算机和湿性实验室之间「就像形成了一个循环」，Bryson说， 「你只是在来回穿梭并试图及时了解这种流行病。」
研究人员在SARS-CoV-2刺突蛋白的近1000个遗传序列上加上了其他类型冠状病毒（例如引起普通感冒的冠状病毒）的另外3000个刺突蛋白序列，然后对其进行了模型训练。病毒通过刺突进入人体细胞，我们的免疫系统也是通过刺突识别病毒。
数以千计的示例让该模型了解了在冠状病毒中如何控制氨基酸的排序规则。Berger小组的博士生、论文的合著者Brian Hie说：「语言模型的好处是，它们可以直接从大量的培训集中学习规则。这就是为什么我们要在生物环境中使用这种模型的原因，因为我们不知道哪些氨基酸可以并存的规则。」
麻省理工学院的研究人员将一些新的变体输入他们的算法进行试验，发现英国和南非的毒株在逃逸可能性上均得分「很高」。然而，Berger说它们的得分不及实验室中创造出的逃逸突变体那么高。
「预测高分何时可以转化为实际从人类免疫系统逃逸的能力超出了模型的能力，」Hie说。从长远来看，他希望继续使用该模型，以便预测尚未发生的病毒突变。「这是该研究领域的一个大胆创新的目标：针对未来的病毒接种疫苗。」
论文地址：https://science.sciencemag.org/content/371/6526/284
https://spectrum.ieee.org/the-human-os/biomedical/devices/ai-predicts-most-potent-covid-19-mutations
Emily Waltz

Cell Res | 突破！清华大学张强锋团队使用深度学习来预测动态细胞蛋白与RNA的相互作用

与RNA结合蛋白（RBP）的相互作用是RNA功能和细胞调节不可或缺的，并动态反映特定的细胞状况。但是，目前用于预测RBP-RNA相互作用的工具使用RNA序列和/或预测的RNA结构，因此无法捕获其条件依赖性。
2021年2月23日，清华大学张强锋团队在Cell Research 在线发表题为“Predicting dynamic cellular protein–RNA interactions by deep learning using in vivo RNA structures”的研究论文，该研究分析了七种细胞类型的转录组范围内的体内RNA二级结构后，开发了PrismNet，这是一种深度学习工具，它集成了实验的体内RNA结构数据和匹配细胞的RBP结合数据，以准确预测各种细胞条件下的动态RBP结合。 
168个RBP的PrismNet结果支持其实用性，既可用于了解CLIP-seq结果，又可广泛扩展此类相互作用数据以准确分析其他细胞类型。此外，PrismNet采用“注意”策略以计算方式识别确切的RBP结合核苷酸，并且该研究发现了动态RBP结合位点之间的富集，这可以将遗传疾病与失调的RBP结合联系起来。该研究丰富的分析数据和基于深度学习的预测工具提供了对以前无法访问的细胞类型特异性RBP-RNA相互作用层的访问，并且对于理解和治疗人类疾病具有明显的实用性。
另外，2021年2月9日，清华大学张强锋，王健伟及丁强共同通讯在Cell 在线发表题为“In vivo structural characterization of the SARS-CoV-2 RNA genome identifies host proteins vulnerable to repurposed drugs”的研究论文，该研究使用icSHAPE技术，确定了SARS-CoV-2 RNA在感染的人细胞中以及从重新折叠的RNA以及SARS-CoV-2和其他六种冠状病毒的调控性非翻译区的结构态势。该研究验证了计算机预测的几种结构元件，并发现了影响细胞中亚基因组病毒RNA的翻译和丰度的结构特征。结构数据提供了一种深度学习工具，可预测与SARS-CoV-2 RNA结合的42种宿主蛋白。引人注目的是，靶向结构元件的反义寡核苷酸和FDA批准的抑制SARS-CoV-2 RNA结合蛋白的药物大大减少了SARS-CoV-2对人细胞的感染。因此，该研究的发现揭示了COVID-19治疗的多种候选疗法（点击阅读）。

RNA结合蛋白（RBP）在调节细胞RNA的转录，代谢和翻译中起着至关重要的作用。确定不同条件下的RBP结合谱并阐明其详细的调控机制对于理解其功能至关重要。然而，鉴于RBP的数量庞大，约占人类蛋白质组的10％，因此在RBP及其目标之间建立联系是一个巨大的挑战。
为了解决这个问题，已经开发了许多高通量技术来分析和预测RBP结合。诸如通过指数选择（SELEX），RNAcompete和RNA Bind-n-Seq进行配体的系统进化等方法可以表征RBPs在体外的序列偏好，以及随后的诸如RNA免疫沉淀（RIP）和UV交联的方法通过免疫沉淀（CLIP）和测序可以识别体内RBP结合位点。
除了直接测量的方法外，还开发了其他方法来建模和预测RBP结合。传统上，位置权重矩阵已用于描述RBP结合决定因素并从RNA序列预测RBP结合靶标。还开发了整合不同类型信息的机器学习方法，以更准确地表征RBP的结合模式。最近，深度学习方法已成功应用于模型化蛋白质-RNA相互作用并预测RBP结合位点。
尽管这些学习方法成功捕获了初级序列的RBP结合偏好，但由于RNA序列与体内条件无关，因此它们在不同生理状态下的预测准确性受到限制。多年来，已经开发出了几种方法来在其建模中包括RBP目标的RNA结构特征，但是这些结构是基于计算预测而非体内分析。
在这里，该研究通过确定多种细胞类型中转录组范围的RNA二级结构来弥合这一知识鸿沟。然后，该研究通过使用深度神经网络（PrismNet）进行结构信息化建模，在深度判别性神经网络蛋白质-RNA相互作用的构建中整合此实验衍生的结构信息，该模型可以准确地建模和预测体内RBP目标。该研究应用PrismNet来预测基因组变异如何影响RBP结合，尤其是在人类疾病的背景下。
总之，该研究丰富的分析数据和基于深度学习的预测工具提供了对以前无法访问的细胞类型特异性RBP-RNA相互作用层的访问，并且对于理解和治疗人类疾病具有明显的实用性。
参考消息：
https://www.nature.com/articles/s41422-021-00476-y

Microbiome/关键类群的特殊代谢功能维持土壤微生物组稳定性

题目：Specialized metabolic functions of keystone taxa sustain soil microbiome stability
期刊：Microbiome
三年均IF：10.402
上线时间：2021.2

摘要
      土壤微生物组的生物多样性与生态系统稳定性之间的关系还不清楚。在这里 我们研究了土壤微生物组细菌系统发育多样性对功能性状与稳定性的影响。通过将连续稀释的土壤悬液接种到无菌土壤中，产生不同系统发育多样性的群落，并通过检测不同pH下的群落变化来评估微生物群的稳定性。通过DNA测序来检测分类学特征与功能性状。
      我们发现高的细菌系统发育多样性群落更稳定，这表明高微生物多样性对扰动有更高的抗性。通过功能基因共发生网络与机器学习鉴定出特定的代谢功能，一些关键基因如氮代谢、磷酸盐与次磷酸盐代谢。分类注释发现关键功能是由特定的细菌分类群执行的，包括Nitrospira和Gemmatimonas等。
      这项研究为我们理解土壤微生物生物多样性和生态系统稳定性之间的关系提供了新的见解，强调关键类群中的特殊代谢功能对土壤微生物稳定性至关重要的作用。

实验设计与微生物组稳定性
      研究微生物群落稳定性大多通过分析推断（Microbiome/稻田土壤产甲烷菌的共存模式与（甲烷的产生和群落构建）密切相关，ISME/环境胁迫破坏微生物网络Environmental Microbiology/稀有类群维持作物真菌组的稳定性与生态系统功能）与实证证明（互作强度决定群落的生物多样性和稳定性），通过研究分析微生物共发生网络的特性以及计算恢复性与抵抗性指数。本文通过计算平均物种变异程度AVD来衡量微生物群落稳定性强弱。实验设计在上面一篇NC（NC：南农沈其荣、张瑞福团队揭示多样性激发的确定性细菌装配过程限制群落功能）不作赘述。在计算AVD之前将每个样本稀释至11020个reads，AVD计算公式：

      ai表示一个otu的变异程度，xi表示一个样本otu稀释的丰度，_xi表示一个样本otu平均稀释的丰度，σi表示一个样本otu稀释的丰度的偏差。k是一个样本组的样本数，n是每个样本组中OTU的数量。
      下面利用机器学习（Decision tree决策树、boosting 提升方法、bagging装袋算法、Nearest neighbor algorithm近邻算法、Support vector machine 支持向量机、Random forest 随机森林、Artificial neural network人工神经网）检验不同微生物功能类别在构建生态系统功能中的重要性。发现随机森林方法精度高，错误率低（一般群落分析多种机器学习方法中随机森林相对比较优良，例如ISME：南农沈其荣团队基于大数据准确预测土壤的枯萎病发生、Microbiome/稻种的驯化在生态进化上塑造水稻种子的细菌与真菌群落）。文章补充材料附上了各种方法的R代码，没有具体跑：
# 数据准备与处理
Fold=function(Z=10,Test,Dv,seed=1000){
n=nrow(Test)
d=1:n;dd=list()
e=levels(Test[,Dv])
T=length(e);set.seed(seed) # T is the dependent variable
for(i in 1:T){
d0=d[w[,Dv]==e[i]];j=length(d0)
ZT=rep(1:Z,ceiling(j/Z))[1:j]
id=cbind(sample(ZT,length(ZT)),d0);dd[[i]]=id}
mm=list();for(i in 1:Z){u=NULL;
for(j in 1:T)u=c(u,dd[[j]][dd[[j]][,1]==i,2])
mm[[i]]=u} # mm[[i]] is the ith subscript set: i=1, 2, …, Z
return(mm)} # Output 10 subscript sets
Test=read.csv(“Test.csv”)
Dv;Z=10;n=nrow(Test);mm=Fold(Z=10,Test,Dv,seed=7777) #Dv is the position of independent
variable
#1Decision tree
library(rpart)
E=rep(0,Z)
for(i in 1:Z){m=mm[[i]]; # mm[[i]] is the ith subscript set extracted by Fold() function: i=1, 2, …, Z
n1=length(m);a=rpart(avd~.,Test[-m,]) # avd is the independent variable
E[i]=sum(Test[m,Dv]!=predict(a,Test[m,])$avd)/n1}
mean(E)
#2boosting
library(adabag)
E=rep(0,Z)
for(i in 1:Z){m=mm[[i]]; # mm[[i]] is the ith subscript set extracted by Fold() function: i=1, 2, …, Z
n1=length(m)
a=boosting(avd~.,Test[-m,]) # avd is the independent variable
E[i]=sum(as.character(Test[m,Dv])!=predict(a,Test[m,])$avd)/n1}
mean(E)
#3bagging
library(adabag)
E=rep(0,Z)
for(i in 1:Z){m=mm[[i]]; # mm[[i]] is the ith subscript set extracted by Fold() function: i=1, 2, …, Z
n1=length(m)
a=bagging(avd~.,Test[-m,]) # avd is the independent variable
E[i]=sum(as.character(Test[m,Dv])!=predict(a,Test[m,])$avd)/n1}
mean(E)
#4Nearest neighbor algorithm
library(kknn)
E=rep(0,Z)
for(i in 1:Z){m=mm[[i]]; # mm[[i]] is the i th subscript set extracted by Fold() function: i=1, 2, …, Z
n1=length(m)
a=kknn(avd~.,k=6,train=Test[-m,],test=Test[m,]) # avd is the independent variable
E[i]=sum(Test[m,Dv])!=a$avd/n1}
mean(E)
#5Support vector machine
library(kernlab)
E=rep(0,Z)
for(i in 1:Z){m=mm[[i]]; # mm[[i]] is the i th subscript set extracted by Fold() function: i=1, 2, …, Z
n1=length(m)
a=ksvm(avd~.,data=Test[-m,]) # avd is the independent variable
E[i]=sum(Test[m,Dv]!=predict(a,Test[m,]))/n1}
mean(E)
#6Random forest
library(randomForest)
E=rep(0,Z)
for(i in 1:Z){m=mm[[i]]; # mm[[i]] is the i th subscript set extracted by Fold() function: i=1, 2, …, Z
n1=length(m)
a=randomForest(avd~.,data=Test[-m,]) # avd is the independent variable
E[i]=sum(Test[m,Dv]!=predict(a,Test[m,]))/n1}
mean(E)
#7Artificial neural network
library(nnet)
E=rep(0,Z)
for(i in 1:Z){m=mm[[i]]; # mm[[i]] is the i th subscript set extracted by Fold() function: i=1, 2, …, Z
n1=length(m)
mc=setdiff(1:n,m)
a=nnet(avd~.,data=Test,subset=mv,size=7,range=0.1,decay=0.01,maxit=200) # avd is the
independent variable
E(i)=sum(Test[m,Dv]!=predict(a,Test[m,])$avd)/n1}
mean(E)
这里再附上其他两种机器学习方法（参考文献：
Kim, H., Lee, K.K., Jeon, J., Harris, W. A., & Lee, Y. H. (2020). Domestication of Oryzaspecies eco-evolutionarily shapes bacterial and fungal communities in rice seed.Microbiome, 8(1), 20.doi:10.1186/s40168-020-00805-0）
#加载包：
library(phyloseq)
library(dplyr)
library(ggplot2)
# Data preparation
table <- otu_table(phy.clean.log)
table <- t(table)
k <- 129-75
result <- c(rep(0,75),rep(1,k))
table <- cbind(table, result)
dataset <- as.data.frame(table)
# Encoding the target feature as factor
dataset$result = factor(dataset$result, levels = c(0, 1))
dataset$result
# Splitting the dataset into the Training set and Test set
library(caTools)
set.seed(123)
split = sample.split(dataset$result, SplitRatio = 0.66)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
#8朴素贝叶斯Naive bayes
library(e1071)
packageVersion('e1071') ## 鈥?1.7.0鈥?
classifier_nb = naiveBayes(x = training_set[-ncol(dataset)],
                             y = training_set$result)
nb_pred = predict(classifier_nb, newdata = test_set[-ncol(dataset)])
cm = table(test_set[, ncol(dataset)], nb_pred)
print(cm)
#9逻辑回归logistic regression
classifier_lr <-  glm(formula = result ~.,
                        family=binomial,
                        data = training_set)
lr_pred <- predict(classifier_lr, type = 'response', newdata=test_set[-ncol(dataset)])
lr_pred <- ifelse(lr_pred > 0.5, 1, 0)
cm = table(test_set[, ncol(dataset)], lr_pred)
print(cm)
构建功能基因共发生网络代码如下
data <- read.table("gene_abundance.txt",header = TRUE,row.names = 1,sep = "\t")
# read in a gene abundance table
data <- t(data)
library(psych)
occor =corr.test(data,method = "spearman",adjust = " fdr") # calculate spearman’s
correlation
occor.r = occor$r # extract r-value of spearman’s correlation
occor.p = occor$p # extract p-value of spearman’s correlation
occor.r[occor.p>0.01|abs(occor.r)<0.75] = 0 # r-value and p-value filtering
write.table(occor.r,"pvalue.csv",sep="\t",quote=F,col.names=NA) # output results，构建的网络可以直接在Gephi或者在R内实现可视化详见（一文学会网络分析——Co-occurrence网络图在R中的实现，利用Gephi软件绘制网络图，Gephi可视化网络的一个简单示例操作视频） 
群落构建过程指标βNTi代码在补充材料不放上面了。

主要结果

1. 稀释降低了土壤细菌群落的α多样性和稳定性

图1：重新组装的细菌群落的多样性和变异。A重组装群落系统发育多样性。B稀释与βNTI之间的关系。C物种丰富度与AVD之间的关系。D基于随机森林评估功能分类的重要性。
2. 稀释简化了功能基因共发生网络
表1：功能基因共发生网络拓扑性质特性与随机网络

图2：稀释梯度下的功能基因共发生网络
      由表1与图2可知随着稀释微生物功能基因网络拓扑结构参数降低，网络更加松散，一些特殊功能会随着稀释消失。在稀释程度低的群落中特殊代谢功能占主导，而在稀释程度高的群落中广泛代谢功能占主导。
3. 特定功能的关键类群有利于微生物物群稳定性。

图3：两个关键功能的分类注释。A8个属于氮代谢与44个磷酸盐、次磷酸盐代谢在pH与稀释梯度上的相对丰度。B相应属的相对丰度。
      由于“氮代谢”和“磷酸盐和次磷酸盐代谢”的特殊代谢功能是随机森林分类方法和功能基因共发生网络分析中与微生物群稳定性相关的关键节点的识别中最重要的功能，因此进行了分类注释以识别相关的分类群。

结论

      结果表明，稀释显著降低了系统发育多样性，简化了功能基因共发生网络的模块化，降低了土壤微生物群落的稳定性。“氮代谢”和“磷酸盐和次磷酸盐代谢”的特殊代谢功能与土壤微生物群落的稳定性有关。这些关键功能和分类群Nitrospira与Gemmatimonas可能为预测微生物群落的变化提供进一步的见解，并增加操纵微生物群功能的可能性。并利用关键分类群对微生物群落稳定性的可能贡献来改善生态系统服务




综述｜Nature：基于测序数据研究抗菌素抗性的方法和资源
论文信息
论文题目：Sequencing-based methods and resources to study antimicrobial resistance
期刊：Nature Reviews Genetics
IF：35.898
发表时间：2019
摘要
抗菌素耐药性使得细菌对抗生素产生免疫力，这使得疾病的发病率和死亡率以及治疗和放空的经济成本大量增加。
识别和理解抗菌素耐药性对于治疗耐药菌感染以及限制耐药菌扩散的临床和公共卫生实践至关重要。
新一代测序技术正在扩展我们检测和研究抗菌素耐药性的能力，本篇综述包含从传统的抗生素敏感性测试到最新的深度学习方法，详细介绍了抗菌素耐药性的鉴定和表征方法。
本篇文章专注于基于测序的耐药性发现，并讨论了用于抗菌素耐药性研究的工具和数据库。
引言
抗菌素是可以抑制或杀死细菌的小分子，这些小分子通常被用于治疗细菌感染，但是尽管有抗菌素的压力，些细菌仍能生长和存活，这种特性被称为抗菌素抗性。
在临床环境中，与易感细菌引起的感染相比，耐药菌感染会降低可用的治疗方法，同时增加发病率和死亡率。
目前，几乎所有的抗菌素都已经发现了细菌对其产生的抗药性，甚至包括那些用于会威胁生命的、对多种药物具有抗药性的细菌感染中使用的所谓的“最后手段”的抗菌药物。
在美国，每年有200万人会面临对临床一线使用的抗菌素具有耐药性的细菌感染，这些感染要花费200亿美元的医疗费用。这个问题并非只存在于美国，在欧盟，抗菌素耐药性导致超过30,000人死亡，近90万的疾病调整生命年。
实际上，多个国家和全球公共卫生组织抗菌素耐药性归为迫在眉睫的危险，并一致认为，追踪其耐药性和流行对最大限度地减少其对人类健康的威胁至关重要。

抗菌素药敏试验（AST）是测定细菌中抗菌素耐药性的传统方法，这些基于培养的测试确定了在存在抗菌素的情况下细菌的生长状况，AST被广泛用于医院临床微生物学实验室，因为它提供了可操作的表型耐药性数据以指导患者的治疗决策。
尽管基于培养的耐药性测定可以为患者管理和耐药性基因流行病学的研究提供重要的信息，但它在实施和信息内容上存在缺陷，进行AST需要微生物学设施和经过培训的临床微生物学人员以确保准确性，此外，AST仅对可培养细菌有效，这使得该方法无法研究那些不可培养的细菌占很大比例的各种复杂微生物群落中耐药性的出现和传播。
细菌的耐药性通常是由编码基因确定的，抗菌素药耐药性可通过多种机制获得，包括现有基因的过表达或复制、点突变以及通过水平基因转移（HGT）获得全新基因。
新一代测序技术和计算方法的进展促进了基因组和宏基因组中抗菌素耐药基因的快速鉴定和表征，这些发展中的技术和方法补充了用于临床和监测应用的传统的基于培养的方法，并为快速、准确地确定可培养和不可培养细菌的耐药性提供了机会。
对人类、动物和环境样品的大规模比较研究为抗菌素耐药性基因的全球分布、多药物抗性细菌的传播、耐药性交换网络以及以及不同的行为和系统发育如何影响全球抗菌素耐药性的演变提供了新的见解。
使用测序数据了解和调查耐药性的遗传决定因素是一种全新的挑战，我们可以通过改进计算算法以组织基因组数据并预测抗菌素耐药性或者通过发展非原位测序的技术来解决这些问题。
在本篇综述中，作者讨论了研究耐药性的各种方法的优缺点，以及用于基因组和宏基因组样品中耐药性基因鉴定的计算策略和资源，作者还描述了解决耐药性检测方法中缺点的最新进展，并介绍了需要重点关注的研究领域。
Box1：基于培养的药敏测试
细菌培养作为临床微生物学的组成部分具有悠久的历史，研究人员在琼脂平板或液体肉汤中培养细菌，以探测细菌表型并发现新的细菌功能，医院临床微生物学实验室可以使用从这些测定中获得的数据来指导临床治疗决策。
目前的技术
对于表型测试，通过非选择性或选择性琼脂平板从患者或环境样品中分离细菌，然后，将分离的细菌直接使用抗菌素刺激，以测试其耐药性。
在基于液体培养基的AST中，会通过不断降低抗菌素的使用浓度以找到细菌能够生长的最大浓度，抑制细菌生长的最低抗菌素浓度称为最低抑制浓度。
固体培养基技术使用Kirby-Bauer圆盘扩散法或梯度扩散条来测量细菌生长距离抗菌源的程度，抗菌圆盘或条带周围的区域称为抑制区。
临床和实验室标准协会（CLSI）或欧洲AST委员会（EUCAST）会基于测定的最小抑菌浓度或抑菌圈区域发布的耐药性确定的标准。
目前，不依赖测序的耐药性确定方法同样也有新的进展，其中包括基质辅助激光解吸/电离飞行时间质谱（MALDI–TOF-MS）、荧光原位杂交（FISH）和基于微流控的技术，这些技术可将AST的测试时间降低至30分钟，这些快速的测试技术对于生长缓慢的微生物特别有价值，例如结核分枝杆菌，传统的AST方法可能需要几个星期才能得到测试结果。
挑战和不足
尽管AST可用于提供表型耐药性结果，但它的通量很低，对于每个样品，临床微生物学家都需要培养细菌并建立和读取AST结果。
这存在一定的局限性，因为并非所有的医疗保健中心都对人员进行了临床微生物学相关的培训，此外对于某些细菌（例如结核分枝杆菌），目前在一些资源贫乏地区的实验室诊断技术灵敏度依然较低。
同时，目前发布的药敏标准还不能完全覆盖所有抗菌素和引起感染的可培养细菌的所有组合，不同国家之间的标准也不统一。
此外，在多种细菌可能导致疾病感染的情况下，基于培养的技术可能无法得到准确的结果，来自患者样本的一些宏基因组学研究表明，基于培养技术检测到的细菌可能与疾病症状无关。
最后，与全基因组测序相比，这些表型方法在检测耐药性决定因素方面具有较低的分辨率，并且有关耐药性基因流行病学的信息相对较少。
创新
自动化技术的快速发展和应用产生了多个基于表型分析的高效分析系统，包括VITEK、ADAGIO、Pheno在内的几种系统已经进入临床领域。
自动化平台具有几个关键优势，包括连续监控系统、用于测量药敏结果的灵敏光学仪器和标准化的内部时钟，这些优势可以加快获取培养细菌的药敏结果。它们还可以提高不同地区药敏结果的一致性，并减轻临床微生物学家的工作负担，不幸的是，这些自动化平台的建立成本过高。
基于测序的耐药性识别
测序技术的发展提高了细菌序列数据的可用性，并且不断降低的成本使测序成为了一种可行的抗菌素耐药性监测工具，近年来，已经发布了几种方法和工具，用于从全基因组测序和宏基因组测序数据中检测抗菌素耐药性的遗传决定因素。

有效组织测序数据是抗菌素耐药性基因分析之前的重要预处理步骤，由Illumina等测序技术产生的短读段可以使用基于拼接的方法进行处理，从而先将测序读段组装成连续的片段（contig），然后通过与自定义或公共参考数据库进行比较进行注释，或者使用基于read的方法，通过将读数直接映射至参考数据库来预测耐药性决定因素。

基于拼接的方法
从短读长数据进行基于全基因组测序的细菌基因组的从头组装，通常使用基于图论（De Bruijn graph，DBG）的工具，例如SPAdes、Velvet、ABySS和SOAPdenovo。
在这种方法中，测序读段被分为长度为k（其中k小于读取长度）的较短的重叠子序列（称为k-mers），并用于形成网络图，然后，拼接软件通过遍历每个网络的边找到一条最佳路径，从而重建基因组序列。
尽管DBG方法在处理大量测序数据方面计算效率很高，但是它受到测序过程中引入错误的极大影响，测序数据中的错误会导致假k-mers形成拼接碎片（重复序列容易引起此方法的问题）。一些拼接工具（例如SPAdes和Velvet）在找到最佳路径之前，会通过试探法消除这些错误。
组装宏基因组数据比单个细菌基因组的组装更为复杂，这是因为算法需要考虑具有未知系统遗传关系的不同生物体的未知丰度。在单基因组组装中，软件可以使用整个基因组的统一测序覆盖率来纠正测序错误并鉴定重复序列和质粒。但是宏基因组数据中不同生物的覆盖率不均匀，因此很难识别重复序列。不同物种中较长的一致序列是的分配短的测序读段到确定的细菌更加困难。
因此，为单基因组组装开发的算法不能直接应用于组装宏基因组，目前已经开发了几种专门用于宏基因组的拼接组成程序来克服这些困难，主要的方法是对不均匀的测序深度进行DBG的分区或优化，包括IDBA-UD、MEGAHIT、MetaSPAdes和MetaVelvet。
但是，目前没有哪个组装程序能脱颖而出，成为可以准确地重建已知基因组并捕获真实数据集中大多数分类多样性的最佳组装程序。生物学因素（例如样品来源和微生物群落结构）和技术因素（例如文库制备方法、测序深度和测序平台选择）都影响组装程序生成准确且较大的重叠群的能力。
因此，建议在一个样本子集上应用多个组装程序，以确定对于给定数据集的最佳组装结果。
组装完成之后，通过预测重叠群上的蛋白质编码区，然后使用基于相似性的搜索工具（例如BLAST、USEARCH或DIAMOND）将其与抗菌素耐药性参考数据库进行比较，为基因组或宏基因组重叠体标注耐药性遗传决定因子。
尽管测序数据与参考序列之间的成对比对是从重叠群中表征菌群的最常用方法，但数据库中天然的对人类相关生物的倾向性会影响输出的结果，因此选择适当的数据库进行拼接重叠群于参考序列的比对是非常重要的。
如果有足够的覆盖率，基于组装的方法可以构建完整的基因组或具有蛋白质编码基因、调控序列信息和完整的周围基因组背景的大型重叠群，这可以用于研究耐药性单元涉及的相关基因和生物学途径。
宏基因组数据的拼接和注释可以识别与参考数据库中已知序列差异更大且缺乏同源性的耐药基因， 然而，从头组装和注释的过程在计算上是昂贵、费时的，并且与基于参考数据库的组装或基于read的比对方法相比，需要更高的基因组覆盖率，这对于所有样品都是一个不小的困难，特别是那些微生物多样性高、分类组成不均的宏基因组样品。
基于read的方法 SRST2 & KmerResistance，GROOT自带抗性基因集
可以不经过基因组组装直接检测样本中的抗菌素耐药基因，主要的方法是通过使用成对比对工具（例如Bowtie2或BWA）将read于参考数据库比对，或者将read拆分为k-mer之后映射到参考数据库上。
SRST2是一种被广泛应用的工具，其利用Bowtie2将reads与自定义参考数据库进行比对，以预测样品中的抗菌素耐药基因。KmerResistance将read拆分为k-mers，对其进行参考数据库映射，以预测耐药基因和相关物种。
即使在存在污染物的情况下，例如，由于实验室或宿主污染而在原始读数中产生背景噪音，以及在没有足够的reads可用于从头组装的样品中，两种方法都可以识别抗菌素耐药基因，但这两种方法都无法预测由单核苷酸多态性导致的抗药性。
与之相比，ARIBA使用一种混合方法，数据库中的参考序列首先使用CD-HIT进行聚类形成独立的类群，之后对同一类群的参考序列进行独立组装，将得到的重叠群与最接近的参考序列进行比较以鉴定等位基因变体。此外，ARIBA还提供有关基因是否完整的信息，并报告序列变异及其潜在影响（例如错义、无义或移码突变以及小的插入和缺失）。
对参考序列进行聚类，并使用来自聚类的代表性序列与测序reads进行比对(metaphlan)，可大大提高比对的准确定，但使用单个线性代表性基因序列掩盖了聚类内基因的亚型和亚家族之间微妙而重要的变异。
为了解决这种信息丢失的问题，GROOT作为一个新发布的宏基因组抗性组识别工具，构建了参考基因集的变异图谱，并将read与其比对，变异图谱是双向非循环序列图，表示一个种群内的的总体序列变异。reads与变异图谱的比对可以有效消除参考偏差，并有助于准确标记抗菌素耐药基因。
在将序列与变异图谱进行比对之前，通过Burrows-Wheeler变换或哈希图建立索引，可以显着提高大规模测序reads与变异图谱的的映射率。
基于read的方法通常快速且对计算的要求较低，因为它绕过了从头组装、蛋白质编码基因预测和与公共数据库的成对比对的过程。
由于这个原因，近年来，基于read的方法获得了广泛的关注，尤其是在临床诊断中，基于实时测序的耐药性预测至关重要。
选择正确的方法
目前，关于哪种序列分析方法更好尚无共识，分析的选择主要取决于测序的类型以及计算资源的可用性和研究目的。
两种方法都需要权衡，因为与直接基于reads的分析相比，基于拼接的分析会丢失一些信息，但可以识别蛋白质编码基因并用于上游和下游调节元件的研究，而直接基于reads的分析缺乏基因的位置信息。新的测序技术，例如长读测序和源自染色体构象捕获的方法，正在通过提高基因组组装的保真度来减少拼接过程中的信息丢失。
基于reads的方法可随着不断增加的测序数据和抗菌素耐药基因参考数据而不断的完善，更重要的是，它们能够从复杂群落中存在的低丰度生物体中鉴定出抗菌素耐药基因，由于组装不全或组装不良，基于拼接的方法可能会遗漏这些基因。
但是，将reads直接映射到大数据集可能会增加预测的假阳性，因为源自蛋白质编码序列的reads可能会由于局部序列同源性而虚假地与参考基因匹配，因此，参考数据库是否全面并且包含参考基因的所有变体非常重要。
当从大型和复杂的群落（例如土壤和海洋）中鉴定抗菌素耐药基因时，数据库的选择尤为重要，因为不完整的数据库可能会漏掉未被充分研究的，特征较少的环境群落中存在的抗菌素耐药基因。
相比之下，经过深入研究的样本类型，例如人的肠道，即使对于低丰度的微生物，基于reads的方法也可以得到可靠的结果。
shortBRED程序从数据库中鉴定出抗性基因蛋白家族的标记序列用以回贴reads
由于缺乏参考序列，对于不同环境中抗菌素耐药基因的分析可能存在低估的现象，ShortBRED被开发出来用于解决该问题，该方法可以在宏基因组学数据集中快速、准确地分析抗性组。
ShortBRED首先从参考数据库中鉴定出代表抗菌素耐药蛋白家族的标记序列，之后将测序reads与这些标记序列进行比对计算相关抗菌素耐药蛋白家族的相对丰度。这宗方法已经成功的用于量化大型和复杂宏基因组学数据集（包括人类、动物和环境数据集）中抗性基因的丰度。
宏基因组样本中抗性组的下游分析可以类似于分类学和功能分析进行，具体信息可以参考This detailed review discusses the best strategies used in shotgun metagenomics studies。
Box2：基于测序的创新
染色体构象捕获技术
染色体构象捕获使用交联、连接和短读长测序来了解细胞内遗传物质的空间关系，这种方法已经有效地应用于细菌分离株和宏基因组样品中。利用来自染色体构象捕获的空间信息，研究人员可以增加对抗菌素耐药基因调控的了解，并阐明更复杂的耐药机制。
例如，这些数据可能有助于解决抗菌素耐药性基因与其上游基因之间的关系，因为基因空间共定位可能表明一些调控功能。染色体构象捕获也可用于从染色体基因中鉴定质粒基因，由于质粒在水平基因转移中的作用，这一点对于抗菌素耐药性非常重要。
最吸引人的是染色体构象捕获对宏基因组样本的适用性，宏基因组样本中的交联信息可以大大降低了宏基因组组装的复杂性，它使研究人员能够按细胞对序列进行分组，因此，结合染色体构象捕获信息的基于拼接的的抗菌素耐药性注释方法可能能为可靠。
长读长测序（几分钟鉴定肺炎链球菌基因组）
通过单分子实时测序或纳米孔测序进行的长读测序可产生10kb至100kb以上的序列，长的序列在研究纯培养细菌和宏基因组学样品中的抗菌素耐药性方面具有很多优势。
长读测序可大大降低纯培养细菌和宏基因组学样品序列组装的复杂性，在某些情况下，可提供完整的细菌基因组（比短读错误率更高，这意味着有时需要短读长和长读长测序相结合）。更好的组装可以对基因组背景进行更深入的解释，提供与染色体捕获类似的好处。
与染色体捕获类似，长读长测序可以更轻松地解析质粒序列，从而可以进行更深入的研究来了解抗菌素耐药基因的水平基因转移。
此外，这两种长读长测序方法都提供了有关DNA甲基化的信息，这可以租住宏基因组样品中基因组的组装。
最后，长读长测序运行时间往往比短读测序快，这对于患者预后的潜在临床部署很有用，由于其实时测序的方式，纳米孔测序在这一领域尤其出色，这些实时数据已被用于快速的预测肺炎链球菌中由AST无法获取的未知抗菌素耐药基因的识别，只需要几分钟就可以得到结果。
转录组
RNA测序等转录组学技术有助于细菌基因表达的分析，这些表达数据具有填补抗菌素耐药性知识空白的潜力。转录数据可以将基因型抗菌素耐药性数据与表型抗菌素耐药性结果联系起来。
在存在抗菌素耐药基因但未发现耐药性表型的情况下，或在无明显抗菌素耐药基因但已确认耐药性的情况下，转录组数据可弥补基因组数据的不足，并有助于发现新型耐药基因。
转录组学还具有鉴定导致抗菌素耐药性的组合基因效应的潜力，此外，转录组学可以成功地鉴定非编码调控RNA导致耐药细菌表型的情况。
抗菌素抗性基因数据库
无论是基于拼接和基于read的方法，预测抗菌素耐药基因在很大程度上都取决于经过整理的抗菌素耐药性基因数据库，这些数据库将已知的耐药性遗传决定因素与它们赋予细菌的耐药性表型联系起来，这些数据库通常代表从多项研究中收集的信息，其中包括携带特定抗菌素耐药基因的细菌的AST实验。

通用/特定的数据库和序列分析网站服务器
公共数据库在其所涵盖的抗性机制的范围以及它们提供注释的信息类型方面很大差异。
通用的抗菌素耐药基因数据库，例如已经不再更新的ARDB活依然更新的ARG-ANNOT和CARD，涵盖了广泛的抗菌素耐药基因及其机理信息。
而特定的抗菌素耐药性数据库则提供了有关特定基因家族或物种的全面信息，例如LacED、Lahey、NCBI β-Lactamase Alleles Initative以及CBMAR，这些数据库只关注β-内酰胺酶，这是一种抗微生物酶，可促进β-内酰胺抗菌素中关键β-内酰胺环的水解，从而使细菌免受抗菌活性的影响。
Resfinder是一个基于Web的独立工具，与其他需要重叠群作为输入的数据库不同，Resfinder还接受短的reads为输入用于检测已测序或部分测序的细菌分离株中含有的抗菌素耐药基因。
2017年，Resfinder更新了其基于Web的服务工具，其可以通过PointFinder识别染色体体突变，但是，只有少数有限的病原微生物（弯曲杆菌、大肠杆菌、结核分枝杆菌、淋病奈瑟氏球菌、恶性疟原虫和沙门氏菌）可鉴定出具有耐药性的染色体突变。
与Resfinder相似，CARD同样提供了自己的分析工具，称为RGI，它使用经过整理的抗菌素耐药基因检测模型来预测内在的抗菌素耐药基因、专用抗菌素耐药基因和药物靶点突变后获得的耐药基因。RGI使用两种检测模型：用于检测抗菌素耐药蛋白功能同源物的Protein Homologue Model和用于检测赋予耐药性的突变的Protein Variant Model。
ARGs-OAP使用由ARDB和CARD构建的自定义数据库SARG，以及一种混合了UBLAST和BLASTX的算法，展示了全面的数据库在基于宏基因组序列抗菌素耐药基因注释中的重要性。
存在针对病原菌或模式细菌的特定物种数据库，例如Tuberculosis Drug Resistance Database和MUBII-TB-DB，这些物种特定的数据库对于了解这些特定生物体的耐药性具有非常的作用，但同时也强调了考虑将抗生素耐药性基因纳入其系统发育研究的重要性，特别是对那些含有先天性耐药性的细菌。
以物种为中心的数据库可以快速有效地处理新的抗菌素耐药基因和染色体突变，并可以提供快速的初步筛选以鉴定其特性。事实证明，这种筛查对结核分枝杆菌等病原体非常有效，在这些病原体中，水平基因转移事件很少见，耐药性主要来自染色体突变。
CRyPTIC Consortium和100,000 Genomes Project证明了物种特异性数据库的有效性，针对结核分期杆菌的数据库对四种一线抗结核药物的耐药性预测均具有90％以上的敏感性和特异性。
尽管这些工具正在朝着正确的方向前进，但仍需要一个持续更新的全面数据库，该数据库应具有广泛的基因元数据，并且必须同时具备能够发现点突变和远距离同源性的能力。
隐马尔可夫模型善于找到功能相似但序列同一性较低的蛋白序列
上述数据库的主要局限性在于它们所包含的抗菌素耐药基因严重偏向人类病原体和易于培养的模式生物，从而很难从未被培养的细菌中鉴定新型的或远距离同源的耐药基因。
这种偏好性使得在不常见细菌中鉴定抗菌素耐药基因更为困难，克服这种偏好性的一种潜在解决方案是使用隐马尔可夫模型（HMM）数据库，从已知序列的多序列比对得出，HMM可以找到具有相似功能但序列同一性较低的序列。
Resfams是抗菌素耐药蛋白的HMM数据库，该数据库通过来自CARD、LacED和Lahey数据库的代表性抗菌素耐药蛋白序列的手动多序列比对进行构建。
Resfams数据库的作者声明，与依靠基于BLAST的方法进行基因鉴定的其他数据库（如ARDB和CARD）相比，Resfams可以识别出数量更多的新型抗菌素耐药基因和已知抗菌素耐药基因的远距离同源物。
对手动选择的抗菌素耐药基因集的直接比较显示，与基于BLAST的CARD和ARDB搜索相比，Resfams在土壤和人类肠道菌群中鉴定出的抗菌素耐药基因的数量提升了64%，这种提高的敏感性证明了HMM在非临床样品序列注释的可行性。
但是，基于HMM的方法可能具有较差的特异性，会产生较高数量的假阳性结果，并且可能无法区分功能密切相关的蛋白质家族。
目前，Resfams包含166个代表主要抗菌素耐药基因家族的HMM，与基于BLAST的数据库相比，基于HMM的抗菌素耐药性数据库在鉴定未充分研究的环境样品中大量多样的耐药性决定因素方面可能具有重要价值。
但是，当前基于HMM的数据库无法识别由染色体突变引起的耐药性，为了进一步促进在大型复杂环境中检测抗菌素耐药基因，建立了FARME数据库，其包含不属于目前已有数据库但是通过功能宏基因组研究发现具有耐药性的微生物序列。除了预测的蛋白质编码的抗菌素耐药序列外，FARME数据库还包括调控元件、移动遗传元件和位于抗菌素耐药基因周围的预测蛋白质。
类似的，通过汇总针对23种抗微生物药物选择的功能宏基因组学数据，建立了ResfinderFG数据库，当将该数据库与Resfinder数据库进行比较时，发现同一个基因出现了不同的耐药行结果，这种结果可能代表了将假定的抗性决定簇克隆到大肠杆菌中时与在其天然细菌宿主中表达时相比，结果可能存在一定的差异。
Mustard数据库使用一种整合3D蛋白结构的创新方法来帮助预测耐药性基因，当这种方法应用于宏基因组样本时，它预测了超过6,000个抗性基因，而同样的数据使用BLASTP只鉴定的67个基因和使用Resfinder只鉴定的50个基因，说明Mustard的方法具有更高的敏感性。
存在的挑战
虽然已经取得了长足的进步，但是抗菌素耐药基因的鉴定和研究依然面临着成本和分析速度的限制，一个主要的瓶颈是缺乏有效的管理策略。除少数例外，抗菌素耐药性数据库都缺乏有效且可持续的更新流程。
大多许抗菌素耐药基因都是根据其核酸或蛋白质序列的相似性来获得匹配的名称，导致了命名规则的冲突，冲突的基因名称和同义词会在数据库之间造成冗余，并给用户带来一些误解。例如在某些数据库中，二氢叶酸还原酶被称为dhfr，而在其他数据库中被称为dfrA。
通过序列的一致性分配基因名称会加剧这种情况，存在许多不同的基于序列一致性的系统，用于将基因名称分配给新的抗性基因。这些系统提供了不同的临界值，并且参考文献也并不一致。
抗菌素耐药性基因数据是一个不断扩展的数据源，水平基因转移事件和选择压力带来抗菌素耐药性突变，需要采取积极的生物固化策略，从而可以在识别输入后对其进行整合。
mcr-1基因于2016年首次在中国人的一个细菌分离株重被发现，随后在世界各地均有检出，这种新型抗菌素耐药基因的增殖过程表明需要经常更新并规划数据库。如果这些操作能被正确的实施，它们有助于快速收集最近发现的耐药性决定因素的流行病学数据。
由于所有下游分析都依赖于参考数据库的准确性，因此抗菌素耐药基因的注释注释应是一项持续的工作，建立最佳的数据整合流程，为新发现的基因系统地分配注释，防止误解，将为公共卫生和基础科学带来不可估量的好处。
当前的抗菌素耐药性数据库的另一个重要局限性在于它们专注于蛋白质编码耐药性基因的鉴定和表征，而忽略了其他潜在的耐药机制，例如基因组变化或核糖体RNA基因从头突变、调控元件和药物靶点突变等等，CARD和Resfinder正在试图解决这个问题。
重要! 功能宏基因组（直接用质粒构建宏基因组文库，转染细菌后挑取耐药菌进行测序）
除基于序列的宏基因组学外，功能宏基因组学是一种强大的、不依赖于培养技术的、无序列偏差的抗性组鉴定方法。

在这种方法中，通过将从样品中提取的总群落DNA克隆到表达载体中来生成宏基因组文库，该文库被转移进入一种对抗菌素易感的指示宿主菌株中，并通过对野生型宿主具有致死性的选择性培养基上进行铺板来测定抗菌素耐药性，最后对存活的细胞中的插入片段进行测序，对所得序列进行组装和注释。
PARFuMS是一个定制的计算流程，它使用Velvet和Phrap将功能宏基因组的数据拼接成contigs，并使用MetaGeneMark和Resfams注释抗菌素耐药基因。
这种方法可以对大基因组含量的数据进行高通量分析，而且耐药性表型可以直接与基因相关联，从而无需培养单个的耐药基因载体。
功能宏基因组学使人们能够发现新的抗菌素耐药机制及其相关基因，一个这样的例子是最近发现的四环素破坏酶对四环素的抗性机制，使用土壤功能宏基因组学发现了九种通过酶促失活赋予四环素抗性的基因，进一步的分析和生化特性表明，这些酶以FAD依赖的方式催化四环素氧化，从而使四环素失活。
尽管先前的研究显示了功能宏基因组学的优势和实用性，但是这种方法同样有一定的局限性。例如，基因必须在其天然微生物宿主之外具有功能，才能通过功能宏基因组学选择进行鉴定。
很多时候，重组表达宿主（如大肠杆菌）与原始宿主（例如某些革兰氏阳性生物）之间的差异不会赋予同一基因相同的表型，因此，需要系统发育上多样化的宿主群体。
另外，在基因组范围之外的基因，例如协同调控元件，在重组表达宿主中可能具有与原始宿主不同的表型，因此，重要的是通过功能宏基因组学筛选鉴定出的新型抗菌素耐药基因必须在生物学和生物化学上进行表征，当前功能宏基因组学方法的扩展和发现新抗性基因的新技术的开发是值得研究的方向。
机器学习用于耐药基因预测
已有众多研究探索了可以用于研究抗菌素耐药性的机器学习算法，重点强调其在直接根据基因型预测耐药性表型中的作用。
机器学习方法可以分为为监督学习方法或无监督学习方法。
在监督学习中，可利用具有感兴趣结果的训练集来构建预测模型，该模型可以进一步应用于查询序列以预测其结果。
多项研究使用基因存在与否以及AST结果作为特征来创建模型训练集，在一项研究中，使用logistic回归方法开发了基于14个基因参数和3个分子分型标记的模型，该模型可以使用公开可用的基因组数据和患者分离菌株识别万古霉素敏感的金黄色葡萄球菌。通过leave-one-out验证方法测试了该模型性能，结果表明分类精度为84％，尽管此准确度水平不符合临床标准，但该方法提供了重要的概念证明，有助于开发更复杂的模型来鉴定抗菌素耐药性。
另一项研究比较基于规则和基于机器学习的方法对抗菌素耐药性的预测，并显示基于机器学习的方法具有更高的准确率。
最近的研究和工具使用衍生自具有耐药性和抗菌素敏感的物种的全基因组的k-mers及其AST结果来开发预测模型。
Mykrobe是一种快速的k-mer筛选工具
Mykrobe是一种快速的k-mer筛选工具，用于鉴定金黄色葡萄球菌和结核分枝杆菌中的抗菌素耐药基因和SNP，它利用相同物种抗性和易感等位基因的精细化遗传信息来构建这两个类别的DBG，并将从测序reads中获得的k-mers映射到这些DBG上，Mykrobe预测因子在独立的验证集上分别显示出对金黄色葡萄球菌和结核分枝杆菌预测的敏感性分别为99.1％和82.6％，特异性分别为99.6％和98.5％。
RAST是另一种基于k-mer的工具，其使用基于PATRIC数据库的机器学习分类器（AdaBoost）来识别特定病原体中目标特异性的抗菌素耐药基因基因。RAST在从每个基因组重叠群衍生的k-mer数据上进行训练，将这些k-mer计数转换为1s和0s的二进制矩阵，以描述该基因组中是否存在特定的k-mer，然后将二元矩阵与AST结果一起用于形成分类模型，并确定与抗性相关的推定k-聚体。RAST分类器可以鉴定鲍曼不动杆菌中的碳青霉烯耐药性、金黄色葡萄球菌的甲氧西林耐药性以及肺炎链球菌的β-lactam和抗曲莫唑耐药率，准确度为88–99％。
所有机器学习分类器的一个主要缺点是其对训练数据或现有知识库的依赖，为了将机器学习分类器应用到临床诊断中，需要大量包含正确的基因型数据和相关AST数据相关的数据集，以建立一个有效而强大的基于机器学习分类器来识别细菌的耐药行。
除了区分抗药性微生物和对药敏性微生物外，目前还采用机器学习方法来预测宏基因组学数据中的抗菌素耐药基因。
DeepArgs是一种新近建立的工具，可通过深度学习来识别抗菌素耐药基因，根据CARD和ARDB的精选数据集以及Uniprot蛋白数据，DeepArgs建立了抗菌素耐药蛋白和非抗菌素耐药蛋白之间的差异矩阵，并用于训练两个深度学习模型：用于分析拼接结果的DeepArg-LS和用于分析reads的DeepArg-SS，这些模型可用于预测新测试数据中的抗菌素耐药基因。
尽管将机器学习应用于抗菌素耐药性预测和分类是有前途的，但在将这些技术用于快速诊断并取代传统的培养技术和AST方面，还有很长的路要走。
结论和展望
抗菌素耐药性是主要的公共卫生威胁，监测和了解抗菌素耐药性的流行、机理和传播是个体患者护理和总体感染控制策略的重点。
尽管已经取得了长足进步，但对抗菌素耐药性检测和理解的障碍仍然存在，测序和自动化的抗菌素耐药性检测仪器的成本正在降低，但是启动和运营成本仍超过许多医疗保健的预算，这些技术进一步的降低成本对于其大范围应用至关重要。
准确确定耐药性决定因素以及抗菌素耐药性基因谱与抗菌素治疗结果的相关性，将有助于制定个性化的治疗方案，这种方法的成功在很大程度上取决于公开的抗菌素耐药基因数据库的全面性和质量，这些数据库在生物测定和计算工具的开发中起着重要作用，从而扩大了我们在单个菌株和微生物群落中检测耐药基因的能力。
尽管在建立全面的抗菌素耐药性基因数据库方面已取得进展，但不同数据库缺乏统一的标准和较长的更新间隔阻碍了它们的应用潜力。
而且，复杂的耐药机制很难在抗菌素耐药性数据库中被捕获，例如，耐药性可能源于多个基因之间的上下游关系，例如在碳青霉烯耐药性可能是由于广谱β-内酰胺酶和外排泵或孔蛋白不渗透性的结合而引起的。抗性甚至可能通过正常基因的过度表达而发生，例如编码外排泵的基因，而检测这些抗性机制需要转录水平的定量数据。
这些复杂的耐药机制与已知的抗菌素耐药基因可能不总是表达的事实相结合，导致难以从基因型抗菌素耐药性数据中准确预测耐药性的表型。
机器学习算法在预测耐药性表型方面取得了进展，但是这些技术倾向于针对特定的细菌，并且对于一般临床部署而言还不够准确。
为了实现根据基因型数据进行表型预测的目标，我们需要更全面的数据库，这些数据库将特定的抗菌素耐药基因与特定的AST结果相关联。更重要的是，这些数据库应包括具有完整序列的细菌、抗菌素耐药基因基因预测的元数据以及有确切抑菌区域大小或最小抑菌浓度的AST结果，而不是仅仅是分类准则，AST和基于序列的耐药基因预测的并行改进将加减轻耐药性性的临床影响。
尽管存在一些可以进行新型抗菌素耐药基因鉴定的技术，例如功能宏基因组学，但这些技术对于鉴定抗菌素耐药基因的类型仍然存在巨大的缺陷，迫切需要确定新型抗菌素耐药基因机制的创新方法。
此外，需要有稳定的模型来预测哪些抗性基因会在医疗机构内的本地水平以及国家之间的全球范围内传播，这些模型可能不仅需要结合抗菌素耐药基因序列和机理，而且还需要结合基因组背景、宿主细菌种类和地理位置。
快速、准确地鉴定分离细菌的和宏基因组样品中的耐药基因将增强临床医生制定细菌感染治疗计划的能力，从而为将来使用常规的基于序列的个性化医学提供便利。这也将简化抗菌素耐药性的监测工作，并使资源贫乏地区能够从测序成本的快速下降中更加充分地受益
